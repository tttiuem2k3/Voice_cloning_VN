{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M1B8hWw8Il2M"
      },
      "outputs": [],
      "source": [
        "import librosa\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "\n",
        "def audio_to_audio_frame_stack(sound_data, frame_length, hop_length_frame):\n",
        "    \"\"\"This function take an audio and split into several frame\n",
        "       in a numpy matrix of size (nb_frame,frame_length)\"\"\"\n",
        "\n",
        "    sequence_sample_length = sound_data.shape[0]\n",
        "\n",
        "    sound_data_list = [sound_data[start:start + frame_length] for start in range(\n",
        "    0, sequence_sample_length - frame_length + 1, hop_length_frame)]  # get sliding windows\n",
        "    sound_data_array = np.vstack(sound_data_list)\n",
        "\n",
        "    return sound_data_array\n",
        "\n",
        "\n",
        "def audio_files_to_numpy(audio_dir, list_audio_files, sample_rate, frame_length, hop_length_frame, min_duration):\n",
        "    \"\"\"This function take audio files of a directory and merge them\n",
        "    in a numpy matrix of size (nb_frame,frame_length) for a sliding window of size hop_length_frame\"\"\"\n",
        "\n",
        "    list_sound_array = []\n",
        "\n",
        "    for file in list_audio_files:\n",
        "        # open the audio file\n",
        "        y, sr = librosa.load(os.path.join(audio_dir, file), sr=sample_rate)\n",
        "        total_duration = librosa.get_duration(y=y, sr=sr)\n",
        "\n",
        "        if (total_duration >= min_duration):\n",
        "            list_sound_array.append(audio_to_audio_frame_stack(\n",
        "                y, frame_length, hop_length_frame))\n",
        "        else:\n",
        "            print(\n",
        "                f\"The following file {os.path.join(audio_dir,file)} is below the min duration\")\n",
        "\n",
        "    return np.vstack(list_sound_array)\n",
        "\n",
        "\n",
        "def blend_noise_randomly(voice, noise, nb_samples, frame_length):\n",
        "    \"\"\"This function takes as input numpy arrays representing frames\n",
        "    of voice sounds, noise sounds and the number of frames to be created\n",
        "    and return numpy arrays with voice randomly blend with noise\"\"\"\n",
        "\n",
        "    prod_voice = np.zeros((nb_samples, frame_length))\n",
        "    prod_noise = np.zeros((nb_samples, frame_length))\n",
        "    prod_noisy_voice = np.zeros((nb_samples, frame_length))\n",
        "\n",
        "    for i in range(nb_samples):\n",
        "        id_voice = np.random.randint(0, voice.shape[0])\n",
        "        id_noise = np.random.randint(0, noise.shape[0])\n",
        "        level_noise = np.random.uniform(0.2, 0.8)\n",
        "        prod_voice[i, :] = voice[id_voice, :]\n",
        "        prod_noise[i, :] = level_noise * noise[id_noise, :]\n",
        "        prod_noisy_voice[i, :] = prod_voice[i, :] + prod_noise[i, :]\n",
        "\n",
        "    return prod_voice, prod_noise, prod_noisy_voice\n",
        "\n",
        "\n",
        "def audio_to_magnitude_db_and_phase(n_fft, hop_length_fft, audio):\n",
        "    \"\"\"This function takes an audio and convert into spectrogram,\n",
        "       it returns the magnitude in dB and the phase\"\"\"\n",
        "\n",
        "    stftaudio = librosa.stft(audio, n_fft=n_fft, hop_length=hop_length_fft)\n",
        "    stftaudio_magnitude, stftaudio_phase = librosa.magphase(stftaudio)\n",
        "\n",
        "    stftaudio_magnitude_db = librosa.amplitude_to_db(\n",
        "        stftaudio_magnitude, ref=np.max)\n",
        "\n",
        "    return stftaudio_magnitude_db, stftaudio_phase\n",
        "\n",
        "\n",
        "def numpy_audio_to_matrix_spectrogram(numpy_audio, dim_square_spec, n_fft, hop_length_fft):\n",
        "    \"\"\"This function takes as input a numpy audi of size (nb_frame,frame_length), and return\n",
        "    a numpy containing the matrix spectrogram for amplitude in dB and phase. It will have the size\n",
        "    (nb_frame,dim_square_spec,dim_square_spec)\"\"\"\n",
        "\n",
        "    nb_audio = numpy_audio.shape[0]\n",
        "\n",
        "    m_mag_db = np.zeros((nb_audio, dim_square_spec, dim_square_spec))\n",
        "    m_phase = np.zeros((nb_audio, dim_square_spec, dim_square_spec), dtype=complex)\n",
        "\n",
        "    for i in range(nb_audio):\n",
        "        m_mag_db[i, :, :], m_phase[i, :, :] = audio_to_magnitude_db_and_phase(\n",
        "            n_fft, hop_length_fft, numpy_audio[i])\n",
        "\n",
        "    return m_mag_db, m_phase\n",
        "\n",
        "\n",
        "def magnitude_db_and_phase_to_audio(frame_length, hop_length_fft, stftaudio_magnitude_db, stftaudio_phase):\n",
        "    \"\"\"This functions reverts a spectrogram to an audio\"\"\"\n",
        "\n",
        "    stftaudio_magnitude_rev = librosa.db_to_amplitude(stftaudio_magnitude_db, ref=1.0)\n",
        "\n",
        "    # taking magnitude and phase of audio\n",
        "    audio_reverse_stft = stftaudio_magnitude_rev * stftaudio_phase\n",
        "    audio_reconstruct = librosa.core.istft(audio_reverse_stft, hop_length=hop_length_fft, length=frame_length)\n",
        "\n",
        "    return audio_reconstruct\n",
        "\n",
        "def matrix_spectrogram_to_numpy_audio(m_mag_db, m_phase, frame_length, hop_length_fft)  :\n",
        "    \"\"\"This functions reverts the matrix spectrograms to numpy audio\"\"\"\n",
        "\n",
        "    list_audio = []\n",
        "\n",
        "    nb_spec = m_mag_db.shape[0]\n",
        "\n",
        "    for i in range(nb_spec):\n",
        "\n",
        "        audio_reconstruct = magnitude_db_and_phase_to_audio(frame_length, hop_length_fft, m_mag_db[i], m_phase[i])\n",
        "        list_audio.append(audio_reconstruct)\n",
        "\n",
        "    return np.vstack(list_audio)\n",
        "\n",
        "def scaled_in(matrix_spec):\n",
        "    \"global scaling apply to noisy voice spectrograms (scale between -1 and 1)\"\n",
        "    matrix_spec = (matrix_spec + 46)/50\n",
        "    return matrix_spec\n",
        "\n",
        "def scaled_ou(matrix_spec):\n",
        "    \"global scaling apply to noise models spectrograms (scale between -1 and 1)\"\n",
        "    matrix_spec = (matrix_spec -6 )/82\n",
        "    return matrix_spec\n",
        "\n",
        "def inv_scaled_in(matrix_spec):\n",
        "    \"inverse global scaling apply to noisy voices spectrograms\"\n",
        "    matrix_spec = matrix_spec * 50 - 46\n",
        "    return matrix_spec\n",
        "\n",
        "def inv_scaled_ou(matrix_spec):\n",
        "    \"inverse global scaling apply to noise models spectrograms\"\n",
        "    matrix_spec = matrix_spec * 82 + 6\n",
        "    return matrix_spec"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "root_path = '/content/drive/MyDrive/'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TmPwJFInIpSq",
        "outputId": "5c86bbce-ae0e-40d7-f5da-46289dfcca87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os,sys,inspect\n",
        "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
        "sys.path.append('..')\n",
        "import librosa\n",
        "import tensorflow as tf\n",
        "import soundfile as sf\n",
        "\n",
        "\n",
        "\n",
        "loaded_model = tf.keras.models.load_model(root_path+'model-010-0.006855.keras')\n",
        "\n",
        "\n",
        "audio_dir_prediction = root_path\n",
        "audio_input_prediction = ['phat-voice2.wav']\n",
        "\n",
        "# Sample rate chosen to read audio\n",
        "sample_rate = 8000\n",
        "\n",
        "# Minimum duration of audio files to consider\n",
        "min_duration = 1.0\n",
        "\n",
        "# Our training data will be frame of slightly above 1 second\n",
        "frame_length = 8064\n",
        "\n",
        "# hop length for clean voice files separation (no overlap)\n",
        "hop_length_frame = 8064\n",
        "\n",
        "# hop length for noise files (we split noise into several windows)\n",
        "hop_length_frame_noise = 5000\n",
        "\n",
        "\n",
        "# Extracting noise and voice from folder and convert to numpy\n",
        "audio = audio_files_to_numpy(audio_dir_prediction, audio_input_prediction, sample_rate,\n",
        "                             frame_length, hop_length_frame, min_duration)\n",
        "\n",
        "# Choosing n_fft and hop_length_fft to have squared spectrograms\n",
        "n_fft = 255\n",
        "hop_length_fft = 63\n",
        "\n",
        "dim_square_spec = int(n_fft / 2) + 1\n",
        "\n",
        "# Create Amplitude and phase of the sounds\n",
        "m_amp_db_audio,  m_pha_audio = numpy_audio_to_matrix_spectrogram(\n",
        "    audio, dim_square_spec, n_fft, hop_length_fft)\n",
        "\n",
        "#global scaling to have distribution -1/1\n",
        "X_in = scaled_in(m_amp_db_audio)\n",
        "print(f\"Amplitude before denoising: {np.max(X_in)}\")\n",
        "\n",
        "#Reshape for prediction\n",
        "X_in = X_in.reshape(X_in.shape[0],X_in.shape[1],X_in.shape[2],1)\n",
        "#Prediction using loaded network\n",
        "X_pred = loaded_model.predict(X_in)\n",
        "#Rescale back the noise model\n",
        "inv_sca_X_pred = inv_scaled_ou(X_pred)\n",
        "#Remove noise model from noisy speech\n",
        "X_denoise = m_amp_db_audio - inv_sca_X_pred[:,:,:,0]\n",
        "#Reconstruct audio from denoised spectrogram and phase\n",
        "audio_denoise_recons = matrix_spectrogram_to_numpy_audio(X_denoise, m_pha_audio, frame_length, hop_length_fft)\n",
        "#Number of frames\n",
        "nb_samples = audio_denoise_recons.shape[0]\n",
        "audio_denoise_recons *= 6.0\n",
        "\n",
        "output_wav_path = 'phat_denoised_output2.wav'\n",
        "sf.write(output_wav_path, audio_denoise_recons.flatten(), sample_rate)\n",
        "print(f\"Audio saved to {output_wav_path}\")\n",
        "print(f\"Amplitude after denoising: {np.max(X_denoise)}\")\n",
        "def test_dimensions_spectrogram():\n",
        "    \"\"\" test that dimensions are correct\"\"\"\n",
        "    assert dim_square_spec == 128\n",
        "    assert dim_square_spec == m_amp_db_audio.shape[1]\n",
        "    assert dim_square_spec == m_amp_db_audio.shape[2]\n",
        "    assert dim_square_spec == X_denoise.shape[1]\n",
        "    assert dim_square_spec == X_denoise.shape[2]\n",
        "    assert nb_samples == 5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XMAja6yfIpPy",
        "outputId": "fc7053aa-a861-4549-a58f-3a88ec69c6db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Amplitude before denoising: 0.9200000190734863\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7d294499cee0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8s/step\n",
            "Audio saved to phat_denoised_output2.wav\n",
            "Amplitude after denoising: 10.48172378540039\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BJ78LJywIpNo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-ugrZvKlIpLZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wxAuJCNrIpJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1Q-f5eJKIpGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W8x-a5SjIpEX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Rr_lZ_ViIpCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y8H6ph8cIo_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vNsS2Sd3Io8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ECMRmF1vIo6F"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}