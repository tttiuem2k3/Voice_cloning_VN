{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2816656,"sourceType":"datasetVersion","datasetId":1722209},{"sourceId":3593344,"sourceType":"datasetVersion","datasetId":2155739},{"sourceId":3910949,"sourceType":"datasetVersion","datasetId":2270944},{"sourceId":6849137,"sourceType":"datasetVersion","datasetId":3937252}],"dockerImageVersionId":30762,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from pathlib import Path\n\nspeakers_per_batch=32\nutterances_per_speaker=10\nseq_len=128\ntrain_steps = 1e12\ntrain_print_interval = 10 # in steps\ntotal_evaluate_steps = 50\nevaluate_interval = 500 # in steps\nsave_interval = 100 # in steps\nsave_dir = Path(r'/kaggle/working/')\nmax_ckpts = 100\nspeaker_lr = 1e-4\nlibri_dataset_path = Path(r'/kaggle/input/librispeech-360-clean/LibriSpeech/train-clean-360')\ndevice = 'cuda:0'\nloss_device = 'cpu'","metadata":{"execution":{"iopub.status.busy":"2024-09-19T04:12:56.012365Z","iopub.execute_input":"2024-09-19T04:12:56.012909Z","iopub.status.idle":"2024-09-19T04:12:56.020413Z","shell.execute_reply.started":"2024-09-19T04:12:56.012868Z","shell.execute_reply":"2024-09-19T04:12:56.019256Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import librosa\nimport numpy as np\n\ndef normalize(S, min_level_db=-100):\n    return np.clip((S - min_level_db) / -min_level_db, 0, 1)\n\ndef linear_to_mel(spectrogram, sample_rate=16000, n_fft=1024, fmin=90, fmax=7600, n_mels=80):\n    return librosa.feature.melspectrogram(\n        S=spectrogram, sr=sample_rate, n_fft=n_fft, n_mels=n_mels, fmin=fmin, fmax=fmax)\n\ndef amp_to_db(x):\n    return 20. * np.log10(np.maximum(1e-5, x))\n\ndef stft(y, n_fft=1024, hop_length=256, win_length=1024):\n    return librosa.stft(\n        y=y,\n        n_fft=n_fft, hop_length=hop_length, win_length=win_length)\n\ndef gen_melspectrogram(y):\n    D = stft(y)\n    S = amp_to_db(linear_to_mel(np.abs(D)))\n    return np.clip(normalize(S), 0, 1)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-19T04:12:56.022490Z","iopub.execute_input":"2024-09-19T04:12:56.022828Z","iopub.status.idle":"2024-09-19T04:12:56.038943Z","shell.execute_reply.started":"2024-09-19T04:12:56.022791Z","shell.execute_reply":"2024-09-19T04:12:56.037974Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"from pathlib import Path\nimport pickle\nimport logging\nimport os\n\nimport librosa\nimport numpy as np\n\nclass Utterance(object):\n    def __init__(self, id: str = None, raw_file: Path = None):\n        self.id = id\n        self.raw_file = raw_file\n    def raw(self, sr=16000, augment=False):\n        \"\"\"Get the raw audio samples.\"\"\"\n\n        y, sr = librosa.load(self.raw_file, sr=sr)\n        # y, _ = librosa.effects.trim(y)\n        if y.size == 0:\n            raise Exception('audio', 'empty audio')\n        y = 0.95 * librosa.util.normalize(y)\n        if augment:\n            amplitude = np.random.uniform(low=0.3, high=1.0)\n            y = y * amplitude\n        return y\n\n    def melspectrogram(self, sr=16000, n_fft=1024, hop_length=256, win_length=1024, n_mels=80):\n        \"\"\"Get the melspectrogram features.\"\"\"\n\n        try:\n            return gen_melspectrogram(self.raw(sr=sr))\n        except Exception:\n            logging.debug(f'failed to load melspectrogram, raw file: {self.raw_file}, mel file: {self.mel_file}')\n            raise\n\n    def random_raw_segment(self, seq_len):\n        \"\"\"Return a audio segment randomly.\"\"\"\n\n        y = self.raw(augment=True)\n        ylen = len(y)\n        if ylen < seq_len:\n            pad_left = (seq_len - ylen) // 2\n            pad_right = seq_len - ylen - pad_left\n            y = np.pad(y, ((pad_left, pad_right)), mode='reflect')\n        elif ylen > seq_len:\n            max_seq_start = ylen - seq_len\n            seq_start = np.random.randint(0, max_seq_start)\n            seq_end = seq_start + seq_len\n            y = y[seq_start:seq_end]\n\n        return y\n\n    def random_mel_segment(self, seq_len):\n        \"\"\"Return a melspectrogram segment randomly.\"\"\"\n\n        mel = self.melspectrogram()\n        freq_len, tempo_len = mel.shape\n        if tempo_len < seq_len:\n            pad_left = (seq_len - tempo_len) // 2\n            pad_right = seq_len - tempo_len - pad_left\n            mel = np.pad(mel, ((0, 0), (pad_left, pad_right)), mode='reflect')\n        elif tempo_len > seq_len:\n            max_seq_start = tempo_len - seq_len\n            seq_start = np.random.randint(0, max_seq_start)\n            seq_end = seq_start + seq_len\n            mel = mel[:, seq_start:seq_end]\n        return mel\n\nclass Speaker(object):\n    def __init__(self, id: str):\n        self.id = id\n        self.utterances = []\n\n    def add_utterance(self, utterance: Utterance):\n        \"\"\"Add an utterance to this speaker.\"\"\"\n\n        self.utterances.append(utterance)\n\n    def random_utterances(self, n):\n        \"\"\"Return n utterances randomly.\"\"\"\n\n        return [self.utterances[idx] for idx in np.random.randint(0, len(self.utterances), n)]\n","metadata":{"execution":{"iopub.status.busy":"2024-09-19T04:12:56.040186Z","iopub.execute_input":"2024-09-19T04:12:56.040959Z","iopub.status.idle":"2024-09-19T04:12:56.058857Z","shell.execute_reply.started":"2024-09-19T04:12:56.040918Z","shell.execute_reply":"2024-09-19T04:12:56.057931Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"from typing import List\nfrom pathlib import Path\nimport pickle\nimport logging\nfrom multiprocessing import Process, JoinableQueue\nimport time\nimport os\nimport random\n\nimport torch\nimport numpy as np\n\n\nclass AudioDataset(object):\n    def __init__(self, id: str, speakers: List[Speaker] = []):\n        self.id = id\n        self.speakers = speakers\n\n    def add_speaker(self, speaker: Speaker):\n        \"\"\"Add a speaker to this dataset.\"\"\"\n\n        self.speakers.append(speaker)\n\n    def random_speakers(self, n):\n        \"\"\"Return n speakers randomly.\"\"\"\n\n        return [self.speakers[idx] for idx in np.random.randint(0, len(self.speakers), n)]\n\n    def serialize_speaker(self, queue: JoinableQueue, counter_queue: JoinableQueue):\n        while True:\n            speaker, root, overwrite = queue.get()\n\n            if not root.exists():\n                root.mkdir(parents=True)\n\n            dsdir = root / self.id\n            if not dsdir.exists():\n                dsdir.mkdir()\n\n            spkdir = dsdir / speaker.id\n            if not spkdir.exists():\n                spkdir.mkdir()\n\n            for uttrn_idx, uttrn in enumerate(speaker.utterances):\n                uttrnpath = spkdir / (uttrn.id + '.pkl')\n                is_overwrite = False\n                is_empty = False\n                if uttrnpath.exists():\n                    if os.path.getsize(uttrnpath) == 0:\n                        logging.debug(f'overrite empty file {uttrnpath}')\n                    elif not overwrite:\n                        logging.debug(f'{uttrnpath} already exists, skip')\n                        counter_queue.put(1)\n                        continue\n                    is_overwrite = True\n                try:\n                    mel = uttrn.melspectrogram()\n                    with uttrnpath.open(mode='wb') as f:\n                        pickle.dump(mel, f)\n                    if is_overwrite:\n                        logging.debug(f'dump pickle object to {uttrnpath} ({uttrn_idx+1}/{len(speaker.utterances)}), overwrite')\n                    else:\n                        logging.debug(f'dump pickle object to {uttrnpath} ({uttrn_idx+1}/{len(speaker.utterances)})')\n                except Exception as err:\n                    logging.warning(f'failed to dump mel features for file {uttrnpath}: {err}')\n                counter_queue.put(1)\n            queue.task_done()\n\n    def serialization_counter(self, total_count, queue: JoinableQueue):\n        count = 0\n        while True:\n            start_time = time.time()\n            done = queue.get()\n            duration = time.time() - start_time\n            count += 1\n            logging.debug(f'serialization progress {count}/{total_count}, {int(duration*1000)}ms/item')\n            queue.task_done()\n\n    def serialize_mel_feature(self, root: Path, overwrite=False):\n        \"\"\"Serialize melspectrogram features for all utterances of all speakers to the disk.\"\"\"\n\n        num_processes = 8\n        queue = JoinableQueue()\n        counter_queue = JoinableQueue()\n        processes = []\n        for i in range(num_processes):\n            p = Process(target=self.serialize_speaker, args=(queue, counter_queue))\n            processes.append(p)\n            p.start()\n        total_count = sum([len(spk.utterances) for spk in self.speakers])\n        counter_process = Process(target=self.serialization_counter, args=(total_count, counter_queue))\n        counter_process.start()\n        # add tasks to queue\n        logging.debug(f'total {len(self.speakers)} speakers')\n        for spk in self.speakers:\n            queue.put((spk, root, overwrite)) \n        # wait for all task done\n        queue.join() \n        counter_queue.join()\n        for p in processes:\n            p.terminate()\n        counter_process.terminate()\n\nclass MultiAudioDataset(object):\n    def __init__(self, datasets: List[AudioDataset]):\n        self.id = ''\n        self.speakers = []\n        ids = []\n        for ds in datasets:\n            ids.append(ds.id)\n            self.speakers.extend(ds.speakers)\n        self.id = '+'.join(ids)\n\nclass SpeakerDataset(object):\n    def __init__(self, speakers, utterances_per_speaker, seq_len):\n        self.speakers = speakers\n        n_speakers = len(self.speakers)\n        n_utterances = sum([len(spk.utterances) for spk in self.speakers])\n        logging.info(f'total {n_speakers} speakers, {n_utterances} utterances')\n        self.utterances_per_speaker = utterances_per_speaker\n        self.seq_len = seq_len\n\n    def random_utterance_segment(self, speaker_idx, seq_len):\n        \"\"\"Must return an utterance segment as long as the speaker has at least\n        one effective utterance.\"\"\"\n\n        while True:\n            try:\n                utterance = self.speakers[speaker_idx].random_utterances(1)[0]\n                return utterance.random_mel_segment(seq_len)\n            except Exception as err:\n                logging.debug(f'failed to load utterances of speaker idx {speaker_idx}: {err}')\n                continue\n\n    def __getitem__(self, idx):\n        \"\"\"Return random segments of random utterances for the specified speaker.\"\"\"\n        seq_len = 0\n        if isinstance(self.seq_len, int):\n            seq_len = self.seq_len\n        elif isinstance(self.seq_len, list):\n            seq_len = self.seq_len[random.randint(0, len(self.seq_len)-1)]\n        else:\n            raise ValueError('seq_len must be int or int list')\n\n        segments = np.array([self.random_utterance_segment(idx, seq_len) for _ in range(self.utterances_per_speaker)])\n        return torch.tensor(segments)\n\n    def __len__(self):\n        return len(self.speakers)","metadata":{"execution":{"iopub.status.busy":"2024-09-19T04:12:56.060817Z","iopub.execute_input":"2024-09-19T04:12:56.061131Z","iopub.status.idle":"2024-09-19T04:12:56.264585Z","shell.execute_reply.started":"2024-09-19T04:12:56.061096Z","shell.execute_reply":"2024-09-19T04:12:56.263568Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"from pathlib import Path\n\ndef load_librispeech360_dataset(root: Path):\n    \"\"\"Load the LibriSpeech train-clean-360 dataset into an AudioDataset.\n\n    The dataset can be downloaded from: https://www.openslr.org/12\n\n    Args:\n        root (Path): Path to the root directory of the LibriSpeech dataset.\n        mel_feature_root (Path, optional): Path to the root directory where the precomputed mel features are stored.\n\n    Returns:\n        AudioDataset: A dataset object containing the loaded speakers and their utterances.\n    \"\"\"\n\n    dataset_id = 'librispeech360'\n    id2speaker = dict()\n\n    # Recursively find all .flac files in the dataset\n    wav_files = root.rglob('*.flac')\n    \n    for f in wav_files:\n        # LibriSpeech files are typically structured as: <root>/<speaker_id>/<chapter_id>/<utterance_id>.flac\n        speaker_id = f.parent.parent.name  # Extract speaker ID from the parent folder\n        chapter_id = f.parent.name  # Extract chapter ID from the immediate parent folder\n        utterance_id = f.stem  # Use the file stem as the utterance ID (without .flac extension)\n\n        uttrn = Utterance(utterance_id, raw_file=f)\n\n        if speaker_id in id2speaker:\n            id2speaker[speaker_id].add_utterance(uttrn)\n        else:\n            spk = Speaker(speaker_id)\n            spk.add_utterance(uttrn)\n            id2speaker[speaker_id] = spk\n\n    dataset = AudioDataset(dataset_id, speakers=list(id2speaker.values()))\n    return dataset\n\ndef load_vivos_dataset(root: Path):\n    \"\"\"Load the VIVOS dataset into an AudioDataset.\n\n    The dataset can be downloaded from: https://ailab.hcmus.edu.vn/vivos\n\n    Args:\n        root (Path): Path to the root directory of the VIVOS dataset.\n\n    Returns:\n        AudioDataset: A dataset object containing the loaded speakers and their utterances.\n    \"\"\"\n\n    dataset_id = 'vivos'\n    id2speaker = dict()\n\n    # Recursively find all .wav files in the dataset\n    wav_files = root.rglob('*.wav')\n    \n    for f in wav_files:\n        # VIVOS files are typically structured as: <root>/train/<speaker_id>/<utterance_id>.wav\n        speaker_id = f.parent.name  # Extract speaker ID from the parent folder\n        utterance_id = f.stem  # Use the file stem as the utterance ID (without .wav extension)\n\n        uttrn = Utterance(utterance_id, raw_file=f)\n\n        if speaker_id in id2speaker:\n            id2speaker[speaker_id].add_utterance(uttrn)\n        else:\n            spk = Speaker(speaker_id)\n            spk.add_utterance(uttrn)\n            id2speaker[speaker_id] = spk\n\n    dataset = AudioDataset(dataset_id, speakers=list(id2speaker.values()))\n    return dataset\n\ndef load_aishell3_dataset(root: Path):\n    \"\"\"Load the AISHELL-3 dataset into an AudioDataset.\n\n    The dataset can be downloaded from: https://www.openslr.org/93\n\n    Args:\n        root (Path): Path to the root directory of the AISHELL-3 dataset.\n\n    Returns:\n        AudioDataset: A dataset object containing the loaded speakers and their utterances.\n    \"\"\"\n\n    dataset_id = 'aishell3'\n    id2speaker = dict()\n\n    # Recursively find all .wav files in the dataset\n    wav_files = root.rglob('*.wav')\n    \n    for f in wav_files:\n        # AISHELL-3 files are typically structured as: <root>/wav/<speaker_id>/<utterance_id>.wav\n        speaker_id = f.parent.name  # Extract speaker ID from the parent folder\n        utterance_id = f.stem  # Use the file stem as the utterance ID (without .wav extension)\n\n        uttrn = Utterance(utterance_id, raw_file=f)\n\n        if speaker_id in id2speaker:\n            id2speaker[speaker_id].add_utterance(uttrn)\n        else:\n            spk = Speaker(speaker_id)\n            spk.add_utterance(uttrn)\n            id2speaker[speaker_id] = spk\n\n    dataset = AudioDataset(dataset_id, speakers=list(id2speaker.values()))\n    return dataset\n\ndef load_voxceleb_dataset(root: Path):\n    \"\"\"Load the VoxCeleb dataset into an AudioDataset.\n\n    The dataset can be downloaded from: https://www.robots.ox.ac.uk/~vgg/data/voxceleb/\n\n    Args:\n        root (Path): Path to the root directory of the VoxCeleb dataset.\n\n    Returns:\n        AudioDataset: A dataset object containing the loaded speakers and their utterances.\n    \"\"\"\n\n    dataset_id = 'voxceleb'\n    id2speaker = dict()\n\n    # Recursively find all .wav files in the dataset\n    wav_files = root.rglob('*.wav')\n    \n    for f in wav_files:\n        # VoxCeleb files are typically structured as: <root>/wav/<speaker_id>/<segment_id>/<utterance_id>.wav\n        speaker_id = f.parts[-3]  # Extract speaker ID from the third-to-last folder\n        utterance_id = f.stem  # Use the file stem as the utterance ID (without .wav extension)\n\n        uttrn = Utterance(utterance_id, raw_file=f)\n\n        if speaker_id in id2speaker:\n            id2speaker[speaker_id].add_utterance(uttrn)\n        else:\n            spk = Speaker(speaker_id)\n            spk.add_utterance(uttrn)\n            id2speaker[speaker_id] = spk\n\n    dataset = AudioDataset(dataset_id, speakers=list(id2speaker.values()))\n    return dataset\n\n","metadata":{"execution":{"iopub.status.busy":"2024-09-19T04:12:56.267152Z","iopub.execute_input":"2024-09-19T04:12:56.267581Z","iopub.status.idle":"2024-09-19T04:12:56.288177Z","shell.execute_reply.started":"2024-09-19T04:12:56.267541Z","shell.execute_reply":"2024-09-19T04:12:56.287157Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"from scipy.interpolate import interp1d\nfrom sklearn.metrics import roc_curve\nfrom torch.nn.utils import clip_grad_norm_\nfrom scipy.optimize import brentq\nimport torch\nimport torch.nn as nn\nimport numpy as np\n\nclass SpeakerEncoder(nn.Module):\n    def __init__(self, input_size=80, hidden_size=256, num_layers=3, num_heads=8, device='cpu', loss_device='cpu'):\n        super().__init__()\n        self.loss_device = loss_device\n\n        # Transformer encoder layers\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=input_size,\n            nhead=num_heads,\n            dim_feedforward=hidden_size,\n            dropout=0.2,\n            activation='relu',\n            batch_first=True\n        )\n        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers).to(device)\n\n        self.linear = nn.Linear(in_features=input_size, out_features=128).to(device)\n        self.relu = nn.ReLU().to(device)\n\n        # Cosine similarity scaling (with fixed initial parameter values)\n        self.similarity_weight = nn.Parameter(torch.tensor([10.])).to(loss_device)\n        self.similarity_bias = nn.Parameter(torch.tensor([-5.])).to(loss_device)\n\n        # Loss\n        self.loss_fn = nn.CrossEntropyLoss().to(loss_device)\n\n    def do_gradient_ops(self):\n        # Gradient scale\n        self.similarity_weight.grad *= 0.01\n        self.similarity_bias.grad *= 0.01\n\n        # Gradient clipping\n        clip_grad_norm_(self.parameters(), 3, norm_type=2)\n\n    def forward(self, utterances, hidden_init=None):\n        \"\"\"\n        Computes the embeddings of a batch of utterance spectrograms.\n\n        :param utterances: batch of mel-scale filterbanks of same duration as a tensor of shape\n        (batch_size, n_frames, n_channels)\n        :param hidden_init: not used in the Transformer version\n        :return: the embeddings as a tensor of shape (batch_size, embedding_size)\n        \"\"\"\n\n        # Pass the input through the Transformer Encoder\n        out = self.transformer_encoder(utterances)\n\n        # We take the mean of all time steps (similar to a global pooling)\n        embeds_raw = self.relu(self.linear(out.mean(dim=1)))\n\n        # L2-normalize it\n        embeds = embeds_raw / torch.norm(embeds_raw, dim=1, keepdim=True)\n\n        return embeds\n\n    def similarity_matrix(self, embeds):\n        \"\"\"\n        Computes the similarity matrix according of GE2E.\n\n        :param embeds: the embeddings as a tensor of shape (speakers_per_batch,\n        utterances_per_speaker, embedding_size)\n        :return: the similarity matrix as a tensor of shape (speakers_per_batch,\n        utterances_per_speaker, speakers_per_batch)\n        \"\"\"\n        speakers_per_batch, utterances_per_speaker = embeds.shape[:2]\n\n        # Inclusive centroids (1 per speaker). Cloning is needed for reverse differentiation\n        centroids_incl = torch.mean(embeds, dim=1, keepdim=True)\n        centroids_incl = centroids_incl.clone() / torch.norm(centroids_incl, dim=2, keepdim=True)\n\n        # Exclusive centroids (1 per utterance)\n        centroids_excl = (torch.sum(embeds, dim=1, keepdim=True) - embeds)\n        centroids_excl /= (utterances_per_speaker - 1)\n        centroids_excl = centroids_excl.clone() / torch.norm(centroids_excl, dim=2, keepdim=True)\n\n        # Similarity matrix computation\n        sim_matrix = torch.zeros(speakers_per_batch, utterances_per_speaker,\n                                 speakers_per_batch).to(self.loss_device)\n        mask_matrix = 1 - np.eye(speakers_per_batch, dtype=np.int64)\n        for j in range(speakers_per_batch):\n            mask = np.where(mask_matrix[j])[0]\n            sim_matrix[mask, :, j] = (embeds[mask] * centroids_incl[j]).sum(dim=2)\n            sim_matrix[j, :, j] = (embeds[j] * centroids_excl[j]).sum(dim=1)\n\n        sim_matrix = sim_matrix * self.similarity_weight + self.similarity_bias\n        return sim_matrix\n\n    def loss(self, embeds):\n        \"\"\"\n        Computes the softmax loss according the section 2.1 of GE2E.\n\n        :param embeds: the embeddings as a tensor of shape (speakers_per_batch,\n        utterances_per_speaker, embedding_size)\n        :return: the loss and the EER for this batch of embeddings.\n        \"\"\"\n        speakers_per_batch, utterances_per_speaker = embeds.shape[:2]\n\n        # Loss\n        sim_matrix = self.similarity_matrix(embeds)\n        sim_matrix = sim_matrix.reshape((speakers_per_batch * utterances_per_speaker,\n                                         speakers_per_batch))\n        ground_truth = np.repeat(np.arange(speakers_per_batch), utterances_per_speaker)\n        target = torch.from_numpy(ground_truth).long().to(self.loss_device)\n        loss = self.loss_fn(sim_matrix, target)\n\n        # EER (not backpropagated)\n        with torch.no_grad():\n            inv_argmax = lambda i: np.eye(1, speakers_per_batch, i, dtype=np.int64)[0]\n            labels = np.array([inv_argmax(i) for i in ground_truth])\n            preds = sim_matrix.detach().cpu().numpy()\n\n            # Snippet from https://yangcha.github.io/EER-ROC/\n            fpr, tpr, thresholds = roc_curve(labels.flatten(), preds.flatten())\n            eer = brentq(lambda x: 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n\n        return loss, eer","metadata":{"execution":{"iopub.status.busy":"2024-09-19T04:55:22.207747Z","iopub.execute_input":"2024-09-19T04:55:22.208196Z","iopub.status.idle":"2024-09-19T04:55:22.235419Z","shell.execute_reply.started":"2024-09-19T04:55:22.208149Z","shell.execute_reply":"2024-09-19T04:55:22.234283Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"from pathlib import Path\nimport logging\nimport random\nimport time\nimport torch\nimport numpy as np\n\ndef evaluate(model, loader, total_evaluate_steps=50, device='cpu', loss_device='cpu'):\n    steps = 0\n    losses = []\n    eers = []\n    start_time = time.time()\n    while True:\n        if (steps+1) > total_evaluate_steps:\n            break\n\n        for batch in loader:\n            if (steps+1) > total_evaluate_steps:\n                break\n\n            n_speakers, n_utterances, freq_len, tempo_len = batch.shape\n            data = batch.view(-1, freq_len, tempo_len)\n            data = data.transpose(1, 2)\n            model.eval()\n            embeds = model(data.to(device))\n            embeds = embeds.view(n_speakers, n_utterances, -1)\n            loss, eer = model.loss(embeds.to(loss_device))\n            losses.append(loss.detach().numpy())\n            eers.append(eer)\n            steps += 1\n\n    mean_loss = np.mean(losses)\n    mean_eer = np.mean(eers)\n    print(f'Evaluate Mean Loss {mean_loss:.3f}, Mean EER {mean_eer:.3f} - Time: {(time.time() - start_time):.3f}s')\n\ndef train():\n    print('Loading data...')\n    libri_dataset = load_librispeech360_dataset(libri_dataset_path)\n    print('Finish to load LibriSpeech360h')\n    vivos_dataset = load_vivos_dataset(Path(r'/kaggle/input/vivos-dataset/vivos'))\n    print('Finish to load Vivos')\n    aishell3_dataset = load_aishell3_dataset(Path(r'/kaggle/input/paddle-speech/AISHELL-3'))\n    print('Finish to load AISHELL-3')\n    voxceleb_dataset = load_voxceleb_dataset(Path(r'/kaggle/input/voxceleb1train/wav'))\n    print('Finish to load Voxceleb')\n\n    datasets = [libri_dataset, aishell3_dataset, vivos_dataset, voxceleb_dataset]\n    mds = MultiAudioDataset(datasets)\n    random.shuffle(mds.speakers)\n    train_speakers = mds.speakers[:-50]\n    eval_speakers = mds.speakers[-50:]\n\n    ds = SpeakerDataset(train_speakers,\n                        utterances_per_speaker=utterances_per_speaker,\n                        seq_len=seq_len)\n    loader = torch.utils.data.DataLoader(ds,\n                                        batch_size=speakers_per_batch,\n                                        shuffle=True,\n                                        num_workers=4)\n\n    eval_ds = SpeakerDataset(eval_speakers,\n                        utterances_per_speaker=utterances_per_speaker,\n                        seq_len=seq_len)\n    eval_loader = torch.utils.data.DataLoader(eval_ds,\n                                        batch_size=speakers_per_batch,\n                                        shuffle=True,\n                                        num_workers=4)\n    \n    dv = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    loss_dv = torch.device(loss_device)\n    model = SpeakerEncoder(device=dv, loss_device=loss_dv)\n    \n    opt = torch.optim.Adam(model.parameters(), lr=speaker_lr)\n\n    total_steps = 0\n\n    ckpts = sorted(list(Path(save_dir).glob('*.pt')))\n    if len(ckpts) > 0:\n        latest_ckpt_path = ckpts[-1]\n        ckpt = torch.load(latest_ckpt_path, weights_only=False)\n        if ckpt:\n            print(f'loading ckpt {latest_ckpt_path}')\n            model.load_state_dict(ckpt['model_state_dict'])\n            opt.load_state_dict(ckpt['optimizer_state_dict'])\n            total_steps = ckpt['total_steps']\n#     ckpt = torch.load(\"/kaggle/working/000000001600.pt\")\n#     if ckpt:\n# #         print(f'loading ckpt {latest_ckpt_path}')\n#         model.load_state_dict(ckpt['model_state_dict'])\n#         opt.load_state_dict(ckpt['optimizer_state_dict'])\n#         total_steps = ckpt['total_steps']\n\n    print(\"Start training . . .\")\n    while True:\n        if total_steps >= train_steps:\n            break\n\n        for batch in loader:\n            if total_steps >= train_steps:\n                break\n            start_time = time.time()\n            for g in opt.param_groups:\n                g['lr'] = speaker_lr\n            n_speakers, n_utterances, freq_len, tempo_len = batch.shape\n            data = batch.view(-1, freq_len, tempo_len)\n            data = data.transpose(1, 2)\n\n            model.train()\n            opt.zero_grad()\n\n            embeds = model(data.to(dv))\n            embeds = embeds.view(n_speakers, n_utterances, -1)\n            loss, eer = model.loss(embeds.to(loss_device))\n\n            loss.backward()\n            model.do_gradient_ops()\n            opt.step()\n\n            total_steps += 1\n\n            if (total_steps+1) % train_print_interval == 0:\n                print(f'Step {total_steps+1} Loss {loss:.3f}, EER {eer:.3f} - Time: {(time.time() - start_time):.3f}s')\n            if (total_steps+1) % save_interval == 0:\n                if not Path(save_dir).exists():\n                    Path(save_dir).mkdir()\n                save_path = Path(save_dir) / f'{total_steps+1:012d}.pt'\n                print(f'saving ckpt {save_path}')\n                torch.save({\n                    'model_state_dict': model.state_dict(),\n                    'optimizer_state_dict': opt.state_dict(),\n                    'total_steps': total_steps\n                }, save_path)\n\n                # remove old ckpts\n                ckpts = sorted(list(Path(save_dir).glob('*.pt')))\n                if len(ckpts) > max_ckpts:\n                    for ckpt in ckpts[:-max_ckpts]:\n                        Path(ckpt).unlink()\n                        print(f'ckpt {ckpt} removed')\n            if (total_steps+1) % evaluate_interval == 0:\n                evaluate(model, eval_loader, total_evaluate_steps=50, device=dv, loss_device=loss_dv)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-19T04:55:22.964087Z","iopub.execute_input":"2024-09-19T04:55:22.964505Z","iopub.status.idle":"2024-09-19T04:55:22.990507Z","shell.execute_reply.started":"2024-09-19T04:55:22.964464Z","shell.execute_reply":"2024-09-19T04:55:22.989409Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"torch.cuda.is_available()","metadata":{"execution":{"iopub.status.busy":"2024-09-19T04:55:23.970106Z","iopub.execute_input":"2024-09-19T04:55:23.970816Z","iopub.status.idle":"2024-09-19T04:55:23.977388Z","shell.execute_reply.started":"2024-09-19T04:55:23.970772Z","shell.execute_reply":"2024-09-19T04:55:23.976407Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"train()","metadata":{"execution":{"iopub.status.busy":"2024-09-19T04:55:24.693501Z","iopub.execute_input":"2024-09-19T04:55:24.693959Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Loading data...\nFinish to load LibriSpeech360h\nFinish to load Vivos\nFinish to load AISHELL-3\nFinish to load Voxceleb\nStart training . . .\nStep 10 Loss 3.052, EER 0.219 - Time: 0.441s\nStep 20 Loss 2.443, EER 0.200 - Time: 0.560s\nStep 30 Loss 1.919, EER 0.149 - Time: 0.575s\nStep 40 Loss 1.892, EER 0.159 - Time: 0.801s\nStep 50 Loss 1.750, EER 0.148 - Time: 0.912s\nStep 60 Loss 1.753, EER 0.146 - Time: 0.833s\nStep 70 Loss 1.555, EER 0.128 - Time: 0.358s\nStep 80 Loss 1.638, EER 0.144 - Time: 0.852s\nStep 90 Loss 1.406, EER 0.134 - Time: 0.788s\nStep 100 Loss 1.379, EER 0.115 - Time: 0.619s\nsaving ckpt /kaggle/working/000000000100.pt\nStep 110 Loss 1.520, EER 0.140 - Time: 0.429s\nStep 120 Loss 1.568, EER 0.131 - Time: 0.516s\nStep 130 Loss 1.285, EER 0.106 - Time: 0.588s\nStep 140 Loss 1.527, EER 0.141 - Time: 0.764s\nStep 150 Loss 1.356, EER 0.111 - Time: 0.964s\nStep 160 Loss 1.480, EER 0.124 - Time: 0.269s\nStep 170 Loss 1.542, EER 0.156 - Time: 0.849s\nStep 180 Loss 1.550, EER 0.146 - Time: 0.815s\nStep 190 Loss 1.127, EER 0.093 - Time: 0.486s\nStep 200 Loss 1.390, EER 0.134 - Time: 0.772s\nsaving ckpt /kaggle/working/000000000200.pt\nStep 210 Loss 1.296, EER 0.119 - Time: 0.859s\nStep 220 Loss 1.282, EER 0.120 - Time: 0.563s\nStep 230 Loss 1.448, EER 0.128 - Time: 0.508s\nStep 240 Loss 1.362, EER 0.119 - Time: 0.494s\nStep 250 Loss 1.247, EER 0.124 - Time: 0.672s\nStep 260 Loss 1.261, EER 0.109 - Time: 0.463s\nStep 270 Loss 1.310, EER 0.119 - Time: 0.639s\nStep 280 Loss 1.126, EER 0.103 - Time: 0.701s\nStep 290 Loss 1.286, EER 0.104 - Time: 0.812s\nStep 300 Loss 1.499, EER 0.136 - Time: 0.568s\nsaving ckpt /kaggle/working/000000000300.pt\nStep 310 Loss 1.132, EER 0.094 - Time: 0.636s\nStep 320 Loss 1.506, EER 0.143 - Time: 0.698s\nStep 330 Loss 0.876, EER 0.087 - Time: 0.673s\nStep 340 Loss 1.214, EER 0.100 - Time: 0.472s\nStep 350 Loss 1.279, EER 0.115 - Time: 0.532s\nStep 360 Loss 1.061, EER 0.088 - Time: 0.686s\nStep 370 Loss 1.256, EER 0.125 - Time: 0.218s\nStep 380 Loss 1.154, EER 0.093 - Time: 0.375s\nStep 390 Loss 1.181, EER 0.108 - Time: 0.677s\nStep 400 Loss 1.017, EER 0.086 - Time: 0.402s\nsaving ckpt /kaggle/working/000000000400.pt\nStep 410 Loss 1.016, EER 0.098 - Time: 0.484s\nStep 420 Loss 1.158, EER 0.109 - Time: 0.817s\nStep 430 Loss 0.987, EER 0.090 - Time: 0.618s\nStep 440 Loss 1.103, EER 0.097 - Time: 0.372s\nStep 450 Loss 1.174, EER 0.094 - Time: 0.448s\nStep 460 Loss 1.204, EER 0.097 - Time: 0.506s\nStep 470 Loss 1.028, EER 0.101 - Time: 0.678s\nStep 480 Loss 1.263, EER 0.119 - Time: 0.819s\nStep 490 Loss 1.426, EER 0.142 - Time: 0.349s\nStep 500 Loss 1.019, EER 0.094 - Time: 0.781s\nsaving ckpt /kaggle/working/000000000500.pt\nEvaluate Mean Loss 0.965, Mean EER 0.100 - Time: 540.915s\nStep 510 Loss 1.037, EER 0.090 - Time: 1.014s\nStep 520 Loss 1.233, EER 0.113 - Time: 0.543s\nStep 530 Loss 1.210, EER 0.099 - Time: 0.412s\nStep 540 Loss 1.186, EER 0.122 - Time: 0.956s\nStep 550 Loss 1.178, EER 0.092 - Time: 0.874s\nStep 560 Loss 1.251, EER 0.119 - Time: 0.657s\nStep 570 Loss 1.511, EER 0.168 - Time: 0.776s\nStep 580 Loss 0.972, EER 0.088 - Time: 0.956s\nStep 590 Loss 1.172, EER 0.109 - Time: 0.631s\nStep 600 Loss 1.341, EER 0.116 - Time: 0.863s\nsaving ckpt /kaggle/working/000000000600.pt\nStep 610 Loss 1.188, EER 0.107 - Time: 0.806s\nStep 620 Loss 1.305, EER 0.116 - Time: 0.918s\nStep 630 Loss 0.949, EER 0.084 - Time: 0.694s\nStep 640 Loss 0.996, EER 0.094 - Time: 0.444s\nStep 650 Loss 1.115, EER 0.109 - Time: 0.494s\nStep 660 Loss 0.882, EER 0.081 - Time: 0.825s\nStep 670 Loss 0.881, EER 0.078 - Time: 0.401s\nStep 680 Loss 1.003, EER 0.088 - Time: 0.451s\nStep 690 Loss 1.073, EER 0.106 - Time: 0.890s\nStep 700 Loss 0.780, EER 0.075 - Time: 0.819s\nsaving ckpt /kaggle/working/000000000700.pt\nStep 710 Loss 1.096, EER 0.112 - Time: 0.430s\nStep 720 Loss 0.855, EER 0.071 - Time: 0.454s\nStep 730 Loss 0.910, EER 0.083 - Time: 0.572s\nStep 740 Loss 0.957, EER 0.084 - Time: 0.185s\nStep 750 Loss 1.049, EER 0.084 - Time: 0.958s\nStep 760 Loss 0.846, EER 0.081 - Time: 0.530s\nStep 770 Loss 1.194, EER 0.103 - Time: 0.610s\nStep 780 Loss 1.360, EER 0.119 - Time: 0.329s\nStep 790 Loss 0.847, EER 0.077 - Time: 0.851s\nStep 800 Loss 0.786, EER 0.072 - Time: 0.639s\nsaving ckpt /kaggle/working/000000000800.pt\nStep 810 Loss 1.061, EER 0.088 - Time: 0.348s\nStep 820 Loss 1.060, EER 0.098 - Time: 0.585s\nStep 830 Loss 0.889, EER 0.082 - Time: 0.713s\nStep 840 Loss 1.019, EER 0.087 - Time: 0.741s\nStep 850 Loss 1.182, EER 0.103 - Time: 0.644s\nStep 860 Loss 0.636, EER 0.050 - Time: 0.885s\nStep 870 Loss 1.061, EER 0.097 - Time: 0.872s\nStep 880 Loss 1.294, EER 0.122 - Time: 0.362s\nStep 890 Loss 0.928, EER 0.091 - Time: 0.405s\nStep 900 Loss 1.085, EER 0.096 - Time: 0.804s\nsaving ckpt /kaggle/working/000000000900.pt\nStep 910 Loss 1.065, EER 0.094 - Time: 0.416s\nStep 920 Loss 0.857, EER 0.078 - Time: 0.679s\nStep 930 Loss 1.012, EER 0.087 - Time: 0.840s\nStep 940 Loss 0.990, EER 0.094 - Time: 0.345s\nStep 950 Loss 0.928, EER 0.072 - Time: 0.694s\nStep 960 Loss 0.989, EER 0.085 - Time: 0.386s\nStep 970 Loss 0.836, EER 0.072 - Time: 0.747s\nStep 980 Loss 0.813, EER 0.086 - Time: 0.791s\nStep 990 Loss 0.959, EER 0.088 - Time: 0.235s\nStep 1000 Loss 0.987, EER 0.100 - Time: 0.494s\nsaving ckpt /kaggle/working/000000001000.pt\nEvaluate Mean Loss 0.832, Mean EER 0.085 - Time: 495.621s\nStep 1010 Loss 0.981, EER 0.095 - Time: 0.811s\nStep 1020 Loss 0.794, EER 0.067 - Time: 0.816s\nStep 1030 Loss 0.924, EER 0.087 - Time: 0.425s\nStep 1040 Loss 0.880, EER 0.084 - Time: 0.543s\nStep 1050 Loss 0.896, EER 0.081 - Time: 0.606s\nStep 1060 Loss 0.747, EER 0.066 - Time: 0.615s\nStep 1070 Loss 0.839, EER 0.078 - Time: 0.631s\nStep 1080 Loss 1.047, EER 0.097 - Time: 0.866s\nStep 1090 Loss 0.932, EER 0.095 - Time: 0.785s\nStep 1100 Loss 0.945, EER 0.088 - Time: 0.447s\nsaving ckpt /kaggle/working/000000001100.pt\nStep 1110 Loss 1.000, EER 0.088 - Time: 0.253s\nStep 1120 Loss 1.105, EER 0.103 - Time: 0.529s\nStep 1130 Loss 0.878, EER 0.080 - Time: 0.688s\nStep 1140 Loss 1.067, EER 0.099 - Time: 0.490s\nStep 1150 Loss 1.026, EER 0.089 - Time: 0.751s\nStep 1160 Loss 0.909, EER 0.097 - Time: 0.715s\nStep 1170 Loss 0.818, EER 0.078 - Time: 0.360s\nStep 1180 Loss 0.870, EER 0.078 - Time: 0.379s\nStep 1190 Loss 0.939, EER 0.070 - Time: 0.286s\nStep 1200 Loss 0.941, EER 0.088 - Time: 0.598s\nsaving ckpt /kaggle/working/000000001200.pt\nStep 1210 Loss 0.678, EER 0.064 - Time: 0.830s\nStep 1220 Loss 1.022, EER 0.100 - Time: 0.804s\nStep 1230 Loss 0.873, EER 0.075 - Time: 0.492s\nStep 1240 Loss 0.963, EER 0.093 - Time: 0.691s\nStep 1250 Loss 0.690, EER 0.056 - Time: 0.532s\nStep 1260 Loss 0.972, EER 0.087 - Time: 0.321s\nStep 1270 Loss 0.829, EER 0.076 - Time: 0.437s\nStep 1280 Loss 0.727, EER 0.059 - Time: 0.418s\nStep 1290 Loss 0.609, EER 0.049 - Time: 0.481s\nStep 1300 Loss 0.938, EER 0.091 - Time: 0.319s\nsaving ckpt /kaggle/working/000000001300.pt\nStep 1310 Loss 0.977, EER 0.083 - Time: 0.648s\nStep 1320 Loss 0.872, EER 0.077 - Time: 0.717s\nStep 1330 Loss 0.946, EER 0.092 - Time: 0.294s\nStep 1340 Loss 0.944, EER 0.081 - Time: 0.895s\nStep 1350 Loss 1.063, EER 0.101 - Time: 0.534s\nStep 1360 Loss 0.854, EER 0.079 - Time: 0.704s\nStep 1370 Loss 1.142, EER 0.099 - Time: 0.985s\nStep 1380 Loss 1.035, EER 0.102 - Time: 0.875s\nStep 1390 Loss 1.003, EER 0.097 - Time: 0.355s\nStep 1400 Loss 0.983, EER 0.074 - Time: 0.706s\nsaving ckpt /kaggle/working/000000001400.pt\nStep 1410 Loss 0.831, EER 0.080 - Time: 0.357s\nStep 1420 Loss 0.853, EER 0.073 - Time: 0.417s\nStep 1430 Loss 0.796, EER 0.080 - Time: 0.865s\nStep 1440 Loss 0.685, EER 0.063 - Time: 0.531s\nStep 1450 Loss 0.888, EER 0.085 - Time: 0.398s\nStep 1460 Loss 0.924, EER 0.082 - Time: 0.774s\nStep 1470 Loss 0.812, EER 0.072 - Time: 0.794s\nStep 1480 Loss 0.874, EER 0.081 - Time: 0.161s\nStep 1490 Loss 0.982, EER 0.088 - Time: 0.466s\nStep 1500 Loss 1.041, EER 0.093 - Time: 0.595s\nsaving ckpt /kaggle/working/000000001500.pt\nEvaluate Mean Loss 0.787, Mean EER 0.079 - Time: 510.302s\nStep 1510 Loss 0.842, EER 0.080 - Time: 0.741s\nStep 1520 Loss 0.614, EER 0.058 - Time: 0.778s\nStep 1530 Loss 0.869, EER 0.084 - Time: 0.530s\nStep 1540 Loss 0.844, EER 0.063 - Time: 0.481s\nStep 1550 Loss 0.768, EER 0.066 - Time: 0.486s\nStep 1560 Loss 1.038, EER 0.094 - Time: 0.554s\nStep 1570 Loss 0.898, EER 0.077 - Time: 0.407s\nStep 1580 Loss 1.071, EER 0.094 - Time: 0.682s\nStep 1590 Loss 0.735, EER 0.075 - Time: 0.641s\nStep 1600 Loss 0.874, EER 0.081 - Time: 0.842s\nsaving ckpt /kaggle/working/000000001600.pt\nStep 1610 Loss 0.802, EER 0.072 - Time: 0.447s\nStep 1620 Loss 0.933, EER 0.081 - Time: 0.691s\nStep 1630 Loss 0.678, EER 0.057 - Time: 0.380s\nStep 1640 Loss 0.914, EER 0.094 - Time: 0.892s\nStep 1650 Loss 0.868, EER 0.081 - Time: 0.534s\nStep 1660 Loss 0.739, EER 0.072 - Time: 0.775s\nStep 1670 Loss 0.829, EER 0.075 - Time: 0.463s\nStep 1680 Loss 0.633, EER 0.055 - Time: 0.721s\nStep 1690 Loss 0.865, EER 0.081 - Time: 0.250s\nStep 1700 Loss 0.941, EER 0.081 - Time: 0.437s\nsaving ckpt /kaggle/working/000000001700.pt\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[16], line 98\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m total_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m train_steps:\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m total_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m train_steps:\n\u001b[1;32m    100\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1327\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1324\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1327\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1328\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1329\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1330\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1293\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1289\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1290\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1291\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1292\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1293\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1294\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1295\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1131\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1119\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1120\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1128\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1129\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1131\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1132\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1133\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1134\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1135\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/multiprocessing/queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[1;32m    112\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m deadline \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n","File \u001b[0;32m/opt/conda/lib/python3.10/multiprocessing/connection.py:257\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/multiprocessing/connection.py:424\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[0;32m--> 424\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n","File \u001b[0;32m/opt/conda/lib/python3.10/multiprocessing/connection.py:931\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    928\u001b[0m     deadline \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 931\u001b[0m     ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    932\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[1;32m    933\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [key\u001b[38;5;241m.\u001b[39mfileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n","File \u001b[0;32m/opt/conda/lib/python3.10/selectors.py:416\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"import os \n\nroot = r'/kaggle/working/'\nfor file in os.listdir(root):\n    if file.split('.')[-1] == 'pt':\n        os.remove(os.path.join(root, file))","metadata":{"execution":{"iopub.status.busy":"2024-09-19T04:55:07.205016Z","iopub.execute_input":"2024-09-19T04:55:07.205906Z","iopub.status.idle":"2024-09-19T04:55:07.211722Z","shell.execute_reply.started":"2024-09-19T04:55:07.205860Z","shell.execute_reply":"2024-09-19T04:55:07.210747Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}