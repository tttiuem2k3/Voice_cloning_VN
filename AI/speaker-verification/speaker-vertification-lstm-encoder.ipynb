{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-05T09:17:30.310811Z",
     "iopub.status.busy": "2024-09-05T09:17:30.310253Z",
     "iopub.status.idle": "2024-09-05T09:17:30.323798Z",
     "shell.execute_reply": "2024-09-05T09:17:30.322837Z",
     "shell.execute_reply.started": "2024-09-05T09:17:30.310776Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "speakers_per_batch=32\n",
    "utterances_per_speaker=10\n",
    "seq_len=128\n",
    "train_steps = 1e12\n",
    "train_print_interval = 10 # in steps\n",
    "total_evaluate_steps = 50\n",
    "evaluate_interval = 500 # in steps\n",
    "save_interval = 100 # in steps\n",
    "save_dir = Path(r'/kaggle/working/')\n",
    "max_ckpts = 30\n",
    "speaker_lr = 1e-4\n",
    "libri_dataset_path = Path(r'/kaggle/input/librispeech-360-clean/LibriSpeech/train-clean-360')\n",
    "device = 'cuda:0'\n",
    "loss_device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-09-05T09:17:30.325726Z",
     "iopub.status.busy": "2024-09-05T09:17:30.325356Z",
     "iopub.status.idle": "2024-09-05T09:17:30.343452Z",
     "shell.execute_reply": "2024-09-05T09:17:30.342496Z",
     "shell.execute_reply.started": "2024-09-05T09:17:30.325700Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "def normalize(S, min_level_db=-100):\n",
    "    return np.clip((S - min_level_db) / -min_level_db, 0, 1)\n",
    "\n",
    "def linear_to_mel(spectrogram, sample_rate=16000, n_fft=1024, fmin=90, fmax=7600, n_mels=80):\n",
    "    return librosa.feature.melspectrogram(\n",
    "        S=spectrogram, sr=sample_rate, n_fft=n_fft, n_mels=n_mels, fmin=fmin, fmax=fmax)\n",
    "\n",
    "def amp_to_db(x):\n",
    "    return 20. * np.log10(np.maximum(1e-5, x))\n",
    "\n",
    "def stft(y, n_fft=1024, hop_length=256, win_length=1024):\n",
    "    return librosa.stft(\n",
    "        y=y,\n",
    "        n_fft=n_fft, hop_length=hop_length, win_length=win_length)\n",
    "\n",
    "def gen_melspectrogram(y):\n",
    "    D = stft(y)\n",
    "    S = amp_to_db(linear_to_mel(np.abs(D)))\n",
    "    return np.clip(normalize(S), 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-05T09:17:30.345146Z",
     "iopub.status.busy": "2024-09-05T09:17:30.344895Z",
     "iopub.status.idle": "2024-09-05T09:17:30.362380Z",
     "shell.execute_reply": "2024-09-05T09:17:30.361543Z",
     "shell.execute_reply.started": "2024-09-05T09:17:30.345123Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pickle\n",
    "import logging\n",
    "import os\n",
    "\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "class Utterance(object):\n",
    "    def __init__(self, id: str = None, raw_file: Path = None):\n",
    "        self.id = id\n",
    "        self.raw_file = raw_file\n",
    "    def raw(self, sr=16000, augment=False):\n",
    "        \"\"\"Get the raw audio samples.\"\"\"\n",
    "\n",
    "        y, sr = librosa.load(self.raw_file, sr=sr)\n",
    "        # y, _ = librosa.effects.trim(y)\n",
    "        if y.size == 0:\n",
    "            raise Exception('audio', 'empty audio')\n",
    "        y = 0.95 * librosa.util.normalize(y)\n",
    "        if augment:\n",
    "            amplitude = np.random.uniform(low=0.3, high=1.0)\n",
    "            y = y * amplitude\n",
    "        return y\n",
    "\n",
    "    def melspectrogram(self, sr=16000, n_fft=1024, hop_length=256, win_length=1024, n_mels=40):\n",
    "        \"\"\"Get the melspectrogram features.\"\"\"\n",
    "\n",
    "        try:\n",
    "            return gen_melspectrogram(self.raw(sr=sr))\n",
    "        except Exception:\n",
    "            logging.debug(f'failed to load melspectrogram, raw file: {self.raw_file}, mel file: {self.mel_file}')\n",
    "            raise\n",
    "\n",
    "    def random_raw_segment(self, seq_len):\n",
    "        \"\"\"Return a audio segment randomly.\"\"\"\n",
    "\n",
    "        y = self.raw(augment=True)\n",
    "        ylen = len(y)\n",
    "        if ylen < seq_len:\n",
    "            pad_left = (seq_len - ylen) // 2\n",
    "            pad_right = seq_len - ylen - pad_left\n",
    "            y = np.pad(y, ((pad_left, pad_right)), mode='reflect')\n",
    "        elif ylen > seq_len:\n",
    "            max_seq_start = ylen - seq_len\n",
    "            seq_start = np.random.randint(0, max_seq_start)\n",
    "            seq_end = seq_start + seq_len\n",
    "            y = y[seq_start:seq_end]\n",
    "\n",
    "        return y\n",
    "\n",
    "    def random_mel_segment(self, seq_len):\n",
    "        \"\"\"Return a melspectrogram segment randomly.\"\"\"\n",
    "\n",
    "        mel = self.melspectrogram()\n",
    "        freq_len, tempo_len = mel.shape\n",
    "        if tempo_len < seq_len:\n",
    "            pad_left = (seq_len - tempo_len) // 2\n",
    "            pad_right = seq_len - tempo_len - pad_left\n",
    "            mel = np.pad(mel, ((0, 0), (pad_left, pad_right)), mode='reflect')\n",
    "        elif tempo_len > seq_len:\n",
    "            max_seq_start = tempo_len - seq_len\n",
    "            seq_start = np.random.randint(0, max_seq_start)\n",
    "            seq_end = seq_start + seq_len\n",
    "            mel = mel[:, seq_start:seq_end]\n",
    "        return mel\n",
    "\n",
    "class Speaker(object):\n",
    "    def __init__(self, id: str):\n",
    "        self.id = id\n",
    "        self.utterances = []\n",
    "\n",
    "    def add_utterance(self, utterance: Utterance):\n",
    "        \"\"\"Add an utterance to this speaker.\"\"\"\n",
    "\n",
    "        self.utterances.append(utterance)\n",
    "\n",
    "    def random_utterances(self, n):\n",
    "        \"\"\"Return n utterances randomly.\"\"\"\n",
    "\n",
    "        return [self.utterances[idx] for idx in np.random.randint(0, len(self.utterances), n)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-05T09:17:30.364594Z",
     "iopub.status.busy": "2024-09-05T09:17:30.364082Z",
     "iopub.status.idle": "2024-09-05T09:17:33.759409Z",
     "shell.execute_reply": "2024-09-05T09:17:33.758608Z",
     "shell.execute_reply.started": "2024-09-05T09:17:30.364561Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import logging\n",
    "from multiprocessing import Process, JoinableQueue\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class AudioDataset(object):\n",
    "    def __init__(self, id: str, speakers: List[Speaker] = []):\n",
    "        self.id = id\n",
    "        self.speakers = speakers\n",
    "\n",
    "    def add_speaker(self, speaker: Speaker):\n",
    "        \"\"\"Add a speaker to this dataset.\"\"\"\n",
    "\n",
    "        self.speakers.append(speaker)\n",
    "\n",
    "    def random_speakers(self, n):\n",
    "        \"\"\"Return n speakers randomly.\"\"\"\n",
    "\n",
    "        return [self.speakers[idx] for idx in np.random.randint(0, len(self.speakers), n)]\n",
    "\n",
    "    def serialize_speaker(self, queue: JoinableQueue, counter_queue: JoinableQueue):\n",
    "        while True:\n",
    "            speaker, root, overwrite = queue.get()\n",
    "\n",
    "            if not root.exists():\n",
    "                root.mkdir(parents=True)\n",
    "\n",
    "            dsdir = root / self.id\n",
    "            if not dsdir.exists():\n",
    "                dsdir.mkdir()\n",
    "\n",
    "            spkdir = dsdir / speaker.id\n",
    "            if not spkdir.exists():\n",
    "                spkdir.mkdir()\n",
    "\n",
    "            for uttrn_idx, uttrn in enumerate(speaker.utterances):\n",
    "                uttrnpath = spkdir / (uttrn.id + '.pkl')\n",
    "                is_overwrite = False\n",
    "                is_empty = False\n",
    "                if uttrnpath.exists():\n",
    "                    if os.path.getsize(uttrnpath) == 0:\n",
    "                        logging.debug(f'overrite empty file {uttrnpath}')\n",
    "                    elif not overwrite:\n",
    "                        logging.debug(f'{uttrnpath} already exists, skip')\n",
    "                        counter_queue.put(1)\n",
    "                        continue\n",
    "                    is_overwrite = True\n",
    "                try:\n",
    "                    mel = uttrn.melspectrogram()\n",
    "                    with uttrnpath.open(mode='wb') as f:\n",
    "                        pickle.dump(mel, f)\n",
    "                    if is_overwrite:\n",
    "                        logging.debug(f'dump pickle object to {uttrnpath} ({uttrn_idx+1}/{len(speaker.utterances)}), overwrite')\n",
    "                    else:\n",
    "                        logging.debug(f'dump pickle object to {uttrnpath} ({uttrn_idx+1}/{len(speaker.utterances)})')\n",
    "                except Exception as err:\n",
    "                    logging.warning(f'failed to dump mel features for file {uttrnpath}: {err}')\n",
    "                counter_queue.put(1)\n",
    "            queue.task_done()\n",
    "\n",
    "    def serialization_counter(self, total_count, queue: JoinableQueue):\n",
    "        count = 0\n",
    "        while True:\n",
    "            start_time = time.time()\n",
    "            done = queue.get()\n",
    "            duration = time.time() - start_time\n",
    "            count += 1\n",
    "            logging.debug(f'serialization progress {count}/{total_count}, {int(duration*1000)}ms/item')\n",
    "            queue.task_done()\n",
    "\n",
    "    def serialize_mel_feature(self, root: Path, overwrite=False):\n",
    "        \"\"\"Serialize melspectrogram features for all utterances of all speakers to the disk.\"\"\"\n",
    "\n",
    "        num_processes = 8\n",
    "        queue = JoinableQueue()\n",
    "        counter_queue = JoinableQueue()\n",
    "        processes = []\n",
    "        for i in range(num_processes):\n",
    "            p = Process(target=self.serialize_speaker, args=(queue, counter_queue))\n",
    "            processes.append(p)\n",
    "            p.start()\n",
    "        total_count = sum([len(spk.utterances) for spk in self.speakers])\n",
    "        counter_process = Process(target=self.serialization_counter, args=(total_count, counter_queue))\n",
    "        counter_process.start()\n",
    "        # add tasks to queue\n",
    "        logging.debug(f'total {len(self.speakers)} speakers')\n",
    "        for spk in self.speakers:\n",
    "            queue.put((spk, root, overwrite)) \n",
    "        # wait for all task done\n",
    "        queue.join() \n",
    "        counter_queue.join()\n",
    "        for p in processes:\n",
    "            p.terminate()\n",
    "        counter_process.terminate()\n",
    "\n",
    "class MultiAudioDataset(object):\n",
    "    def __init__(self, datasets: List[AudioDataset]):\n",
    "        self.id = ''\n",
    "        self.speakers = []\n",
    "        ids = []\n",
    "        for ds in datasets:\n",
    "            ids.append(ds.id)\n",
    "            self.speakers.extend(ds.speakers)\n",
    "        self.id = '+'.join(ids)\n",
    "\n",
    "class SpeakerDataset(object):\n",
    "    def __init__(self, speakers, utterances_per_speaker, seq_len):\n",
    "        self.speakers = speakers\n",
    "        n_speakers = len(self.speakers)\n",
    "        n_utterances = sum([len(spk.utterances) for spk in self.speakers])\n",
    "        logging.info(f'total {n_speakers} speakers, {n_utterances} utterances')\n",
    "        self.utterances_per_speaker = utterances_per_speaker\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def random_utterance_segment(self, speaker_idx, seq_len):\n",
    "        \"\"\"Must return an utterance segment as long as the speaker has at least\n",
    "        one effective utterance.\"\"\"\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                utterance = self.speakers[speaker_idx].random_utterances(1)[0]\n",
    "                return utterance.random_mel_segment(seq_len)\n",
    "            except Exception as err:\n",
    "                logging.debug(f'failed to load utterances of speaker idx {speaker_idx}: {err}')\n",
    "                continue\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Return random segments of random utterances for the specified speaker.\"\"\"\n",
    "        seq_len = 0\n",
    "        if isinstance(self.seq_len, int):\n",
    "            seq_len = self.seq_len\n",
    "        elif isinstance(self.seq_len, list):\n",
    "            seq_len = self.seq_len[random.randint(0, len(self.seq_len)-1)]\n",
    "        else:\n",
    "            raise ValueError('seq_len must be int or int list')\n",
    "\n",
    "        segments = np.array([self.random_utterance_segment(idx, seq_len) for _ in range(self.utterances_per_speaker)])\n",
    "        return torch.tensor(segments)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.speakers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-05T09:17:33.761768Z",
     "iopub.status.busy": "2024-09-05T09:17:33.761177Z",
     "iopub.status.idle": "2024-09-05T09:17:33.770213Z",
     "shell.execute_reply": "2024-09-05T09:17:33.769319Z",
     "shell.execute_reply.started": "2024-09-05T09:17:33.761733Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def load_librispeech360_dataset(root: Path):\n",
    "    \"\"\"Load the LibriSpeech train-clean-360 dataset into an AudioDataset.\n",
    "\n",
    "    The dataset can be downloaded from: https://www.openslr.org/12\n",
    "\n",
    "    Args:\n",
    "        root (Path): Path to the root directory of the LibriSpeech dataset.\n",
    "        mel_feature_root (Path, optional): Path to the root directory where the precomputed mel features are stored.\n",
    "\n",
    "    Returns:\n",
    "        AudioDataset: A dataset object containing the loaded speakers and their utterances.\n",
    "    \"\"\"\n",
    "\n",
    "    dataset_id = 'librispeech360'\n",
    "    id2speaker = dict()\n",
    "\n",
    "    # Recursively find all .flac files in the dataset\n",
    "    wav_files = root.rglob('*.flac')\n",
    "    \n",
    "    for f in wav_files:\n",
    "        # LibriSpeech files are typically structured as: <root>/<speaker_id>/<chapter_id>/<utterance_id>.flac\n",
    "        speaker_id = f.parent.parent.name  # Extract speaker ID from the parent folder\n",
    "        chapter_id = f.parent.name  # Extract chapter ID from the immediate parent folder\n",
    "        utterance_id = f.stem  # Use the file stem as the utterance ID (without .flac extension)\n",
    "\n",
    "        uttrn = Utterance(utterance_id, raw_file=f)\n",
    "\n",
    "        if speaker_id in id2speaker:\n",
    "            id2speaker[speaker_id].add_utterance(uttrn)\n",
    "        else:\n",
    "            spk = Speaker(speaker_id)\n",
    "            spk.add_utterance(uttrn)\n",
    "            id2speaker[speaker_id] = spk\n",
    "\n",
    "    dataset = AudioDataset(dataset_id, speakers=list(id2speaker.values()))\n",
    "    return dataset\n",
    "\n",
    "def load_vivos_dataset(root: Path):\n",
    "    \"\"\"Load the VIVOS dataset into an AudioDataset.\n",
    "\n",
    "    The dataset can be downloaded from: https://ailab.hcmus.edu.vn/vivos\n",
    "\n",
    "    Args:\n",
    "        root (Path): Path to the root directory of the VIVOS dataset.\n",
    "\n",
    "    Returns:\n",
    "        AudioDataset: A dataset object containing the loaded speakers and their utterances.\n",
    "    \"\"\"\n",
    "\n",
    "    dataset_id = 'vivos'\n",
    "    id2speaker = dict()\n",
    "\n",
    "    # Recursively find all .wav files in the dataset\n",
    "    wav_files = root.rglob('*.wav')\n",
    "    \n",
    "    for f in wav_files:\n",
    "        # VIVOS files are typically structured as: <root>/train/<speaker_id>/<utterance_id>.wav\n",
    "        speaker_id = f.parent.name  # Extract speaker ID from the parent folder\n",
    "        utterance_id = f.stem  # Use the file stem as the utterance ID (without .wav extension)\n",
    "\n",
    "        uttrn = Utterance(utterance_id, raw_file=f)\n",
    "\n",
    "        if speaker_id in id2speaker:\n",
    "            id2speaker[speaker_id].add_utterance(uttrn)\n",
    "        else:\n",
    "            spk = Speaker(speaker_id)\n",
    "            spk.add_utterance(uttrn)\n",
    "            id2speaker[speaker_id] = spk\n",
    "\n",
    "    dataset = AudioDataset(dataset_id, speakers=list(id2speaker.values()))\n",
    "    return dataset\n",
    "\n",
    "def load_aishell3_dataset(root: Path):\n",
    "    \"\"\"Load the AISHELL-3 dataset into an AudioDataset.\n",
    "\n",
    "    The dataset can be downloaded from: https://www.openslr.org/93\n",
    "\n",
    "    Args:\n",
    "        root (Path): Path to the root directory of the AISHELL-3 dataset.\n",
    "\n",
    "    Returns:\n",
    "        AudioDataset: A dataset object containing the loaded speakers and their utterances.\n",
    "    \"\"\"\n",
    "\n",
    "    dataset_id = 'aishell3'\n",
    "    id2speaker = dict()\n",
    "\n",
    "    # Recursively find all .wav files in the dataset\n",
    "    wav_files = root.rglob('*.wav')\n",
    "    \n",
    "    for f in wav_files:\n",
    "        # AISHELL-3 files are typically structured as: <root>/wav/<speaker_id>/<utterance_id>.wav\n",
    "        speaker_id = f.parent.name  # Extract speaker ID from the parent folder\n",
    "        utterance_id = f.stem  # Use the file stem as the utterance ID (without .wav extension)\n",
    "\n",
    "        uttrn = Utterance(utterance_id, raw_file=f)\n",
    "\n",
    "        if speaker_id in id2speaker:\n",
    "            id2speaker[speaker_id].add_utterance(uttrn)\n",
    "        else:\n",
    "            spk = Speaker(speaker_id)\n",
    "            spk.add_utterance(uttrn)\n",
    "            id2speaker[speaker_id] = spk\n",
    "\n",
    "    dataset = AudioDataset(dataset_id, speakers=list(id2speaker.values()))\n",
    "    return dataset\n",
    "\n",
    "def load_voxceleb_dataset(root: Path):\n",
    "    \"\"\"Load the VoxCeleb dataset into an AudioDataset.\n",
    "\n",
    "    The dataset can be downloaded from: https://www.robots.ox.ac.uk/~vgg/data/voxceleb/\n",
    "\n",
    "    Args:\n",
    "        root (Path): Path to the root directory of the VoxCeleb dataset.\n",
    "\n",
    "    Returns:\n",
    "        AudioDataset: A dataset object containing the loaded speakers and their utterances.\n",
    "    \"\"\"\n",
    "\n",
    "    dataset_id = 'voxceleb'\n",
    "    id2speaker = dict()\n",
    "\n",
    "    # Recursively find all .wav files in the dataset\n",
    "    wav_files = root.rglob('*.wav')\n",
    "    \n",
    "    for f in wav_files:\n",
    "        # VoxCeleb files are typically structured as: <root>/wav/<speaker_id>/<segment_id>/<utterance_id>.wav\n",
    "        speaker_id = f.parts[-3]  # Extract speaker ID from the third-to-last folder\n",
    "        utterance_id = f.stem  # Use the file stem as the utterance ID (without .wav extension)\n",
    "\n",
    "        uttrn = Utterance(utterance_id, raw_file=f)\n",
    "\n",
    "        if speaker_id in id2speaker:\n",
    "            id2speaker[speaker_id].add_utterance(uttrn)\n",
    "        else:\n",
    "            spk = Speaker(speaker_id)\n",
    "            spk.add_utterance(uttrn)\n",
    "            id2speaker[speaker_id] = spk\n",
    "\n",
    "    dataset = AudioDataset(dataset_id, speakers=list(id2speaker.values()))\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-05T09:17:33.771973Z",
     "iopub.status.busy": "2024-09-05T09:17:33.771633Z",
     "iopub.status.idle": "2024-09-05T09:17:34.409297Z",
     "shell.execute_reply": "2024-09-05T09:17:34.408533Z",
     "shell.execute_reply.started": "2024-09-05T09:17:33.771941Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from scipy.interpolate import interp1d\n",
    "from sklearn.metrics import roc_curve\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from scipy.optimize import brentq\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "class SpeakerEncoder(nn.Module):\n",
    "    def __init__(self, input_size=40, hidden_size=256, num_layers=3, bidirectional=True, device='cpu', loss_device='cpu'):\n",
    "        super().__init__()\n",
    "        self.loss_device = loss_device\n",
    "\n",
    "        # Network defition\n",
    "        self.lstm = nn.LSTM(input_size=input_size,\n",
    "                            hidden_size=hidden_size,\n",
    "                            num_layers=num_layers,\n",
    "                            bidirectional=bidirectional,\n",
    "                            batch_first=True).to(device)\n",
    "        self.linear = nn.Linear(in_features=hidden_size if bidirectional else hidden_size * 2,\n",
    "                                out_features=256).to(device)\n",
    "        self.relu = torch.nn.ReLU().to(device)\n",
    "\n",
    "        # Cosine similarity scaling (with fixed initial parameter values)\n",
    "        self.similarity_weight = nn.Parameter(torch.tensor([10.])).to(loss_device)\n",
    "        self.similarity_bias = nn.Parameter(torch.tensor([-5.])).to(loss_device)\n",
    "\n",
    "        # Loss\n",
    "        self.loss_fn = nn.CrossEntropyLoss().to(loss_device)\n",
    "\n",
    "    def do_gradient_ops(self):\n",
    "        # Gradient scale\n",
    "        self.similarity_weight.grad *= 0.01\n",
    "        self.similarity_bias.grad *= 0.01\n",
    "\n",
    "        # Gradient clipping\n",
    "        clip_grad_norm_(self.parameters(), 3, norm_type=2)\n",
    "\n",
    "    def forward(self, utterances, hidden_init=None):\n",
    "        \"\"\"\n",
    "        Computes the embeddings of a batch of utterance spectrograms.\n",
    "\n",
    "        :param utterances: batch of mel-scale filterbanks of same duration as a tensor of shape\n",
    "        (batch_size, n_frames, n_channels)\n",
    "        :param hidden_init: initial hidden state of the LSTM as a tensor of shape (num_layers,\n",
    "        batch_size, hidden_size). Will default to a tensor of zeros if None.\n",
    "        :return: the embeddings as a tensor of shape (batch_size, embedding_size)\n",
    "        \"\"\"\n",
    "\n",
    "        # Pass the input through the LSTM layers and retrieve all outputs, the final hidden state\n",
    "        # and the final cell state.\n",
    "        out, (hidden, cell) = self.lstm(utterances, hidden_init)\n",
    "\n",
    "        # We take only the hidden state of the last layer\n",
    "        embeds_raw = self.relu(self.linear(hidden[-1]))\n",
    "\n",
    "        # L2-normalize it\n",
    "        embeds = embeds_raw / torch.norm(embeds_raw, dim=1, keepdim=True)\n",
    "\n",
    "        return embeds\n",
    "\n",
    "    def similarity_matrix(self, embeds):\n",
    "        \"\"\"\n",
    "        Computes the similarity matrix according the section 2.1 of GE2E.\n",
    "\n",
    "        :param embeds: the embeddings as a tensor of shape (speakers_per_batch,\n",
    "        utterances_per_speaker, embedding_size)\n",
    "        :return: the similarity matrix as a tensor of shape (speakers_per_batch,\n",
    "        utterances_per_speaker, speakers_per_batch)\n",
    "        \"\"\"\n",
    "        speakers_per_batch, utterances_per_speaker = embeds.shape[:2]\n",
    "\n",
    "        # Inclusive centroids (1 per speaker). Cloning is needed for reverse differentiation\n",
    "        centroids_incl = torch.mean(embeds, dim=1, keepdim=True)\n",
    "        centroids_incl = centroids_incl.clone() / torch.norm(centroids_incl, dim=2, keepdim=True)\n",
    "\n",
    "        # Exclusive centroids (1 per utterance)\n",
    "        centroids_excl = (torch.sum(embeds, dim=1, keepdim=True) - embeds)\n",
    "        centroids_excl /= (utterances_per_speaker - 1)\n",
    "        centroids_excl = centroids_excl.clone() / torch.norm(centroids_excl, dim=2, keepdim=True)\n",
    "\n",
    "        # Similarity matrix. The cosine similarity of already 2-normed vectors is simply the dot\n",
    "        # product of these vectors (which is just an element-wise multiplication reduced by a sum).\n",
    "        # We vectorize the computation for efficiency.\n",
    "        sim_matrix = torch.zeros(speakers_per_batch, utterances_per_speaker,\n",
    "                                 speakers_per_batch).to(self.loss_device)\n",
    "        mask_matrix = 1 - np.eye(speakers_per_batch, dtype=np.int64)\n",
    "        for j in range(speakers_per_batch):\n",
    "            mask = np.where(mask_matrix[j])[0]\n",
    "            sim_matrix[mask, :, j] = (embeds[mask] * centroids_incl[j]).sum(dim=2)\n",
    "            sim_matrix[j, :, j] = (embeds[j] * centroids_excl[j]).sum(dim=1)\n",
    "\n",
    "        ## Even more vectorized version (slower maybe because of transpose)\n",
    "        sim_matrix = sim_matrix * self.similarity_weight + self.similarity_bias\n",
    "        return sim_matrix\n",
    "\n",
    "    def loss(self, embeds):\n",
    "        \"\"\"\n",
    "        Computes the softmax loss according the section 2.1 of GE2E.\n",
    "\n",
    "        :param embeds: the embeddings as a tensor of shape (speakers_per_batch,\n",
    "        utterances_per_speaker, embedding_size)\n",
    "        :return: the loss and the EER for this batch of embeddings.\n",
    "        \"\"\"\n",
    "        speakers_per_batch, utterances_per_speaker = embeds.shape[:2]\n",
    "\n",
    "        # Loss\n",
    "        sim_matrix = self.similarity_matrix(embeds)\n",
    "        sim_matrix = sim_matrix.reshape((speakers_per_batch * utterances_per_speaker,\n",
    "                                         speakers_per_batch))\n",
    "        ground_truth = np.repeat(np.arange(speakers_per_batch), utterances_per_speaker)\n",
    "        target = torch.from_numpy(ground_truth).long().to(self.loss_device)\n",
    "        loss = self.loss_fn(sim_matrix, target)\n",
    "\n",
    "        # EER (not backpropagated)\n",
    "        with torch.no_grad():\n",
    "            inv_argmax = lambda i: np.eye(1, speakers_per_batch, i, dtype=np.int64)[0]\n",
    "            labels = np.array([inv_argmax(i) for i in ground_truth])\n",
    "            preds = sim_matrix.detach().cpu().numpy()\n",
    "\n",
    "            # Snippet from https://yangcha.github.io/EER-ROC/\n",
    "            fpr, tpr, thresholds = roc_curve(labels.flatten(), preds.flatten())\n",
    "            eer = brentq(lambda x: 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n",
    "\n",
    "        return loss, eer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-05T09:17:34.411635Z",
     "iopub.status.busy": "2024-09-05T09:17:34.411323Z",
     "iopub.status.idle": "2024-09-05T09:17:34.435991Z",
     "shell.execute_reply": "2024-09-05T09:17:34.434983Z",
     "shell.execute_reply.started": "2024-09-05T09:17:34.411609Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import logging\n",
    "import random\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def evaluate(model, loader, total_evaluate_steps=50, device='cpu', loss_device='cpu'):\n",
    "    steps = 0\n",
    "    losses = []\n",
    "    eers = []\n",
    "    start_time = time.time()\n",
    "    while True:\n",
    "        if (steps+1) > total_evaluate_steps:\n",
    "            break\n",
    "\n",
    "        for batch in loader:\n",
    "            if (steps+1) > total_evaluate_steps:\n",
    "                break\n",
    "\n",
    "            n_speakers, n_utterances, freq_len, tempo_len = batch.shape\n",
    "            data = batch.view(-1, freq_len, tempo_len)\n",
    "            data = data.transpose(1, 2)\n",
    "            model.eval()\n",
    "            embeds = model(data.to(device))\n",
    "            embeds = embeds.view(n_speakers, n_utterances, -1)\n",
    "            loss, eer = model.loss(embeds.to(loss_device))\n",
    "            losses.append(loss.detach().numpy())\n",
    "            eers.append(eer)\n",
    "            steps += 1\n",
    "\n",
    "    mean_loss = np.mean(losses)\n",
    "    mean_eer = np.mean(eers)\n",
    "    print(f'Evaluate Mean Loss {mean_loss:.3f}, Mean EER {mean_eer:.3f} - Time: {(time.time() - start_time):.3f}s')\n",
    "\n",
    "def train():\n",
    "    print('Loading data...')\n",
    "    libri_dataset = load_librispeech360_dataset(libri_dataset_path)\n",
    "    print('Finish to load LibriSpeech360h')\n",
    "    vivos_dataset = load_vivos_dataset(Path(r'/kaggle/input/vivos-dataset/vivos'))\n",
    "    print('Finish to load Vivos')\n",
    "    aishell3_dataset = load_aishell3_dataset(Path(r'/kaggle/input/paddle-speech/AISHELL-3'))\n",
    "    print('Finish to load AISHELL-3')\n",
    "    voxceleb_dataset = load_voxceleb_dataset(Path(r'/kaggle/input/voxceleb1train/wav'))\n",
    "    print('Finish to load Voxceleb')\n",
    "\n",
    "    datasets = [libri_dataset, aishell3_dataset, vivos_dataset, voxceleb_dataset]\n",
    "    mds = MultiAudioDataset(datasets)\n",
    "    random.shuffle(mds.speakers)\n",
    "    train_speakers = mds.speakers[:-50]\n",
    "    eval_speakers = mds.speakers[-50:]\n",
    "\n",
    "    ds = SpeakerDataset(train_speakers,\n",
    "                        utterances_per_speaker=utterances_per_speaker,\n",
    "                        seq_len=seq_len)\n",
    "    loader = torch.utils.data.DataLoader(ds,\n",
    "                                        batch_size=speakers_per_batch,\n",
    "                                        shuffle=True,\n",
    "                                        num_workers=4)\n",
    "\n",
    "    eval_ds = SpeakerDataset(eval_speakers,\n",
    "                        utterances_per_speaker=utterances_per_speaker,\n",
    "                        seq_len=seq_len)\n",
    "    eval_loader = torch.utils.data.DataLoader(eval_ds,\n",
    "                                        batch_size=speakers_per_batch,\n",
    "                                        shuffle=True,\n",
    "                                        num_workers=4)\n",
    "    \n",
    "    dv = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    loss_dv = torch.device(loss_device)\n",
    "    model = SpeakerEncoder(device=dv, loss_device=loss_dv)\n",
    "    \n",
    "    opt = torch.optim.Adam(model.parameters(), lr=speaker_lr)\n",
    "\n",
    "    total_steps = 0\n",
    "\n",
    "    ckpts = sorted(list(Path(save_dir).glob('*.pt')))\n",
    "    if len(ckpts) > 0:\n",
    "        latest_ckpt_path = ckpts[-1]\n",
    "        ckpt = torch.load(latest_ckpt_path, weights_only=False)\n",
    "        if ckpt:\n",
    "            print(f'loading ckpt {latest_ckpt_path}')\n",
    "            model.load_state_dict(ckpt['model_state_dict'])\n",
    "            opt.load_state_dict(ckpt['optimizer_state_dict'])\n",
    "            total_steps = ckpt['total_steps']\n",
    "#     ckpt = torch.load(\"/kaggle/working/000000001600.pt\")\n",
    "#     if ckpt:\n",
    "# #         print(f'loading ckpt {latest_ckpt_path}')\n",
    "#         model.load_state_dict(ckpt['model_state_dict'])\n",
    "#         opt.load_state_dict(ckpt['optimizer_state_dict'])\n",
    "#         total_steps = ckpt['total_steps']\n",
    "\n",
    "    print(\"Start training . . .\")\n",
    "    while True:\n",
    "        if total_steps >= train_steps:\n",
    "            break\n",
    "\n",
    "        for batch in loader:\n",
    "            if total_steps >= train_steps:\n",
    "                break\n",
    "            start_time = time.time()\n",
    "            for g in opt.param_groups:\n",
    "                g['lr'] = speaker_lr\n",
    "            n_speakers, n_utterances, freq_len, tempo_len = batch.shape\n",
    "            data = batch.view(-1, freq_len, tempo_len)\n",
    "            data = data.transpose(1, 2)\n",
    "\n",
    "            model.train()\n",
    "            opt.zero_grad()\n",
    "\n",
    "            embeds = model(data.to(dv))\n",
    "            embeds = embeds.view(n_speakers, n_utterances, -1)\n",
    "            loss, eer = model.loss(embeds.to(loss_device))\n",
    "\n",
    "            loss.backward()\n",
    "            model.do_gradient_ops()\n",
    "            opt.step()\n",
    "\n",
    "            total_steps += 1\n",
    "\n",
    "            if (total_steps+1) % train_print_interval == 0:\n",
    "                print(f'Step {total_steps+1} Loss {loss:.3f}, EER {eer:.3f} - Time: {(time.time() - start_time):.3f}s')\n",
    "            if (total_steps+1) % save_interval == 0:\n",
    "                if not Path(save_dir).exists():\n",
    "                    Path(save_dir).mkdir()\n",
    "                save_path = Path(save_dir) / f'{total_steps+1:012d}.pt'\n",
    "                print(f'saving ckpt {save_path}')\n",
    "                torch.save({\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': opt.state_dict(),\n",
    "                    'total_steps': total_steps\n",
    "                }, save_path)\n",
    "\n",
    "                # remove old ckpts\n",
    "                ckpts = sorted(list(Path(save_dir).glob('*.pt')))\n",
    "                if len(ckpts) > max_ckpts:\n",
    "                    for ckpt in ckpts[:-max_ckpts]:\n",
    "                        Path(ckpt).unlink()\n",
    "                        print(f'ckpt {ckpt} removed')\n",
    "            if (total_steps+1) % evaluate_interval == 0:\n",
    "                evaluate(model, eval_loader, total_evaluate_steps=50, device=dv, loss_device=loss_dv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-05T09:17:34.437410Z",
     "iopub.status.busy": "2024-09-05T09:17:34.437124Z",
     "iopub.status.idle": "2024-09-05T09:17:34.481792Z",
     "shell.execute_reply": "2024-09-05T09:17:34.480820Z",
     "shell.execute_reply.started": "2024-09-05T09:17:34.437386Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-05T09:17:34.483686Z",
     "iopub.status.busy": "2024-09-05T09:17:34.483053Z",
     "iopub.status.idle": "2024-09-05T10:37:51.533987Z",
     "shell.execute_reply": "2024-09-05T10:37:51.530980Z",
     "shell.execute_reply.started": "2024-09-05T09:17:34.483651Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 2155739,
     "sourceId": 3593344,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 2270944,
     "sourceId": 3910949,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3937252,
     "sourceId": 6849137,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 111693,
     "modelInstanceId": 87456,
     "sourceId": 104357,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 111796,
     "modelInstanceId": 87559,
     "sourceId": 104485,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30747,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
