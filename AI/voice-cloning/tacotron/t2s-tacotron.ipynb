{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Utterance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import logging\n",
    "from io import BytesIO\n",
    "import librosa\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class Utterance(object):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        _id: str = None,\n",
    "        raw_file: Path | BytesIO = None,\n",
    "        processor=None,\n",
    "    ):\n",
    "        self._id = _id\n",
    "        self.raw_file = raw_file\n",
    "        self.processor = processor if processor is not None else AudioPreprocessor()\n",
    "#         self.audio = self.raw()\n",
    "\n",
    "    def raw(self):\n",
    "        if isinstance(self.raw_file, Path) and self.raw_file.suffix == '.npy':\n",
    "            return np.load(self.raw_file)\n",
    "            \n",
    "        audio, _ = librosa.load(\n",
    "            self.raw_file, sr=self.processor.config.SAMPLE_RATE\n",
    "        )\n",
    "        \n",
    "        if audio.size == 0:\n",
    "            raise ValueError(\"Empty audio\")\n",
    "\n",
    "        audio = (\n",
    "            self.processor.config.SCALING_FACTOR\n",
    "            * librosa.util.normalize(audio)\n",
    "        )\n",
    "        return audio\n",
    "\n",
    "    def mel_in_db(self):\n",
    "        try:\n",
    "            return self.processor.audio_to_mel_db(self.raw())\n",
    "        except Exception:\n",
    "\n",
    "            logging.debug(\n",
    "                \"Failed to load Mel spectrogram, raw file: %s\", {self.raw_file}\n",
    "            )\n",
    "            raise\n",
    "    \n",
    "    def random_mel_in_db(self, seq_len):\n",
    "        random_mel = self.mel_in_db()\n",
    "        _, tempo_len = random_mel.shape\n",
    "        if tempo_len < seq_len:\n",
    "            pad_left = (seq_len - tempo_len) // 2\n",
    "            pad_right = seq_len - tempo_len - pad_left\n",
    "            random_mel = np.pad(random_mel, ((0, 0), (pad_left, pad_right)), mode=\"reflect\")\n",
    "        elif tempo_len > seq_len:\n",
    "            max_seq_start = tempo_len - seq_len\n",
    "            seq_start = np.random.randint(0, max_seq_start)\n",
    "            seq_end = seq_start + seq_len\n",
    "            random_mel = random_mel[:, seq_start:seq_end]\n",
    "        return random_mel\n",
    "    \n",
    "    def magtitude(self):\n",
    "        return self.processor.audio_to_magnitude_db(self.raw())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Processor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Audio Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioConfig:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class AudioPreprocessor:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "\n",
    "    def normalize(self, spectrogram_in_db):\n",
    "        '''Normalize spectrogram in decibel values between 0 and 1.'''\n",
    "        normalized_spectrogram_in_db = (\n",
    "            spectrogram_in_db - self.config.REF_LEVEL_DB - self.config.MIN_LEVEL_DB\n",
    "        ) / -self.config.MIN_LEVEL_DB\n",
    "\n",
    "        return np.clip(normalized_spectrogram_in_db, self.config.ZERO_THRESHOLD, 1)\n",
    "\n",
    "    def magnitude_to_mel(self, magnitude):\n",
    "        '''Convert a magnitude spectrogram to a mel spectrogram.'''\n",
    "        return librosa.feature.melspectrogram(\n",
    "            S=magnitude,\n",
    "            sr=self.config.SAMPLE_RATE,\n",
    "            n_fft=self.config.N_FFT,\n",
    "            n_mels=self.config.N_MELS,\n",
    "            fmin=self.config.FMIN,\n",
    "            fmax=self.config.FMAX,\n",
    "        )\n",
    "\n",
    "    def amp_to_db(self, mel_spectrogram):\n",
    "        '''Convert amplitude spectrogram to decibel scale.'''\n",
    "        return 20.0 * np.log10(\n",
    "            np.maximum(self.config.ZERO_THRESHOLD, mel_spectrogram)\n",
    "        )\n",
    "\n",
    "    def audio_to_stft(self, audio):\n",
    "        '''Generate Short-Time Fourier Transform (STFT) from the audio time series.'''\n",
    "        return librosa.stft(\n",
    "            y=audio,\n",
    "            n_fft=self.config.N_FFT,\n",
    "            hop_length=self.config.HOP_LENGTH,\n",
    "            win_length=self.config.WIN_LENGTH,\n",
    "        )\n",
    "\n",
    "    def apply_pre_emphasis(self, y):\n",
    "        '''Apply a pre-emphasis filter to the audio signal.'''\n",
    "        return np.append(y[0], y[1:] - self.config.PRE_EMPHASIS * y[:-1])\n",
    "\n",
    "    def stft_to_magnitude(self, linear):\n",
    "        '''Compute the magnitude spectrogram from STFT.'''\n",
    "        return np.abs(linear)\n",
    "\n",
    "    def audio_to_mel_db(self, audio):\n",
    "        '''Convert a given linear spectrogram to a log mel spectrogram (mel spectrogram in db) and return it.'''\n",
    "        stft = self.audio_to_stft(audio)\n",
    "        magnitude = self.stft_to_magnitude(stft)\n",
    "        mel = self.magnitude_to_mel(magnitude)\n",
    "        mel = self.amp_to_db(mel)\n",
    "        return self.normalize(mel)\n",
    "    \n",
    "    def audio_to_magnitude_db(self, audio):\n",
    "        '''Convert a given linear spectrogram to a magnitude spectrogram.'''\n",
    "        stft = self.audio_to_stft(audio)\n",
    "        magnitude_in_amp =  self.stft_to_magnitude(stft)\n",
    "        magnitude_in_db = self.amp_to_db(magnitude_in_amp)\n",
    "        return self.normalize(magnitude_in_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Text Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_by_key(dictionary: dict):\n",
    "    return dict(sorted(dictionary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Acronym Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "ACRONYMS_FILEPATH = \"./acronyms.json\"\n",
    "\n",
    "with open(ACRONYMS_FILEPATH, \"r\", encoding=\"utf-8\") as file:\n",
    "    ACRONYMS = sort_by_key(json.load(file).items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class AcronymNormalizer(object):\n",
    "\n",
    "    pattern = re.compile(r\"\\b(\" + \"|\".join(map(re.escape, ACRONYMS)) + r\")\\b\")\n",
    "\n",
    "    @classmethod\n",
    "    def normalize(cls, text: str):\n",
    "        def replace_unit(match):\n",
    "            return ACRONYMS[match.group(0)]\n",
    "\n",
    "        return cls.pattern.sub(replace_unit, text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Breaker Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BREAKS = {\n",
    "    \".\": \" chấm \",\n",
    "    \",\": \" phẩy \",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class BreakNormalizer(object):\n",
    "\n",
    "    BREAKS = BREAKS\n",
    "\n",
    "    duplicate_dot_comma_pattern = re.compile(r\"([,.]){2,}\")\n",
    "    adjacent_symbols_pattern = re.compile(r\"(\\S)([,.])(\\S)\")\n",
    "    left_symbol_pattern = re.compile(r\"(\\S)([,.])\")\n",
    "    right_symbol_pattern = re.compile(r\"([,.])(\\S)\")\n",
    "\n",
    "    @classmethod\n",
    "    def normalize(cls, text):\n",
    "        text = cls.duplicate_dot_comma_pattern.sub(lambda m: m.group(1), text)\n",
    "\n",
    "        def replace_dot_and_comma(match):\n",
    "            return match.group(1) + cls.BREAKS[match.group(2)] + match.group(3)\n",
    "\n",
    "        text = cls.adjacent_symbols_pattern.sub(replace_dot_and_comma, text)\n",
    "        text = cls.left_symbol_pattern.sub(r\"\\1 \\2\", text)\n",
    "        text = cls.right_symbol_pattern.sub(r\"\\1 \\2\", text)\n",
    "\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3 Character Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "class CharacterNormalizer(object):\n",
    "\n",
    "    pattern = re.compile(r\"[^a-zA-Z0-9\\sđâăêôơư.,]\")\n",
    "\n",
    "    @classmethod\n",
    "    def normalize(cls, text: str):\n",
    "\n",
    "        return cls.pattern.sub(\"\", text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.4 Date Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "DATE_PREFIXS_FILEPATH = \"./date_prefixs.json\"\n",
    "\n",
    "with open(DATE_PREFIXS_FILEPATH, \"r\", encoding=\"utf-8\") as file:\n",
    "    DATE_PREFIXS = sorted(json.load(file), key=len, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "class DateNormalizer(object):\n",
    "\n",
    "    DATE_PREFIXS = DATE_PREFIXS\n",
    "\n",
    "    date_pattern1 = re.compile(r\"(\\b\\w{0,4}\\b)\\s*([12][0-9]|3[01]|0?[1-9])\\/(1[0-2]|0?[1-9])\\/(\\d{1,4})\")\n",
    "    date_pattern2 = re.compile(r\"(\\b\\w{0,4}\\b)\\s*([12][0-9]|3[01]|0?[1-9])\\-(1[0-2]|0?[1-9])\\-(\\d{1,4})\")\n",
    "    date_pattern3 = re.compile(r\"(\\b\\w{0,5}\\b)\\s*(0?[1-9]|1[0,1,2])[\\/|\\-](\\d{4})\")\n",
    "    prefixs = \"|\".join(DATE_PREFIXS)\n",
    "    date_pattern4 = re.compile(r\"(\" + prefixs + r\")\\s([12][0-9]|3[01]|0?[1-9])[\\-|\\/](1[0-2]|0?[1-9])\")\n",
    "\n",
    "    @classmethod\n",
    "    def normalize_date_pattern1(cls, text: str):\n",
    "        # Date pattern 1\n",
    "        # Example: 11/12/2002\n",
    "\n",
    "        def replace(match):\n",
    "            prefix = match.group(1).strip()\n",
    "            day = match.group(2)\n",
    "            month = match.group(3)\n",
    "            year = match.group(4)\n",
    "\n",
    "            if prefix == \"ngày\":\n",
    "                return f\"{prefix} {day} tháng {month} năm {year}\"\n",
    "            else:\n",
    "                return f'{prefix + \" \" if prefix != \"\" else \"\"}ngày {day} tháng {month} năm {year}'\n",
    "\n",
    "        return cls.date_pattern1.sub(replace, text)\n",
    "\n",
    "    @classmethod\n",
    "    def normalize_date_pattern2(cls, text: str):\n",
    "        # Date pattern 1\n",
    "        # Example: 11-12-2002\n",
    "\n",
    "        def replace(match):\n",
    "            prefix = match.group(1).strip()\n",
    "            day = match.group(2)\n",
    "            month = match.group(3)\n",
    "            year = match.group(4)\n",
    "\n",
    "            if prefix == \"ngày\":\n",
    "                return f\"{prefix} {day} tháng {month} năm {year}\"\n",
    "            else:\n",
    "                return f'{prefix + \" \" if prefix != \"\" else \"\"}ngày {day} tháng {month} năm {year}'\n",
    "\n",
    "        return cls.date_pattern2.sub(replace, text)\n",
    "\n",
    "    @classmethod\n",
    "    def normalize_date_pattern3(cls, text: str):\n",
    "        # Date pattern 3\n",
    "        # Example: 12/2022 -> tháng 12 năm 2002\n",
    "\n",
    "        def replace(match):\n",
    "            prefix = match.group(1)\n",
    "            month = match.group(2)\n",
    "            year = match.group(3)\n",
    "\n",
    "            if prefix == \"tháng\":\n",
    "                return f\"tháng {month} năm {year}\"\n",
    "            else:\n",
    "                return f'{prefix + \" \" if prefix != \"\" else \"\"}tháng {month} năm {year}'\n",
    "\n",
    "        return cls.date_pattern3.sub(replace, text)\n",
    "\n",
    "    @classmethod\n",
    "    def normalize_date_pattern4(cls, text: str):\n",
    "        # Date pattern 4\n",
    "        # Example: ngày 11/12\n",
    "\n",
    "        def replace(match):\n",
    "            prefix = match.group(1)\n",
    "            day = match.group(2)\n",
    "            month = match.group(3)\n",
    "\n",
    "            if prefix == \"ngày\":\n",
    "                return f\"ngày {day} tháng {month}\"\n",
    "            else:\n",
    "                return f'{prefix + \" \" if prefix != \"\" else \"\"}ngày {day} tháng {month}'\n",
    "\n",
    "        return cls.date_pattern4.sub(replace, text)\n",
    "\n",
    "    @classmethod\n",
    "    def normalize(cls, text: str):\n",
    "        text = cls.normalize_date_pattern4(text)\n",
    "        text = cls.normalize_date_pattern1(text)\n",
    "        text = cls.normalize_date_pattern2(text)\n",
    "        text = cls.normalize_date_pattern3(text)\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.5 Letter Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "LETTERS_FILEPATH = \"./letters.json\"\n",
    "\n",
    "with open(LETTERS_FILEPATH, \"r\", encoding=\"utf-8\") as file:\n",
    "    LETTERS = sort_by_key(json.load(file).items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class LetterNormalizer(object):\n",
    "\n",
    "    pattern = re.compile(r\"\\b(\" + \"|\".join(map(re.escape, LETTERS)) + r\")\\b\")\n",
    "\n",
    "    @classmethod\n",
    "    def normalize(cls, text: str):\n",
    "\n",
    "        def replace_unit(match):\n",
    "            return LETTERS[match.group(0)]\n",
    "\n",
    "        return cls.pattern.sub(replace_unit, text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.6 Number Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "BASE_NUMBERS_FILEPATH = \"./base_numbers.json\"\n",
    "\n",
    "with open(BASE_NUMBERS_FILEPATH, \"r\") as file:\n",
    "    BASE_NUMBERS = {int(key): value for key, value in json.load(file).items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "NUMBER_LEVEL_FILEPATH = \"./number_levels.json\"\n",
    "\n",
    "with open(NUMBER_LEVEL_FILEPATH, \"r\") as file:\n",
    "    NUMBER_LEVELS = {int(key): value for key, value in json.load(file).items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class NumberNomalizer(object):\n",
    "\n",
    "    pattern = re.compile(r\"\\d+\")\n",
    "\n",
    "    @classmethod\n",
    "    def _convert_number_2_digits(cls, number: int):\n",
    "        if number in BASE_NUMBERS:\n",
    "            return BASE_NUMBERS[number]\n",
    "\n",
    "        tens = number // 10\n",
    "        base = number % 10\n",
    "        if base > 0:\n",
    "            return f\"{BASE_NUMBERS[tens]} mươi {BASE_NUMBERS[base]}\"\n",
    "\n",
    "        return f\"{BASE_NUMBERS[tens]} mươi\"\n",
    "\n",
    "    @classmethod\n",
    "    def _convert_number_3_digits(cls, number: int):\n",
    "        if number == 0:\n",
    "            return \"\"\n",
    "\n",
    "        remainder = number % 100\n",
    "        hundred = number // 100\n",
    "        if remainder == 0:\n",
    "            return f\"{BASE_NUMBERS[hundred]} trăm\"\n",
    "\n",
    "        if remainder < 10:\n",
    "            return f\"{BASE_NUMBERS[number // 100]} trăm linh {BASE_NUMBERS[remainder]}\"\n",
    "\n",
    "        return f\"{BASE_NUMBERS[hundred]} trăm {cls._convert_number_2_digits(remainder)}\"\n",
    "\n",
    "    @classmethod\n",
    "    def number_to_vietnamese(cls, number: int):\n",
    "        if number == 0:\n",
    "            return \"không\"\n",
    "\n",
    "        if number in BASE_NUMBERS:\n",
    "            return BASE_NUMBERS[number]\n",
    "\n",
    "        if number < 100:\n",
    "            return cls._convert_number_2_digits(number)\n",
    "\n",
    "        result = cls._convert_number_3_digits(number % 1000)\n",
    "        current_level = None\n",
    "\n",
    "        for current_level in NUMBER_LEVELS:\n",
    "            next_level = current_level * 1000\n",
    "            if number // (next_level) == 0:\n",
    "                break\n",
    "            level_base = number % (next_level) // current_level\n",
    "            result = f\"{cls._convert_number_3_digits(level_base)} {NUMBER_LEVELS[current_level]} {result}\"\n",
    "\n",
    "        level_base = number // current_level\n",
    "\n",
    "        if level_base == 0:\n",
    "            return result\n",
    "\n",
    "        if level_base in BASE_NUMBERS:\n",
    "            return f\"{BASE_NUMBERS[level_base]} {NUMBER_LEVELS[current_level]} {result}\"\n",
    "\n",
    "        if level_base > 99:\n",
    "            return f\"{cls._convert_number_3_digits(level_base)} {NUMBER_LEVELS[current_level]} {result}\"\n",
    "\n",
    "        if level_base > 11:\n",
    "            return f\"{cls._convert_number_2_digits(level_base)} {NUMBER_LEVELS[current_level]} {result}\"\n",
    "\n",
    "    @classmethod\n",
    "    def normalize(cls, text: str) -> str:\n",
    "\n",
    "        replaced_text = cls.pattern.sub(lambda x: cls.number_to_vietnamese(int(x.group())), text)\n",
    "\n",
    "        return replaced_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.7 Phoneme Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "SAME_PHONEMES_FILEPATH = \"./same_phonemes.json\"\n",
    "\n",
    "with open(SAME_PHONEMES_FILEPATH, \"r\", encoding=\"utf-8\") as file:\n",
    "    SAME_PHONEMES = sort_by_key(json.load(file).items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class PhonemeNormalizer(object):\n",
    "\n",
    "    pattern = re.compile(r\"(\" + \"|\".join(map(re.escape, SAME_PHONEMES)) + r\")\")\n",
    "\n",
    "    @classmethod\n",
    "    def normalize(cls, text: str):\n",
    "\n",
    "        def replace_symbol(match):\n",
    "            return SAME_PHONEMES[match.group(0)]\n",
    "\n",
    "        return cls.pattern.sub(replace_symbol, text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.8 Symbol Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "SYMBOLS_FILEPATH = \"./symbols.json\"\n",
    "\n",
    "with open(SYMBOLS_FILEPATH, \"r\", encoding=\"utf-8\") as file:\n",
    "    SYMBOLS = sort_by_key(json.load(file).items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class SymbolNormalizer(object):\n",
    "\n",
    "    pattern = re.compile(r\"([\\s\\S])(\" + \"|\".join(map(re.escape, SYMBOLS)) + r\")([\\s\\S])\")\n",
    "\n",
    "    @classmethod\n",
    "    def normalize(cls, text: str):\n",
    "\n",
    "        def replace_symbol(match):\n",
    "            return (\n",
    "                (match.group(1) if match.group(1) == \" \" else match.group(1) + \" \")\n",
    "                + SYMBOLS[match.group(2)]\n",
    "                + (match.group(3) if match.group(3) == \" \" else match.group(3) + \" \")\n",
    "            )\n",
    "\n",
    "        return cls.pattern.sub(replace_symbol, text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.9 Tone Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "TONES_FILEPATH = \"./tones.json\"\n",
    "\n",
    "with open(TONES_FILEPATH, \"r\") as file:\n",
    "    TONES = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class ToneNormalizer(object):\n",
    "    pattern = re.compile(r\"(\\w*)([áàảãạấầẩẫậắằẳẵặéèẻẽẹếềểễệíìỉĩịóòỏõọốồổỗộớờởỡợúùủũụứừửữựýỳỷỹỵ])(\\w*)\")\n",
    "\n",
    "    @classmethod\n",
    "    def normalize(cls, text):\n",
    "\n",
    "        def replace(match):\n",
    "            accented = match.group(2)\n",
    "            base, tone = TONES[accented]\n",
    "            return f\"{match.group(1)}{base}{match.group(3)}{tone}\"\n",
    "\n",
    "        text = cls.pattern.sub(replace, text)\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.10 Unit Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "UNITS_FILEPATH = \"./units.json\"\n",
    "\n",
    "with open(UNITS_FILEPATH, \"r\", encoding=\"utf-8\") as file:\n",
    "    UNITS = sort_by_key(json.load(file).items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class UnitNormalizer(object):\n",
    "\n",
    "    pattern = re.compile(r\"\\b(\" + \"|\".join(map(re.escape, UNITS)) + r\")\\b\")\n",
    "\n",
    "    @classmethod\n",
    "    def normalize(cls, text):\n",
    "\n",
    "        def replace_unit(match):\n",
    "            return UNITS[match.group(0)]\n",
    "\n",
    "        return cls.pattern.sub(replace_unit, text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.11 Text Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_PIPELINE = [\n",
    "    DateNormalizer,\n",
    "    NumberNomalizer,\n",
    "    LetterNormalizer,\n",
    "    AcronymNormalizer,\n",
    "    SymbolNormalizer,\n",
    "    UnitNormalizer,\n",
    "    PhonemeNormalizer,\n",
    "    ToneNormalizer,\n",
    "    CharacterNormalizer,\n",
    "    BreakNormalizer,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextNormalizer(object):\n",
    "\n",
    "    def __init__(self, pipeline=DEFAULT_PIPELINE, lower=True):\n",
    "        self.pipeline = pipeline\n",
    "        self.lower = lower\n",
    "\n",
    "    def normalize(self, text):\n",
    "        if self.lower:\n",
    "            text = text.lower()\n",
    "\n",
    "        for processor in self.pipeline:\n",
    "            text = processor.normalize(text)\n",
    "\n",
    "        return text\n",
    "\n",
    "    def __call__(self, text):\n",
    "        return self.normalize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.12 Text To Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACCENTS = ['1', '2', '3', '4', '5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOWELS_FILEPATH = \"./vowels.json\"\n",
    "\n",
    "with open(VOWELS_FILEPATH, \"r\", encoding=\"utf-8\") as file:\n",
    "    VOWELS = sorted(json.load(file), key=len, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEAD_CONSONANTS_FILEPATH = \"./head_consonants.json\"\n",
    "\n",
    "with open(HEAD_CONSONANTS_FILEPATH, \"r\", encoding=\"utf-8\") as file:\n",
    "    HEAD_CONSONANTS = sorted(json.load(file), key=len, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "FINAL_CONSONANTS_FILEPATH = \"./final_consonants.json\"\n",
    "    \n",
    "with open(FINAL_CONSONANTS_FILEPATH, \"r\", encoding=\"utf-8\") as file:\n",
    "    FINAL_CONSONANTS = sorted(json.load(file), key=len, reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "PHONEMES = sorted(VOWELS + HEAD_CONSONANTS + FINAL_CONSONANTS + ACCENTS + list(BREAKS.keys()) + [\" \"], key=len, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(PHONEMES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "phoneme_to_ids = {s: i for i, s in enumerate(PHONEMES)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class WordByPhonemesEmbedding(object):\n",
    "\n",
    "    def __init__(self, phonemes=PHONEMES, normalizer=TextNormalizer(), spliter=\" \"):\n",
    "        self.phonemes = phonemes\n",
    "        self.normalizer = normalizer\n",
    "        self.spliter = spliter\n",
    "\n",
    "    def _parse_head_constants(self, word):\n",
    "        pattern = r'^(' + '|'.join(HEAD_CONSONANTS) + ')'\n",
    "        match = re.match(pattern, word)\n",
    "        head_consonant = None\n",
    "        if match:\n",
    "            head_consonant = r'\\b' + match.group(1)\n",
    "        return re.sub(pattern, '', word), head_consonant\n",
    "    \n",
    "    def _parse_vowels(self, word):\n",
    "        pattern = r'^(' + '|'.join(VOWELS) + ')'\n",
    "        match = re.match(pattern, word)\n",
    "        vowel = None\n",
    "        if match:\n",
    "            vowel =  match.group(1)\n",
    "        return re.sub(pattern, '', word), vowel\n",
    "\n",
    "    def _parse_final_constants(self, word):\n",
    "        pattern = r'^(' + '|'.join(FINAL_CONSONANTS) + ')'\n",
    "        match = re.match(pattern, word)\n",
    "        final_consonant = None\n",
    "        if match:\n",
    "            final_consonant =  match.group(1)\n",
    "        return re.sub(pattern, '', word), final_consonant\n",
    "\n",
    "    def word2vec(self, word:str):\n",
    "        embedding_vector = []\n",
    "        \n",
    "        word, head_consonant = self._parse_head_constants(word)\n",
    "        word, vowel = self._parse_vowels(word)\n",
    "        word, final_consonant = self._parse_final_constants(word)\n",
    "\n",
    "        if head_consonant is not None:\n",
    "            embedding_vector.append(phoneme_to_ids[head_consonant])\n",
    "            \n",
    "        if vowel is not None:\n",
    "            embedding_vector.append(phoneme_to_ids[vowel])\n",
    "            \n",
    "        if final_consonant is not None:\n",
    "            embedding_vector.append(phoneme_to_ids[final_consonant])\n",
    "        \n",
    "        if len(word) > 0 and word[-1] in PHONEMES:\n",
    "            accent_or_break = word[-1]\n",
    "            embedding_vector.append(phoneme_to_ids[accent_or_break])\n",
    "\n",
    "        return {\n",
    "            \"head_consonant\": head_consonant,\n",
    "            \"final_consonant\": final_consonant,\n",
    "            \"vowel\": vowel,\n",
    "            \"emmbedding_vector\": embedding_vector\n",
    "        }\n",
    "\n",
    "    def embedding(self, text):\n",
    "        text = self.normalizer.normalize(text)\n",
    "        words = text.split(self.spliter)\n",
    "        sequence = []\n",
    "        for word in words:\n",
    "            sequence.extend(self.word2vec(word)[\"emmbedding_vector\"])\n",
    "            sequence.append(phoneme_to_ids[self.spliter])\n",
    "        return sequence[:-1]\n",
    "        # return [self.word2vec(word)[\"emmbedding_vector\"] for word in words]\n",
    "\n",
    "    def __call__(self, text):\n",
    "        return self.embedding(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## English Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "class ENAcronymNormalizer(object):\n",
    "    \n",
    "    _acronyms = [(re.compile(\"\\\\b%s\\\\.\" % x[0], re.IGNORECASE), x[1]) for x in [\n",
    "        (\"mrs\", \"misess\"),\n",
    "        (\"mr\", \"mister\"),\n",
    "        (\"dr\", \"doctor\"),\n",
    "        (\"st\", \"saint\"),\n",
    "        (\"co\", \"company\"),\n",
    "        (\"jr\", \"junior\"),\n",
    "        (\"maj\", \"major\"),\n",
    "        (\"gen\", \"general\"),\n",
    "        (\"drs\", \"doctors\"),\n",
    "        (\"rev\", \"reverend\"),\n",
    "        (\"lt\", \"lieutenant\"),\n",
    "        (\"hon\", \"honorable\"),\n",
    "        (\"sgt\", \"sergeant\"),\n",
    "        (\"capt\", \"captain\"),\n",
    "        (\"esq\", \"esquire\"),\n",
    "        (\"ltd\", \"limited\"),\n",
    "        (\"col\", \"colonel\"),\n",
    "        (\"ft\", \"fort\"),\n",
    "    ]]\n",
    "\n",
    "\n",
    "    def normalize_acronym(self, text):\n",
    "        for regex, replacement in self._acronyms:\n",
    "            text = re.sub(regex, replacement, text)\n",
    "            \n",
    "        return text\n",
    "\n",
    "    def __call__(self, text):\n",
    "        return self.normalize_acronym(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import inflect\n",
    "\n",
    "\n",
    "class ENNumberNormalizer(object):\n",
    "    \n",
    "    _inflect = inflect.engine()\n",
    "    _comma_number_re = re.compile(r\"([0-9][0-9\\,]+[0-9])\")\n",
    "    _decimal_number_re = re.compile(r\"([0-9]+\\.[0-9]+)\")\n",
    "    _pounds_re = re.compile(r\"£([0-9\\,]*[0-9]+)\")\n",
    "    _dollars_re = re.compile(r\"\\$([0-9\\.\\,]*[0-9]+)\")\n",
    "    _ordinal_re = re.compile(r\"[0-9]+(st|nd|rd|th)\")\n",
    "    _number_re = re.compile(r\"[0-9]+\")\n",
    "\n",
    "    def _remove_commas(self, m):\n",
    "        return m.group(1).replace(\",\", \"\")\n",
    "\n",
    "    def _expand_decimal_point(self, m):\n",
    "        return m.group(1).replace(\".\", \" point \")\n",
    "\n",
    "    def _expand_dollars(self, m):\n",
    "        match = m.group(1)\n",
    "        parts = match.split(\".\")\n",
    "        \n",
    "        if len(parts) > 2:\n",
    "            return match + \" dollars\"\n",
    "        \n",
    "        dollars = int(parts[0]) if parts[0] else 0\n",
    "        cents = int(parts[1]) if len(parts) > 1 and parts[1] else 0\n",
    "        \n",
    "        if dollars and cents:\n",
    "            dollar_unit = \"dollar\" if dollars == 1 else \"dollars\"\n",
    "            cent_unit = \"cent\" if cents == 1 else \"cents\"\n",
    "            \n",
    "            return \"%s %s, %s %s\" % (dollars, dollar_unit, cents, cent_unit)\n",
    "        \n",
    "        elif dollars:\n",
    "            dollar_unit = \"dollar\" if dollars == 1 else \"dollars\"\n",
    "            return \"%s %s\" % (dollars, dollar_unit)\n",
    "        \n",
    "        elif cents:\n",
    "            cent_unit = \"cent\" if cents == 1 else \"cents\"\n",
    "            \n",
    "            return \"%s %s\" % (cents, cent_unit)\n",
    "        \n",
    "        else:\n",
    "            return \"zero dollars\"\n",
    "\n",
    "    def _expand_ordinal(self, m):\n",
    "        return self._inflect.number_to_words(m.group(0))\n",
    "\n",
    "    def _expand_number(self, m):\n",
    "        num = int(m.group(0))\n",
    "        if num > 1000 and num < 3000:\n",
    "            if num == 2000:\n",
    "                return \"two thousand\"\n",
    "            elif num > 2000 and num < 2010:\n",
    "                return \"two thousand \" + self._inflect.number_to_words(num % 100)\n",
    "            elif num % 100 == 0:\n",
    "                return self._inflect.number_to_words(num // 100) + \" hundred\"\n",
    "            else:\n",
    "                return self._inflect.number_to_words(num, andword=\"\", zero=\"oh\", group=2).replace(\", \", \" \")\n",
    "        else:\n",
    "            return self._inflect.number_to_words(num, andword=\"\")\n",
    "\n",
    "    def normalize_numbers(self, text):\n",
    "        text = re.sub(self._comma_number_re, self._remove_commas, text)\n",
    "        text = re.sub(self._pounds_re, r\"\\1 pounds\", text)\n",
    "        text = re.sub(self._dollars_re, self._expand_dollars, text)\n",
    "        text = re.sub(self._decimal_number_re, self._expand_decimal_point, text)\n",
    "        text = re.sub(self._ordinal_re, self._expand_ordinal, text)\n",
    "        text = re.sub(self._number_re, self._expand_number, text)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def __call__(self, text):\n",
    "        return self.normalize_numbers(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "class WhiteSpaceNormalizer(object):\n",
    "\n",
    "    _whitespace_re = re.compile(r\"\\s+\")\n",
    "\n",
    "    def collapse_whitespace(self, text):\n",
    "        return re.sub(self._whitespace_re, \" \", text)\n",
    "    \n",
    "    def __call__(self, text):\n",
    "        return self.collapse_whitespace(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from unidecode import unidecode\n",
    "\n",
    "class EnglishText2Sequence(object):\n",
    "    \n",
    "    _pad        = \"_\"\n",
    "    _eos        = \"~\"\n",
    "    _characters = \"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz!\\'\\\"(),-.:;? \"\n",
    "\n",
    "    symbols = [_pad, _eos] + list(_characters)\n",
    "\n",
    "    _symbol_to_id = {s: i for i, s in enumerate(symbols)}\n",
    "    _id_to_symbol = {i: s for i, s in enumerate(symbols)}\n",
    "    _curly_re = re.compile(r\"(.*?)\\{(.+?)\\}(.*)\")\n",
    "\n",
    "    _acronym_normalizer = ENAcronymNormalizer()\n",
    "    _number_normalizer = ENNumberNormalizer()\n",
    "    _whitespace_normalizer = WhiteSpaceNormalizer()\n",
    "\n",
    "    def lowercase(self, text):\n",
    "        return text.lower()\n",
    "\n",
    "\n",
    "    def convert_to_ascii(self, text):\n",
    "        return unidecode(text)\n",
    "\n",
    "\n",
    "    def normalize(self, text):\n",
    "        text = self.convert_to_ascii(text)\n",
    "        text = self.lowercase(text)\n",
    "        text = self._number_normalizer(text)\n",
    "        text = self._acronym_normalizer(text)\n",
    "        text = self._whitespace_normalizer(text)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def _symbols_to_sequence(self, symbols):\n",
    "        return [self._symbol_to_id[s] for s in symbols if s in self._symbol_to_id and s not in (\"_\", \"~\")]\n",
    "\n",
    "    \n",
    "    def text_to_sequence(self, text):\n",
    "        sequence = []\n",
    "\n",
    "        while len(text):\n",
    "            m = self._curly_re.match(text)\n",
    "            if not m:\n",
    "                sequence += self._symbols_to_sequence(self.normalize(text))\n",
    "                break\n",
    "            \n",
    "            sequence += self._symbols_to_sequence(self.normalize(m.group(1)))\n",
    "            sequence += self._arpabet_to_sequence(m.group(2))\n",
    "            text = m.group(3)\n",
    "\n",
    "        sequence.append(self._symbol_to_id[\"~\"])\n",
    "        return sequence\n",
    "    \n",
    "    def __call__(self, text):\n",
    "        return self.text_to_sequence(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Speech Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Speech Encoder Audio Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeakerEncoderAudioConfig(AudioConfig):\n",
    "    N_MELS = 80\n",
    "    SAMPLE_RATE = 16000\n",
    "    FRAME_SHIFT = 0.01\n",
    "    FRAME_LENGTH = 0.025\n",
    "    HOP_LENGTH = int(SAMPLE_RATE * FRAME_SHIFT)\n",
    "    WIN_LENGTH = int(SAMPLE_RATE * FRAME_LENGTH)\n",
    "    N_FFT = 1024\n",
    "    FMIN = 90\n",
    "    FMAX = 7600\n",
    "    ZERO_THRESHOLD = 1e-5\n",
    "    MIN_AMPLITUDE = 0.3\n",
    "    MAX_AMPLITUDE = 1.0\n",
    "    MIN_LEVEL_DB = -100\n",
    "    REF_LEVEL_DB = 0\n",
    "    NUM_FRAMES = 160 * 30\n",
    "    SCALING_FACTOR = 0.95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Speech Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from scipy.interpolate import interp1d\n",
    "from sklearn.metrics import roc_curve\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from scipy.optimize import brentq\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "class SpeechTransformerEncoder(nn.Module):\n",
    "    def __init__(self, input_size=80, hidden_size=786, num_layers=12, num_heads=8, device='cpu', loss_device='cpu'):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.loss_device = loss_device\n",
    "        \n",
    "        # Transformer encoder layers\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=input_size,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=hidden_size,\n",
    "            dropout=0.05,\n",
    "            activation='relu',\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers).to(device)\n",
    "\n",
    "        self.linear = nn.Linear(in_features=input_size, out_features=256).to(device)\n",
    "        self.relu = nn.ReLU().to(device)\n",
    "\n",
    "        # Cosine similarity scaling (with fixed initial parameter values)\n",
    "        self.similarity_weight = nn.Parameter(torch.tensor([10.])).to(loss_device)\n",
    "        self.similarity_bias = nn.Parameter(torch.tensor([-5.])).to(loss_device)\n",
    "\n",
    "        # Loss\n",
    "        self.loss_fn = nn.CrossEntropyLoss().to(loss_device)\n",
    "\n",
    "    def do_gradient_ops(self):\n",
    "        # Gradient scale\n",
    "        self.similarity_weight.grad *= 0.01\n",
    "        self.similarity_bias.grad *= 0.01\n",
    "\n",
    "        # Gradient clipping\n",
    "        clip_grad_norm_(self.parameters(), 3, norm_type=2)\n",
    "\n",
    "    def forward(self, utterances, hidden_init=None):\n",
    "        \"\"\"\n",
    "        Computes the embeddings of a batch of utterance spectrograms.\n",
    "\n",
    "        :param utterances: batch of mel-scale filterbanks of same duration as a tensor of shape\n",
    "        (batch_size, n_frames, n_channels)\n",
    "        :param hidden_init: not used in the Transformer version\n",
    "        :return: the embeddings as a tensor of shape (batch_size, embedding_size)\n",
    "        \"\"\"\n",
    "        utterances = utterances.to(self.device)\n",
    "        # Pass the input through the Transformer Encoder\n",
    "        out = self.transformer_encoder(utterances)\n",
    "\n",
    "        # We take the mean of all time steps (similar to a global pooling)\n",
    "        embeds_raw = self.relu(self.linear(out.mean(dim=1)))\n",
    "\n",
    "        # L2-normalize it\n",
    "        embeds = embeds_raw / torch.norm(embeds_raw, dim=1, keepdim=True)\n",
    "\n",
    "        return embeds\n",
    "\n",
    "    def similarity_matrix(self, embeds):\n",
    "        \"\"\"\n",
    "        Computes the similarity matrix according of GE2E.\n",
    "\n",
    "        :param embeds: the embeddings as a tensor of shape (speakers_per_batch,\n",
    "        utterances_per_speaker, embedding_size)\n",
    "        :return: the similarity matrix as a tensor of shape (speakers_per_batch,\n",
    "        utterances_per_speaker, speakers_per_batch)\n",
    "        \"\"\"\n",
    "        speakers_per_batch, utterances_per_speaker = embeds.shape[:2]\n",
    "\n",
    "        # Inclusive centroids (1 per speaker). Cloning is needed for reverse differentiation\n",
    "        centroids_incl = torch.mean(embeds, dim=1, keepdim=True)\n",
    "        centroids_incl = centroids_incl.clone() / torch.norm(centroids_incl, dim=2, keepdim=True)\n",
    "\n",
    "        # Exclusive centroids (1 per utterance)\n",
    "        centroids_excl = (torch.sum(embeds, dim=1, keepdim=True) - embeds)\n",
    "        centroids_excl /= (utterances_per_speaker - 1)\n",
    "        centroids_excl = centroids_excl.clone() / torch.norm(centroids_excl, dim=2, keepdim=True)\n",
    "\n",
    "        # Similarity matrix computation\n",
    "        sim_matrix = torch.zeros(speakers_per_batch, utterances_per_speaker,\n",
    "                                 speakers_per_batch).to(self.loss_device)\n",
    "        mask_matrix = 1 - np.eye(speakers_per_batch, dtype=np.int64)\n",
    "        for j in range(speakers_per_batch):\n",
    "            mask = np.where(mask_matrix[j])[0]\n",
    "            sim_matrix[mask, :, j] = (embeds[mask] * centroids_incl[j]).sum(dim=2)\n",
    "            sim_matrix[j, :, j] = (embeds[j] * centroids_excl[j]).sum(dim=1)\n",
    "\n",
    "        sim_matrix = sim_matrix * self.similarity_weight + self.similarity_bias\n",
    "        return sim_matrix\n",
    "\n",
    "    def loss(self, embeds):\n",
    "        \"\"\"\n",
    "        Computes the softmax loss according the section 2.1 of GE2E.\n",
    "\n",
    "        :param embeds: the embeddings as a tensor of shape (speakers_per_batch,\n",
    "        utterances_per_speaker, embedding_size)\n",
    "        :return: the loss and the EER for this batch of embeddings.\n",
    "        \"\"\"\n",
    "        speakers_per_batch, utterances_per_speaker = embeds.shape[:2]\n",
    "\n",
    "        # Loss\n",
    "        sim_matrix = self.similarity_matrix(embeds)\n",
    "        sim_matrix = sim_matrix.reshape((speakers_per_batch * utterances_per_speaker,\n",
    "                                         speakers_per_batch))\n",
    "        ground_truth = np.repeat(np.arange(speakers_per_batch), utterances_per_speaker)\n",
    "        target = torch.from_numpy(ground_truth).long().to(self.loss_device)\n",
    "        loss = self.loss_fn(sim_matrix, target)\n",
    "\n",
    "        # EER (not backpropagated)\n",
    "        with torch.no_grad():\n",
    "            inv_argmax = lambda i: np.eye(1, speakers_per_batch, i, dtype=np.int64)[0]\n",
    "            labels = np.array([inv_argmax(i) for i in ground_truth])\n",
    "            preds = sim_matrix.detach().cpu().numpy()\n",
    "\n",
    "            # Snippet from https://yangcha.github.io/EER-ROC/\n",
    "            fpr, tpr, thresholds = roc_curve(labels.flatten(), preds.flatten())\n",
    "            eer = brentq(lambda x: 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n",
    "\n",
    "        return loss, eer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Speech Encoder Model Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeechrTransformerEncoderModelConfigs:\n",
    "    DEVICE = \"cuda:0\"\n",
    "    LOSS_DEVICE = \"cpu\"\n",
    "    MODEL_PATH = \"./000000036000.pt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 Load Speech Encoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def load_speaker_transformer_encoder(model_settings):\n",
    "    model = SpeechTransformerEncoder(\n",
    "        device=model_settings.DEVICE, loss_device=model_settings.LOSS_DEVICE\n",
    "    )\n",
    "    ckpt = torch.load(model_settings.MODEL_PATH, weights_only=False,\n",
    "                      map_location=torch.device('cuda:0'))\n",
    "\n",
    "    if ckpt:\n",
    "        model.load_state_dict(ckpt[\"model_state_dict\"], strict=False)\n",
    "\n",
    "    model.eval()\n",
    "    model.to(model_settings.DEVICE)\n",
    "    return nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPEECH_TRANSFORMER_ENCODER = load_speaker_transformer_encoder(\n",
    "#     SpeechrTransformerEncoderModelConfigs\n",
    "# )\n",
    "\n",
    "SPEECH_TRANSFORMER_ENCODER = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Text to Speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Text to Speech Audio Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Text2SpeechAudioConfig(AudioConfig):\n",
    "    N_MELS = 80\n",
    "    SAMPLE_RATE = 16000\n",
    "    N_FFT = 800\n",
    "    FRAME_SHIFT = 0.0125\n",
    "    FRAME_LENGTH = 0.05\n",
    "    REF_LEVEL_DB = 20\n",
    "    HOP_LENGTH = int(SAMPLE_RATE * FRAME_SHIFT)\n",
    "    WIN_LENGTH = int(SAMPLE_RATE * FRAME_LENGTH)\n",
    "    PRE_EMPHASIS = 0.97\n",
    "    POWER = 1.2\n",
    "    FMIN = 90\n",
    "    FMAX = 7600\n",
    "    ZERO_THRESHOLD = 1e-5\n",
    "    MIN_LEVEL_DB = -100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Text to Speech Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Positional Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import pprint\n",
    "\n",
    "class HParams(object):\n",
    "    def __init__(self, **kwargs): self.__dict__.update(kwargs)\n",
    "    def __setitem__(self, key, value): setattr(self, key, value)\n",
    "    def __getitem__(self, key): return getattr(self, key)\n",
    "    def __repr__(self): return pprint.pformat(self.__dict__)\n",
    "\n",
    "    def parse(self, string):\n",
    "        # Overrides hparams from a comma-separated string of name=value pairs\n",
    "        if len(string) > 0:\n",
    "            overrides = [s.split(\"=\") for s in string.split(\",\")]\n",
    "            keys, values = zip(*overrides)\n",
    "            keys = list(map(str.strip, keys))\n",
    "            values = list(map(str.strip, values))\n",
    "            for k in keys:\n",
    "                self.__dict__[k] = ast.literal_eval(values[keys.index(k)])\n",
    "        return self\n",
    "\n",
    "hparams = HParams(\n",
    "        ### Signal Processing (used in both synthesizer and vocoder)\n",
    "        sample_rate = 16000,\n",
    "        n_fft = 800,\n",
    "        num_mels = 80,\n",
    "        hop_size = 200,                             # Tacotron uses 12.5 ms frame shift (set to sample_rate * 0.0125)\n",
    "        win_size = 800,                             # Tacotron uses 50 ms frame length (set to sample_rate * 0.050)\n",
    "        fmin = 55,\n",
    "        min_level_db = -100,\n",
    "        ref_level_db = 20,\n",
    "        max_abs_value = 4.,                         # Gradient explodes if too big, premature convergence if too small.\n",
    "        preemphasis = 0.97,                         # Filter coefficient to use if preemphasize is True\n",
    "        preemphasize = True,\n",
    "\n",
    "        ### Tacotron Text-to-Speech (TTS)\n",
    "        tts_embed_dims = 512,                       # Embedding dimension for the graphemes/phoneme inputs\n",
    "        tts_encoder_dims = 256,\n",
    "        tts_decoder_dims = 128,\n",
    "        tts_postnet_dims = 512,\n",
    "        tts_encoder_K = 5,\n",
    "        tts_lstm_dims = 1024,\n",
    "        tts_postnet_K = 5,\n",
    "        tts_num_highways = 4,\n",
    "        tts_dropout = 0.5,\n",
    "        tts_cleaner_names = [\"english_cleaners\"],\n",
    "        tts_stop_threshold = -3.4,                  # Value below which audio generation ends.\n",
    "                                                    # For example, for a range of [-4, 4], this\n",
    "                                                    # will terminate the sequence at the first\n",
    "                                                    # frame that has all values < -3.4\n",
    "\n",
    "        ### Tacotron Training\n",
    "        tts_schedule = [(2,  1e-3,  20_000,  2),   # Progressive training schedule\n",
    "                        (2,  5e-4,  40_000,  2),   # (r, lr, step, batch_size)\n",
    "                        (2,  2e-4,  80_000,  2),   #\n",
    "                        (2,  1e-4, 160_000,  2),   # r = reduction factor (# of mel frames\n",
    "                        (2,  3e-5, 320_000,  2),   #     synthesized for each decoder iteration)\n",
    "                        (2,  1e-5, 640_000,  2)],  # lr = learning rate\n",
    "\n",
    "        tts_clip_grad_norm = 1.0,                   # clips the gradient norm to prevent explosion - set to None if not needed\n",
    "        tts_eval_interval = 500,                    # Number of steps between model evaluation (sample generation)\n",
    "                                                    # Set to -1 to generate after completing epoch, or 0 to disable\n",
    "\n",
    "        tts_eval_num_samples = 1,                   # Makes this number of samples\n",
    "\n",
    "        ### Data Preprocessing\n",
    "        max_mel_frames = 900,\n",
    "        rescale = True,\n",
    "        rescaling_max = 0.9,\n",
    "        synthesis_batch_size = 16,                  # For vocoder preprocessing and inference.\n",
    "\n",
    "        ### Mel Visualization and Griffin-Lim\n",
    "        signal_normalization = True,\n",
    "        power = 1.5,\n",
    "        griffin_lim_iters = 60,\n",
    "\n",
    "        ### Audio processing options\n",
    "        fmax = 7600,                                # Should not exceed (sample_rate // 2)\n",
    "        allow_clipping_in_normalization = True,     # Used when signal_normalization = True\n",
    "        clip_mels_length = True,                    # If true, discards samples exceeding max_mel_frames\n",
    "        use_lws = False,                            # \"Fast spectrogram phase recovery using local weighted sums\"\n",
    "        symmetric_mels = True,                      # Sets mel range to [-max_abs_value, max_abs_value] if True,\n",
    "                                                    #               and [0, max_abs_value] if False\n",
    "        trim_silence = True,                        # Use with sample_rate of 16000 for best results\n",
    "\n",
    "        ### SV2TTS\n",
    "        speaker_embedding_size = 256,               # Dimension for the speaker embedding\n",
    "        silence_min_duration_split = 0.4,           # Duration in seconds of a silence for an utterance to be split\n",
    "        utterance_min_duration = 1.6,               # Duration in seconds below which utterances are discarded\n",
    "        )\n",
    "\n",
    "def hparams_debug_string():\n",
    "    return str(hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from typing import Union\n",
    "\n",
    "\n",
    "class HighwayNetwork(nn.Module):\n",
    "    def __init__(self, size):\n",
    "        super().__init__()\n",
    "        self.W1 = nn.Linear(size, size)\n",
    "        self.W2 = nn.Linear(size, size)\n",
    "        self.W1.bias.data.fill_(0.)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.W1(x)\n",
    "        x2 = self.W2(x)\n",
    "        g = torch.sigmoid(x2)\n",
    "        y = g * F.relu(x1) + (1. - g) * x\n",
    "        return y\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, embed_dims, num_chars, encoder_dims, K, num_highways, dropout):\n",
    "        super().__init__()\n",
    "        prenet_dims = (encoder_dims, encoder_dims)\n",
    "        cbhg_channels = encoder_dims\n",
    "        self.embedding = nn.Embedding(num_chars, embed_dims)\n",
    "        self.pre_net = PreNet(embed_dims, fc1_dims=prenet_dims[0], fc2_dims=prenet_dims[1],\n",
    "                              dropout=dropout)\n",
    "        self.cbhg = CBHG(K=K, in_channels=cbhg_channels, channels=cbhg_channels,\n",
    "                         proj_channels=[cbhg_channels, cbhg_channels],\n",
    "                         num_highways=num_highways)\n",
    "\n",
    "    def forward(self, x, speaker_embedding=None):\n",
    "        x = self.embedding(x)\n",
    "        x = self.pre_net(x)\n",
    "        x.transpose_(1, 2)\n",
    "        x = self.cbhg(x)\n",
    "        if speaker_embedding is not None:\n",
    "            x = self.add_speaker_embedding(x, speaker_embedding)\n",
    "        return x\n",
    "\n",
    "    def add_speaker_embedding(self, x, speaker_embedding):\n",
    "        # SV2TTS\n",
    "        # The input x is the encoder output and is a 3D tensor with size (batch_size, num_chars, tts_embed_dims)\n",
    "        # When training, speaker_embedding is also a 2D tensor with size (batch_size, speaker_embedding_size)\n",
    "        #     (for inference, speaker_embedding is a 1D tensor with size (speaker_embedding_size))\n",
    "        # This concats the speaker embedding for each char in the encoder output\n",
    "\n",
    "        # Save the dimensions as human-readable names\n",
    "        batch_size = x.size()[0]\n",
    "        num_chars = x.size()[1]\n",
    "\n",
    "        if speaker_embedding.dim() == 1:\n",
    "            idx = 0\n",
    "        else:\n",
    "            idx = 1\n",
    "\n",
    "        # Start by making a copy of each speaker embedding to match the input text length\n",
    "        # The output of this has size (batch_size, num_chars * tts_embed_dims)\n",
    "        speaker_embedding_size = speaker_embedding.size()[idx]\n",
    "        e = speaker_embedding.repeat_interleave(num_chars, dim=idx)\n",
    "\n",
    "        # Reshape it and transpose\n",
    "        e = e.reshape(batch_size, speaker_embedding_size, num_chars)\n",
    "        e = e.transpose(1, 2)\n",
    "\n",
    "        # Concatenate the tiled speaker embedding with the encoder output\n",
    "        x = torch.cat((x, e), 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class BatchNormConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel, relu=True):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv1d(in_channels, out_channels, kernel, stride=1, padding=kernel // 2, bias=False)\n",
    "        self.bnorm = nn.BatchNorm1d(out_channels)\n",
    "        self.relu = relu\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = F.relu(x) if self.relu is True else x\n",
    "        return self.bnorm(x)\n",
    "\n",
    "\n",
    "class CBHG(nn.Module):\n",
    "    def __init__(self, K, in_channels, channels, proj_channels, num_highways):\n",
    "        super().__init__()\n",
    "\n",
    "        # List of all rnns to call `flatten_parameters()` on\n",
    "        self._to_flatten = []\n",
    "\n",
    "        self.bank_kernels = [i for i in range(1, K + 1)]\n",
    "        self.conv1d_bank = nn.ModuleList()\n",
    "        for k in self.bank_kernels:\n",
    "            conv = BatchNormConv(in_channels, channels, k)\n",
    "            self.conv1d_bank.append(conv)\n",
    "\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=2, stride=1, padding=1)\n",
    "\n",
    "        self.conv_project1 = BatchNormConv(len(self.bank_kernels) * channels, proj_channels[0], 3)\n",
    "        self.conv_project2 = BatchNormConv(proj_channels[0], proj_channels[1], 3, relu=False)\n",
    "\n",
    "        # Fix the highway input if necessary\n",
    "        if proj_channels[-1] != channels:\n",
    "            self.highway_mismatch = True\n",
    "            self.pre_highway = nn.Linear(proj_channels[-1], channels, bias=False)\n",
    "        else:\n",
    "            self.highway_mismatch = False\n",
    "\n",
    "        self.highways = nn.ModuleList()\n",
    "        for i in range(num_highways):\n",
    "            hn = HighwayNetwork(channels)\n",
    "            self.highways.append(hn)\n",
    "\n",
    "        self.rnn = nn.GRU(channels, channels // 2, batch_first=True, bidirectional=True)\n",
    "        self._to_flatten.append(self.rnn)\n",
    "\n",
    "        # Avoid fragmentation of RNN parameters and associated warning\n",
    "        self._flatten_parameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Although we `_flatten_parameters()` on init, when using DataParallel\n",
    "        # the model gets replicated, making it no longer guaranteed that the\n",
    "        # weights are contiguous in GPU memory. Hence, we must call it again\n",
    "        self._flatten_parameters()\n",
    "\n",
    "        # Save these for later\n",
    "        residual = x\n",
    "        seq_len = x.size(-1)\n",
    "        conv_bank = []\n",
    "\n",
    "        # Convolution Bank\n",
    "        for conv in self.conv1d_bank:\n",
    "            c = conv(x) # Convolution\n",
    "            conv_bank.append(c[:, :, :seq_len])\n",
    "\n",
    "        # Stack along the channel axis\n",
    "        conv_bank = torch.cat(conv_bank, dim=1)\n",
    "\n",
    "        # dump the last padding to fit residual\n",
    "        x = self.maxpool(conv_bank)[:, :, :seq_len]\n",
    "\n",
    "        # Conv1d projections\n",
    "        x = self.conv_project1(x)\n",
    "        x = self.conv_project2(x)\n",
    "\n",
    "        # Residual Connect\n",
    "        x = x + residual\n",
    "\n",
    "        # Through the highways\n",
    "        x = x.transpose(1, 2)\n",
    "        if self.highway_mismatch is True:\n",
    "            x = self.pre_highway(x)\n",
    "        for h in self.highways: x = h(x)\n",
    "\n",
    "        # And then the RNN\n",
    "        x, _ = self.rnn(x)\n",
    "        return x\n",
    "\n",
    "    def _flatten_parameters(self):\n",
    "        \"\"\"Calls `flatten_parameters` on all the rnns used by the WaveRNN. Used\n",
    "        to improve efficiency and avoid PyTorch yelling at us.\"\"\"\n",
    "        [m.flatten_parameters() for m in self._to_flatten]\n",
    "\n",
    "class PreNet(nn.Module):\n",
    "    def __init__(self, in_dims, fc1_dims=256, fc2_dims=128, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_dims, fc1_dims)\n",
    "        self.fc2 = nn.Linear(fc1_dims, fc2_dims)\n",
    "        self.p = dropout\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, self.p, training=True)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, self.p, training=True)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, attn_dims):\n",
    "        super().__init__()\n",
    "        self.W = nn.Linear(attn_dims, attn_dims, bias=False)\n",
    "        self.v = nn.Linear(attn_dims, 1, bias=False)\n",
    "\n",
    "    def forward(self, encoder_seq_proj, query, t):\n",
    "\n",
    "        # print(encoder_seq_proj.shape)\n",
    "        # Transform the query vector\n",
    "        query_proj = self.W(query).unsqueeze(1)\n",
    "\n",
    "        # Compute the scores\n",
    "        u = self.v(torch.tanh(encoder_seq_proj + query_proj))\n",
    "        scores = F.softmax(u, dim=1)\n",
    "\n",
    "        return scores.transpose(1, 2)\n",
    "\n",
    "\n",
    "class LSA(nn.Module):\n",
    "    def __init__(self, attn_dim, kernel_size=31, filters=32):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv1d(1, filters, padding=(kernel_size - 1) // 2, kernel_size=kernel_size, bias=True)\n",
    "        self.L = nn.Linear(filters, attn_dim, bias=False)\n",
    "        self.W = nn.Linear(attn_dim, attn_dim, bias=True) # Include the attention bias in this term\n",
    "        self.v = nn.Linear(attn_dim, 1, bias=False)\n",
    "        self.cumulative = None\n",
    "        self.attention = None\n",
    "\n",
    "    def init_attention(self, encoder_seq_proj):\n",
    "        device = next(self.parameters()).device  # use same device as parameters\n",
    "        b, t, c = encoder_seq_proj.size()\n",
    "        self.cumulative = torch.zeros(b, t, device=device)\n",
    "        self.attention = torch.zeros(b, t, device=device)\n",
    "\n",
    "    def forward(self, encoder_seq_proj, query, t, chars):\n",
    "\n",
    "        if t == 0: self.init_attention(encoder_seq_proj)\n",
    "\n",
    "        processed_query = self.W(query).unsqueeze(1)\n",
    "\n",
    "        location = self.cumulative.unsqueeze(1)\n",
    "        processed_loc = self.L(self.conv(location).transpose(1, 2))\n",
    "\n",
    "        u = self.v(torch.tanh(processed_query + encoder_seq_proj + processed_loc))\n",
    "        u = u.squeeze(-1)\n",
    "\n",
    "        # Mask zero padding chars\n",
    "        u = u * (chars != 0).float()\n",
    "\n",
    "        # Smooth Attention\n",
    "        # scores = torch.sigmoid(u) / torch.sigmoid(u).sum(dim=1, keepdim=True)\n",
    "        scores = F.softmax(u, dim=1)\n",
    "        self.attention = scores\n",
    "        self.cumulative = self.cumulative + self.attention\n",
    "\n",
    "        return scores.unsqueeze(-1).transpose(1, 2)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    # Class variable because its value doesn't change between classes\n",
    "    # yet ought to be scoped by class because its a property of a Decoder\n",
    "    max_r = 20\n",
    "    def __init__(self, n_mels, encoder_dims, decoder_dims, lstm_dims,\n",
    "                 dropout, speaker_embedding_size):\n",
    "        super().__init__()\n",
    "        self.register_buffer(\"r\", torch.tensor(1, dtype=torch.int))\n",
    "        self.n_mels = n_mels\n",
    "        prenet_dims = (decoder_dims * 2, decoder_dims * 2)\n",
    "        self.prenet = PreNet(n_mels, fc1_dims=prenet_dims[0], fc2_dims=prenet_dims[1],\n",
    "                             dropout=dropout)\n",
    "        self.attn_net = LSA(decoder_dims)\n",
    "        self.attn_rnn = nn.GRUCell(encoder_dims + prenet_dims[1] + speaker_embedding_size, decoder_dims)\n",
    "        self.rnn_input = nn.Linear(encoder_dims + decoder_dims + speaker_embedding_size, lstm_dims)\n",
    "        self.res_rnn1 = nn.LSTMCell(lstm_dims, lstm_dims)\n",
    "        self.res_rnn2 = nn.LSTMCell(lstm_dims, lstm_dims)\n",
    "        self.mel_proj = nn.Linear(lstm_dims, n_mels * self.max_r, bias=False)\n",
    "        self.stop_proj = nn.Linear(encoder_dims + speaker_embedding_size + lstm_dims, 1)\n",
    "\n",
    "    def zoneout(self, prev, current, p=0.1):\n",
    "        device = next(self.parameters()).device  # Use same device as parameters\n",
    "        mask = torch.zeros(prev.size(), device=device).bernoulli_(p)\n",
    "        return prev * mask + current * (1 - mask)\n",
    "\n",
    "    def forward(self, encoder_seq, encoder_seq_proj, prenet_in,\n",
    "                hidden_states, cell_states, context_vec, t, chars):\n",
    "\n",
    "        # Need this for reshaping mels\n",
    "        batch_size = encoder_seq.size(0)\n",
    "\n",
    "        # Unpack the hidden and cell states\n",
    "        attn_hidden, rnn1_hidden, rnn2_hidden = hidden_states\n",
    "        rnn1_cell, rnn2_cell = cell_states\n",
    "\n",
    "        # PreNet for the Attention RNN\n",
    "        prenet_out = self.prenet(prenet_in)\n",
    "\n",
    "        # Compute the Attention RNN hidden state\n",
    "        attn_rnn_in = torch.cat([context_vec, prenet_out], dim=-1)\n",
    "        attn_hidden = self.attn_rnn(attn_rnn_in.squeeze(1), attn_hidden)\n",
    "\n",
    "        # Compute the attention scores\n",
    "        scores = self.attn_net(encoder_seq_proj, attn_hidden, t, chars)\n",
    "\n",
    "        # Dot product to create the context vector\n",
    "        context_vec = scores @ encoder_seq\n",
    "        context_vec = context_vec.squeeze(1)\n",
    "\n",
    "        # Concat Attention RNN output w. Context Vector & project\n",
    "        x = torch.cat([context_vec, attn_hidden], dim=1)\n",
    "        x = self.rnn_input(x)\n",
    "\n",
    "        # Compute first Residual RNN\n",
    "        rnn1_hidden_next, rnn1_cell = self.res_rnn1(x, (rnn1_hidden, rnn1_cell))\n",
    "        if self.training:\n",
    "            rnn1_hidden = self.zoneout(rnn1_hidden, rnn1_hidden_next)\n",
    "        else:\n",
    "            rnn1_hidden = rnn1_hidden_next\n",
    "        x = x + rnn1_hidden\n",
    "\n",
    "        # Compute second Residual RNN\n",
    "        rnn2_hidden_next, rnn2_cell = self.res_rnn2(x, (rnn2_hidden, rnn2_cell))\n",
    "        if self.training:\n",
    "            rnn2_hidden = self.zoneout(rnn2_hidden, rnn2_hidden_next)\n",
    "        else:\n",
    "            rnn2_hidden = rnn2_hidden_next\n",
    "        x = x + rnn2_hidden\n",
    "\n",
    "        # Project Mels\n",
    "        mels = self.mel_proj(x)\n",
    "        mels = mels.view(batch_size, self.n_mels, self.max_r)[:, :, :self.r]\n",
    "        hidden_states = (attn_hidden, rnn1_hidden, rnn2_hidden)\n",
    "        cell_states = (rnn1_cell, rnn2_cell)\n",
    "\n",
    "        # Stop token prediction\n",
    "        s = torch.cat((x, context_vec), dim=1)\n",
    "        s = self.stop_proj(s)\n",
    "        stop_tokens = torch.sigmoid(s)\n",
    "\n",
    "        return mels, scores, hidden_states, cell_states, context_vec, stop_tokens\n",
    "\n",
    "\n",
    "class Tacotron(nn.Module):\n",
    "    def __init__(self, embed_dims=512, num_chars=81, encoder_dims=256, decoder_dims=128, n_mels=80, \n",
    "                 fft_bins=80, postnet_dims=512, encoder_K=5, lstm_dims=1024, postnet_K=5, num_highways=4,\n",
    "                 dropout=0.5, stop_threshold=-3.4, speaker_embedding_size=256):\n",
    "        super().__init__()\n",
    "        self.n_mels = n_mels\n",
    "        self.lstm_dims = lstm_dims\n",
    "        self.encoder_dims = encoder_dims\n",
    "        self.decoder_dims = decoder_dims\n",
    "        self.speaker_embedding_size = speaker_embedding_size\n",
    "        self.encoder = Encoder(embed_dims, num_chars, encoder_dims,\n",
    "                               encoder_K, num_highways, dropout)\n",
    "        self.encoder_proj = nn.Linear(encoder_dims + speaker_embedding_size, decoder_dims, bias=False)\n",
    "        self.decoder = Decoder(n_mels, encoder_dims, decoder_dims, lstm_dims,\n",
    "                               dropout, speaker_embedding_size)\n",
    "        self.postnet = CBHG(postnet_K, n_mels, postnet_dims,\n",
    "                            [postnet_dims, fft_bins], num_highways)\n",
    "        self.post_proj = nn.Linear(postnet_dims, fft_bins, bias=False)\n",
    "\n",
    "        self.init_model()\n",
    "        self.num_params()\n",
    "\n",
    "        self.register_buffer(\"step\", torch.zeros(1, dtype=torch.long))\n",
    "        self.register_buffer(\"stop_threshold\", torch.tensor(stop_threshold, dtype=torch.float32))\n",
    "\n",
    "    @property\n",
    "    def r(self):\n",
    "        return self.decoder.r.item()\n",
    "\n",
    "    @r.setter\n",
    "    def r(self, value):\n",
    "        self.decoder.r = self.decoder.r.new_tensor(value, requires_grad=False)\n",
    "\n",
    "    def forward(self, x, m, speaker_embedding):\n",
    "        device = next(self.parameters()).device  # use same device as parameters\n",
    "\n",
    "        self.step += 1\n",
    "        batch_size, _, steps  = m.size()\n",
    "\n",
    "        # Initialise all hidden states and pack into tuple\n",
    "        attn_hidden = torch.zeros(batch_size, self.decoder_dims, device=device)\n",
    "        rnn1_hidden = torch.zeros(batch_size, self.lstm_dims, device=device)\n",
    "        rnn2_hidden = torch.zeros(batch_size, self.lstm_dims, device=device)\n",
    "        hidden_states = (attn_hidden, rnn1_hidden, rnn2_hidden)\n",
    "\n",
    "        # Initialise all lstm cell states and pack into tuple\n",
    "        rnn1_cell = torch.zeros(batch_size, self.lstm_dims, device=device)\n",
    "        rnn2_cell = torch.zeros(batch_size, self.lstm_dims, device=device)\n",
    "        cell_states = (rnn1_cell, rnn2_cell)\n",
    "\n",
    "        # <GO> Frame for start of decoder loop\n",
    "        go_frame = torch.zeros(batch_size, self.n_mels, device=device)\n",
    "\n",
    "        # Need an initial context vector\n",
    "        context_vec = torch.zeros(batch_size, self.encoder_dims + self.speaker_embedding_size, device=device)\n",
    "\n",
    "        # SV2TTS: Run the encoder with the speaker embedding\n",
    "        # The projection avoids unnecessary matmuls in the decoder loop\n",
    "        encoder_seq = self.encoder(x, speaker_embedding)\n",
    "        encoder_seq_proj = self.encoder_proj(encoder_seq)\n",
    "\n",
    "        # Need a couple of lists for outputs\n",
    "        mel_outputs, attn_scores, stop_outputs = [], [], []\n",
    "\n",
    "        # Run the decoder loop\n",
    "        for t in range(0, steps, self.r):\n",
    "            prenet_in = m[:, :, t - 1] if t > 0 else go_frame\n",
    "            mel_frames, scores, hidden_states, cell_states, context_vec, stop_tokens = \\\n",
    "                self.decoder(encoder_seq, encoder_seq_proj, prenet_in,\n",
    "                             hidden_states, cell_states, context_vec, t, x)\n",
    "            mel_outputs.append(mel_frames)\n",
    "            attn_scores.append(scores)\n",
    "            stop_outputs.extend([stop_tokens] * self.r)\n",
    "\n",
    "        # Concat the mel outputs into sequence\n",
    "        mel_outputs = torch.cat(mel_outputs, dim=2)\n",
    "\n",
    "        # Post-Process for Linear Spectrograms\n",
    "        postnet_out = self.postnet(mel_outputs)\n",
    "        linear = self.post_proj(postnet_out)\n",
    "        linear = linear.transpose(1, 2)\n",
    "\n",
    "        # For easy visualisation\n",
    "        attn_scores = torch.cat(attn_scores, 1)\n",
    "        # attn_scores = attn_scores.cpu().data.numpy()\n",
    "        stop_outputs = torch.cat(stop_outputs, 1)\n",
    "\n",
    "        return mel_outputs, linear, attn_scores, stop_outputs\n",
    "\n",
    "    def generate(self, x, speaker_embedding=None, steps=2000):\n",
    "        self.eval()\n",
    "        device = next(self.parameters()).device  # use same device as parameters\n",
    "\n",
    "        batch_size, _  = x.size()\n",
    "\n",
    "        # Need to initialise all hidden states and pack into tuple for tidyness\n",
    "        attn_hidden = torch.zeros(batch_size, self.decoder_dims, device=device)\n",
    "        rnn1_hidden = torch.zeros(batch_size, self.lstm_dims, device=device)\n",
    "        rnn2_hidden = torch.zeros(batch_size, self.lstm_dims, device=device)\n",
    "        hidden_states = (attn_hidden, rnn1_hidden, rnn2_hidden)\n",
    "\n",
    "        # Need to initialise all lstm cell states and pack into tuple for tidyness\n",
    "        rnn1_cell = torch.zeros(batch_size, self.lstm_dims, device=device)\n",
    "        rnn2_cell = torch.zeros(batch_size, self.lstm_dims, device=device)\n",
    "        cell_states = (rnn1_cell, rnn2_cell)\n",
    "\n",
    "        # Need a <GO> Frame for start of decoder loop\n",
    "        go_frame = torch.zeros(batch_size, self.n_mels, device=device)\n",
    "\n",
    "        # Need an initial context vector\n",
    "        context_vec = torch.zeros(batch_size, self.encoder_dims + self.speaker_embedding_size, device=device)\n",
    "\n",
    "        # SV2TTS: Run the encoder with the speaker embedding\n",
    "        # The projection avoids unnecessary matmuls in the decoder loop\n",
    "        encoder_seq = self.encoder(x, speaker_embedding)\n",
    "        encoder_seq_proj = self.encoder_proj(encoder_seq)\n",
    "\n",
    "        # Need a couple of lists for outputs\n",
    "        mel_outputs, attn_scores, stop_outputs = [], [], []\n",
    "\n",
    "        # Run the decoder loop\n",
    "        for t in range(0, steps, self.r):\n",
    "            prenet_in = mel_outputs[-1][:, :, -1] if t > 0 else go_frame\n",
    "            mel_frames, scores, hidden_states, cell_states, context_vec, stop_tokens = \\\n",
    "            self.decoder(encoder_seq, encoder_seq_proj, prenet_in,\n",
    "                         hidden_states, cell_states, context_vec, t, x)\n",
    "            mel_outputs.append(mel_frames)\n",
    "            attn_scores.append(scores)\n",
    "            stop_outputs.extend([stop_tokens] * self.r)\n",
    "            # Stop the loop when all stop tokens in batch exceed threshold\n",
    "            if (stop_tokens > 0.5).all() and t > 10: break\n",
    "\n",
    "        # Concat the mel outputs into sequence\n",
    "        mel_outputs = torch.cat(mel_outputs, dim=2)\n",
    "\n",
    "        # Post-Process for Linear Spectrograms\n",
    "        postnet_out = self.postnet(mel_outputs)\n",
    "        linear = self.post_proj(postnet_out)\n",
    "\n",
    "\n",
    "        linear = linear.transpose(1, 2)\n",
    "\n",
    "        # For easy visualisation\n",
    "        attn_scores = torch.cat(attn_scores, 1)\n",
    "        stop_outputs = torch.cat(stop_outputs, 1)\n",
    "\n",
    "        self.train()\n",
    "\n",
    "        return mel_outputs, linear, attn_scores\n",
    "\n",
    "    def init_model(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1: nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def get_step(self):\n",
    "        return self.step.data.item()\n",
    "\n",
    "    def reset_step(self):\n",
    "        # assignment to parameters or buffers is overloaded, updates internal dict entry\n",
    "        self.step = self.step.data.new_tensor(1)\n",
    "\n",
    "    def log(self, path, msg):\n",
    "        with open(path, \"a\") as f:\n",
    "            print(msg, file=f)\n",
    "\n",
    "    def load(self, path, optimizer=None):\n",
    "        # Use device of model params as location for loaded state\n",
    "        device = next(self.parameters()).device\n",
    "        checkpoint = torch.load(str(path), map_location=device)\n",
    "        self.load_state_dict(checkpoint[\"model_state\"])\n",
    "\n",
    "        if \"optimizer_state\" in checkpoint and optimizer is not None:\n",
    "            optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n",
    "\n",
    "    def save(self, path, optimizer=None):\n",
    "        if optimizer is not None:\n",
    "            torch.save({\n",
    "                \"model_state\": self.state_dict(),\n",
    "                \"optimizer_state\": optimizer.state_dict(),\n",
    "            }, str(path))\n",
    "        else:\n",
    "            torch.save({\n",
    "                \"model_state\": self.state_dict(),\n",
    "            }, str(path))\n",
    "\n",
    "\n",
    "    def num_params(self, print_out=True):\n",
    "        parameters = filter(lambda p: p.requires_grad, self.parameters())\n",
    "        parameters = sum([np.prod(p.size()) for p in parameters]) / 1_000_000\n",
    "        if print_out:\n",
    "            print(\"Trainable Parameters: %.3fM\" % parameters)\n",
    "        return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable Parameters: 30.878M\n"
     ]
    }
   ],
   "source": [
    "model = Tacotron()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4887/1550932188.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\"synthesizer.pt\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(\"synthesizer.pt\")\n",
    "model_dict = model.state_dict()\n",
    "\n",
    "# Filter out the embedding layer from the checkpoint\n",
    "filtered_dict = {k: v for k, v in checkpoint['model_state'].items() if \"encoder.embedding\" not in k}\n",
    "\n",
    "# Load the filtered state dict\n",
    "model_dict.update(filtered_dict)\n",
    "model.load_state_dict(model_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Dataset Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def load_mel(file_path: Path):\n",
    "    \"\"\"\n",
    "    Load a Mel spectrogram saved as a .npy file.\n",
    "\n",
    "    :param file_path: Path to the .npy file containing the Mel spectrogram\n",
    "    :return: Numpy array of the Mel spectrogram\n",
    "    \"\"\"\n",
    "    if not file_path.exists():\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    try:\n",
    "        mel_spectrogram = np.load(file_path)\n",
    "    except EOFError:\n",
    "        print(f\"File could not be loaded due to EOFError: {random_file}\")\n",
    "        return False\n",
    "    return mel_spectrogram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "\n",
    "class TTSDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, root: Path=None, text_map: dict=None, text_map_file: Path=None, tokenizer=None):\n",
    "        \"\"\"\n",
    "        :param root: Path to the audio files directory\n",
    "        :param text_map: A dictionary mapping audio file names to their corresponding texts\n",
    "        :param text_map_file: Path to the JSON file containing the text map\n",
    "        :param tokenizer: A custom tokenizer for text processing\n",
    "        \"\"\"\n",
    "        self.root = root\n",
    "        self.text_map = text_map if text_map is not None else self.load_text_map(text_map_file)\n",
    "        # self.audios = os.listdir(\"./encoded_speechs\")\n",
    "        self.audios = [f[:-4] for f in os.listdir(\"./trimmed_mels\")]\n",
    "        self.texts = self.text_map\n",
    "        self.tokenizer = tokenizer if tokenizer is not None else self.default_tokenizer\n",
    "        self.encoded_speech_dir = Path(\"./encoded_speechs\")\n",
    "        self.mel_dir = Path(\"./trimmed_mels\")\n",
    "\n",
    "        \n",
    "    def load_text_map(self, text_map_file: Path):\n",
    "        with text_map_file.open('r') as file:\n",
    "            text_map = json.load(file)\n",
    "        return text_map\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.audios)\n",
    "\n",
    "    def _get_audio_input(self, speech_output):\n",
    "        \"\"\"\n",
    "        Method to get input audio for the decoder, used for inference, like teacher-forcing mode\n",
    "        \"\"\"\n",
    "        speech_input = np.concatenate([np.zeros([1, Text2SpeechAudioConfig.N_MELS], np.float32), \n",
    "                                       speech_output[:-1, :]], axis=0)\n",
    "        return speech_input\n",
    "\n",
    "    def _get_audio_output(self, idx):\n",
    "        \"\"\"\n",
    "        Get ground truth audio (target output), loading from .npy file if it exists.\n",
    "        \"\"\"\n",
    "        mel_file = self.mel_dir / f\"{self.audios[idx]}.npy\"\n",
    "        \n",
    "        if mel_file.exists():\n",
    "            return load_mel(mel_file)\n",
    "        else:\n",
    "            print(f\"not found: {mel_file}\")\n",
    "            utterance = Utterance(\n",
    "                raw_file=self.root / f\"{self.audios[idx]}.npy\",\n",
    "                processor=AudioPreprocessor(Text2SpeechAudioConfig)\n",
    "            )\n",
    "            return utterance.mel_in_db().T\n",
    "    \n",
    "    def default_tokenizer(self, text):\n",
    "        \"\"\"\n",
    "        Default tokenizer method if no custom tokenizer is provided.\n",
    "        Tokenizes text to a sequence of integers or phonemes.\n",
    "        \"\"\"\n",
    "        # Example: basic character tokenizer, can be replaced with more complex tokenizers\n",
    "        return [ord(char) for char in text]\n",
    "    \n",
    "    def _get_encoded_speech(self, idx):\n",
    "        \"\"\"\n",
    "        Get encoded speech randomly from a folder.\n",
    "        \"\"\"\n",
    "        audio_name = self.audios[idx]  # Extract the audio name without extension\n",
    "        encoded_speech_subfolder = self.encoded_speech_dir / audio_name  # Path to subfolder\n",
    "        encoded_speech_files = list(encoded_speech_subfolder.glob(\"*.npy\"))  # List of all `.pt` files\n",
    "        \n",
    "        if not encoded_speech_files:\n",
    "            utterance = Utterance(raw_file=self.root / self.audios[idx], \n",
    "                              processor=AudioPreprocessor(SpeakerEncoderAudioConfig))\n",
    "        \n",
    "            random_mel = torch.tensor(np.array([utterance.random_mel_in_db(4800)])).transpose(1, 2)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                encoded_speech = SPEECH_TRANSFORMER_ENCODER(random_mel)\n",
    "                \n",
    "            return encoded_speech\n",
    "        \n",
    "        random_file = random.choice(encoded_speech_files)  # Select a random `.pt` file\n",
    "        try:\n",
    "            encoded_speech = np.load(random_file)\n",
    "        except EOFError:\n",
    "            utterance = Utterance(raw_file=self.root / self.audios[idx], \n",
    "                              processor=AudioPreprocessor(SpeakerEncoderAudioConfig))\n",
    "        \n",
    "            random_mel = torch.tensor(np.array([utterance.random_mel_in_db(4800)])).transpose(1, 2)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                encoded_speech = SPEECH_TRANSFORMER_ENCODER(random_mel)\n",
    "                \n",
    "            return encoded_speech.cpu().numpy()\n",
    "        \n",
    "        return encoded_speech\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get text sequence, input audio (for teacher-forcing), and output audio (ground truth)\n",
    "        \"\"\"\n",
    "        # Text to sequence using custom tokenizer\n",
    "        text_sequence = self.tokenizer(self.texts[self.audios[idx]])\n",
    "\n",
    "        # Get Encoding Speech\n",
    "        encoded_speech = self._get_encoded_speech(idx)\n",
    "\n",
    "        # Get the audio output (target output)\n",
    "        output_audio = self._get_audio_output(idx)\n",
    "        \n",
    "        return text_sequence, output_audio, encoded_speech, idx, output_audio.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_synthesizer(batch, r, hparams):\n",
    "    # Text\n",
    "    x_lens = [len(x[0]) for x in batch]\n",
    "    max_x_len = max(x_lens)\n",
    "\n",
    "    texts = [pad1d(x[0], max_x_len) for x in batch]\n",
    "    texts = np.stack(texts)\n",
    "\n",
    "    # Mel spectrogram\n",
    "    spec_lens = [x[1].shape[-1] for x in batch]\n",
    "    max_spec_len = max(spec_lens) + 1 \n",
    "    # if max_spec_len % r != 0:\n",
    "    #     max_spec_len += r - max_spec_len % r \n",
    "\n",
    "    # WaveRNN mel spectrograms are normalized to [0, 1] so zero padding adds silence\n",
    "    # By default, SV2TTS uses symmetric mels, where -1*max_abs_value is silence.\n",
    "    # if hparams.symmetric_mels:\n",
    "    #     mel_pad_value = -1 * hparams.max_abs_value\n",
    "    # else:\n",
    "    #     mel_pad_value = 0\n",
    "\n",
    "    # mel = [pad2d(x[1], max_spec_len, pad_value=mel_pad_value) for x in batch]\n",
    "    mel = [pad2d(x[1], max_spec_len, pad_value=0) for x in batch]\n",
    "    mel = np.stack(mel)\n",
    "\n",
    "    # Speaker embedding (SV2TTS)\n",
    "    embeds = np.array([x[2] for x in batch])\n",
    "\n",
    "    # Index (for vocoder preprocessing)\n",
    "    indices = [x[3] for x in batch]\n",
    "    mel_len = [x[4] for x in batch]\n",
    "\n",
    "    # Convert all to tensor\n",
    "    texts = torch.tensor(texts).long()\n",
    "    mel = torch.tensor(mel)\n",
    "    embeds = torch.tensor(embeds)\n",
    "    mel_len = torch.tensor(mel_len)\n",
    "\n",
    "    return texts, mel, embeds, indices, mel_len\n",
    "\n",
    "def pad1d(x, max_len, pad_value=0):\n",
    "    return np.pad(x, (0, max_len - len(x)), mode=\"constant\", constant_values=pad_value)\n",
    "\n",
    "def pad2d(x, max_len, pad_value=0):\n",
    "    return np.pad(x, ((0, 0), (0, max_len - x.shape[-1])), mode=\"constant\", constant_values=pad_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "DATA_PATH = Path(r\".\")\n",
    "TEXT_MAP_PATH = Path(r\"./transcripts.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TTSDataset(root=DATA_PATH, text_map_file=TEXT_MAP_PATH, tokenizer=WordByPhonemesEmbedding())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size], generator=torch.Generator().manual_seed(42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Trainning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(\n",
    "    model, optimizer, epoch,train_losses, eval_losses, save_path\n",
    "):\n",
    "    checkpoint = {\n",
    "        \"epoch\": epoch,\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"train_losses\": train_losses,\n",
    "        \"eval_losses\": eval_losses,\n",
    "    }\n",
    "    model_path = f\"{save_path}/model_epoch_{epoch}_subset_{current_subset}.pt\"\n",
    "    torch.save(checkpoint, f\"{model_path}\")\n",
    "    print(f\"Checkpoint saved at {model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def load_checkpoint(save_path, model, optimizer, device):\n",
    "    if os.path.exists(save_path):\n",
    "        checkpoint = torch.load(save_path, map_location=device)\n",
    "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "        epoch = checkpoint[\"epoch\"]\n",
    "        train_losses = checkpoint[\"train_losses\"]\n",
    "        eval_losses = checkpoint[\"eval_losses\"]\n",
    "        print(f\"Checkpoint loaded from {save_path}\")\n",
    "        return model, optimizer, epoch, train_losses, eval_losses\n",
    "    else:\n",
    "        print(f\"No checkpoint found at {save_path}, starting fresh.\")\n",
    "        return model, optimizer, 0, [], []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def np_now(x: torch.Tensor): return x.detach().cpu().numpy()\n",
    "\n",
    "def time_string():\n",
    "    return datetime.now().strftime(\"%Y-%m-%d %H:%M\")\n",
    "\n",
    "def save_checkpoint(model, optimizer, epoch, train_losses, eval_losses, save_path, subset=\"full\"):\n",
    "    checkpoint = {\n",
    "        \"epoch\": epoch,\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"train_losses\": train_losses,\n",
    "        \"eval_losses\": eval_losses,\n",
    "    }\n",
    "    model_path = f\"{save_path}/model_epoch_{epoch}_subset_{subset}.pt\"\n",
    "    torch.save(checkpoint, model_path)\n",
    "    print(f\"Checkpoint saved at {model_path}\")\n",
    "\n",
    "def load_checkpoint(save_path, model, optimizer, device):\n",
    "    if os.path.exists(save_path):\n",
    "        checkpoint = torch.load(save_path, map_location=device)\n",
    "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "        epoch = checkpoint[\"epoch\"]\n",
    "        train_losses = checkpoint[\"train_losses\"]\n",
    "        eval_losses = checkpoint[\"eval_losses\"]\n",
    "        print(f\"Checkpoint loaded from {save_path}\")\n",
    "        return model, optimizer, epoch, train_losses, eval_losses\n",
    "    else:\n",
    "        print(f\"No checkpoint found at {save_path}, starting fresh.\")\n",
    "        return model, optimizer, 0, [], []\n",
    "\n",
    "def train(model, train_dataset, eval_dataset, hparams):\n",
    "    weights_fpath = Path(\"saved_models/synthesizer.pt\")\n",
    "    save_dir = Path(\"saved_models\")\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using device:\", device)\n",
    "\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    model, optimizer, start_epoch, train_losses, eval_losses = load_checkpoint(weights_fpath, model, optimizer, device)\n",
    "\n",
    "    for session in hparams.tts_schedule:\n",
    "        r, lr, max_step, batch_size = session\n",
    "        model.r = r\n",
    "\n",
    "        collate_fn = partial(collate_synthesizer, r=r, hparams=hparams)\n",
    "        train_loader = DataLoader(train_dataset, batch_size, shuffle=True, num_workers=2, collate_fn=collate_fn)\n",
    "        eval_loader = DataLoader(eval_dataset, batch_size, shuffle=False, num_workers=2, collate_fn=collate_fn)\n",
    "\n",
    "        for p in optimizer.param_groups:\n",
    "            p['lr'] = lr\n",
    "\n",
    "        for epoch in range(start_epoch + 1, 100):  # Continue from last epoch\n",
    "            total_loss = 0\n",
    "            for texts, mels, embeds, idx, mel_lens in train_loader:\n",
    "                texts, mels, embeds = texts.to(device), mels.to(device), embeds.to(device)\n",
    "\n",
    "                stop = torch.ones(mels.shape[0], mels.shape[2]).to(device)\n",
    "                for j, k in enumerate(idx):\n",
    "                    stop[j, :int(mel_lens[j]) - 1] = 0\n",
    "\n",
    "                m1_hat, m2_hat, _, stop_pred = model(texts, mels, embeds)\n",
    "\n",
    "                m1_loss = F.mse_loss(m1_hat, mels) + F.l1_loss(m1_hat, mels)\n",
    "                m2_loss = F.mse_loss(m2_hat, mels)\n",
    "                stop_loss = F.binary_cross_entropy(stop_pred, stop)\n",
    "\n",
    "                loss = m1_loss + m2_loss + stop_loss\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            avg_loss = total_loss / len(train_loader)\n",
    "            print(f\"Epoch {epoch} - Loss: {avg_loss:.4f}\")\n",
    "\n",
    "            train_losses.append(avg_loss)\n",
    "\n",
    "            if model.get_step() >= max_step:\n",
    "                break\n",
    "\n",
    "            if epoch % 10 == 0:  # Evaluate every 10 epochs\n",
    "                eval_loss = evaluate(model, eval_loader, device, hparams)\n",
    "                eval_losses.append(eval_loss)\n",
    "                save_checkpoint(model, optimizer, epoch, train_losses, eval_losses, save_dir)\n",
    "\n",
    "def evaluate(model, data_loader, device, hparams):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for texts, mels, embeds, idx in data_loader:\n",
    "            texts, mels, embeds = texts.to(device), mels.to(device), embeds.to(device)\n",
    "\n",
    "            stop = torch.ones(mels.shape[0], mels.shape[2]).to(device)\n",
    "            m1_hat, m2_hat, _, stop_pred = model(texts, mels, embeds)\n",
    "\n",
    "            m1_loss = F.mse_loss(m1_hat, mels) + F.l1_loss(m1_hat, mels)\n",
    "            m2_loss = F.mse_loss(m2_hat, mels)\n",
    "            stop_loss = F.binary_cross_entropy(stop_pred, stop)\n",
    "\n",
    "            loss = m1_loss + m2_loss + stop_loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    print(f\"Evaluation Loss: {avg_loss:.4f}\")\n",
    "    model.train()\n",
    "    return avg_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "No checkpoint found at saved_models/synthesizer.pt, starting fresh.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[55], line 74\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_dataset, eval_dataset, hparams)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j, k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(idx):\n\u001b[1;32m     72\u001b[0m     stop[j, :\u001b[38;5;28mint\u001b[39m(mel_lens[j]) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 74\u001b[0m m1_hat, m2_hat, _, stop_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m m1_loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmse_loss(m1_hat, mels) \u001b[38;5;241m+\u001b[39m F\u001b[38;5;241m.\u001b[39ml1_loss(m1_hat, mels)\n\u001b[1;32m     77\u001b[0m m2_loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmse_loss(m2_hat, mels)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[43], line 386\u001b[0m, in \u001b[0;36mTacotron.forward\u001b[0;34m(self, x, m, speaker_embedding)\u001b[0m\n\u001b[1;32m    382\u001b[0m context_vec \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(batch_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder_dims \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspeaker_embedding_size, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m    384\u001b[0m \u001b[38;5;66;03m# SV2TTS: Run the encoder with the speaker embedding\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;66;03m# The projection avoids unnecessary matmuls in the decoder loop\u001b[39;00m\n\u001b[0;32m--> 386\u001b[0m encoder_seq \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspeaker_embedding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    387\u001b[0m encoder_seq_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder_proj(encoder_seq)\n\u001b[1;32m    389\u001b[0m \u001b[38;5;66;03m# Need a couple of lists for outputs\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[43], line 39\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[0;34m(self, x, speaker_embedding)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, speaker_embedding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     38\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(x)\n\u001b[0;32m---> 39\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpre_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m     x\u001b[38;5;241m.\u001b[39mtranspose_(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     41\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcbhg(x)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[43], line 177\u001b[0m, in \u001b[0;36mPreNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 177\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[1;32m    179\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mdropout(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mp, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`"
     ]
    }
   ],
   "source": [
    "train(model, train_dataset, test_dataset, hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Adjust learning rate function\n",
    "def adjust_learning_rate(optimizer, step_num, warmup_step=4000):\n",
    "    lr = LEARNING_RATE * warmup_step**0.5 * min(step_num * warmup_step**-1.5, step_num**-0.5)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(torch.cuda.device_count()):\n",
    "   print(torch.cuda.get_device_properties(i).name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_train_dataset(dataset, num_splits=4):\n",
    "    \"\"\"Splits the dataset into `num_splits` equal parts.\"\"\"\n",
    "    indices = list(range(len(dataset)))\n",
    "    split_size = len(dataset) // num_splits\n",
    "    subsets = []\n",
    "    \n",
    "    for i in range(num_splits):\n",
    "        start_idx = i * split_size\n",
    "        end_idx = len(dataset) if i == num_splits - 1 else (i + 1) * split_size\n",
    "        subsets.append(Subset(dataset, indices[start_idx:end_idx]))\n",
    "    return subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_splits = split_train_dataset(train_dataset, num_splits=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_save_path ='./saved_models/model_epoch_59_subset_0.pt'\n",
    "# model, optimizer, start_epoch, global_step, current_subset, train_losses, eval_losses = load_checkpoint(\n",
    "#     save_path=model_save_path, model=model, optimizer=optimizer, device=device\n",
    "# )\n",
    "# current_subset = 0\n",
    "# start_epoch += 1\n",
    "# if current_subset == 3:\n",
    "#     current_subset = 0\n",
    "#     start_epoch += 1\n",
    "# else:\n",
    "#     current_subset += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into subsets\n",
    "train_splits = split_train_dataset(train_dataset, num_splits=1)\n",
    "EPOCHS = 10000\n",
    "LEARNING_RATE = 1e-4\n",
    "# Default Model, Optimizer and params\n",
    "device = \"cuda:0\"\n",
    "model = nn.DataParallel(model.to(device))\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "global_step = 0\n",
    "current_subset = 0\n",
    "start_epoch = 0\n",
    "train_losses = []\n",
    "eval_losses = []\n",
    "save_path = \"./saved_models2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_evaluate_splits(\n",
    "    model, optimizer, criterion, train_splits, eval_dataloader,\n",
    "    device, num_epochs=EPOCHS, start_epoch=start_epoch, save_path=save_path,\n",
    "    global_step=global_step, current_subset=current_subset,\n",
    "    train_losses=train_losses, eval_losses=eval_losses\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5786986,
     "sourceId": 9507723,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5843294,
     "sourceId": 9582705,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 161435,
     "modelInstanceId": 138814,
     "sourceId": 163221,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
