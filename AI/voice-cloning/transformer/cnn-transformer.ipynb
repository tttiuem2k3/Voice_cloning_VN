{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Utterance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import logging\n",
    "from io import BytesIO\n",
    "import librosa\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class Utterance(object):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        _id: str = None,\n",
    "        raw_file: Path | BytesIO = None,\n",
    "        processor=None,\n",
    "    ):\n",
    "        self._id = _id\n",
    "        self.raw_file = raw_file\n",
    "        self.processor = processor if processor is not None else AudioPreprocessor()\n",
    "#         self.audio = self.raw()\n",
    "\n",
    "    def raw(self):\n",
    "        if isinstance(self.raw_file, Path) and self.raw_file.suffix == '.npy':\n",
    "            return np.load(self.raw_file)\n",
    "            \n",
    "        audio, _ = librosa.load(\n",
    "            self.raw_file, sr=self.processor.config.SAMPLE_RATE\n",
    "        )\n",
    "        \n",
    "        if audio.size == 0:\n",
    "            raise ValueError(\"Empty audio\")\n",
    "\n",
    "        audio = (\n",
    "            self.processor.config.SCALING_FACTOR\n",
    "            * librosa.util.normalize(audio)\n",
    "        )\n",
    "        return audio\n",
    "\n",
    "    def mel_in_db(self):\n",
    "        try:\n",
    "            return self.processor.audio_to_mel_db(self.raw())\n",
    "        except Exception:\n",
    "\n",
    "            logging.debug(\n",
    "                \"Failed to load Mel spectrogram, raw file: %s\", {self.raw_file}\n",
    "            )\n",
    "            raise\n",
    "    \n",
    "    def random_mel_in_db(self, seq_len):\n",
    "        random_mel = self.mel_in_db()\n",
    "        _, tempo_len = random_mel.shape\n",
    "        if tempo_len < seq_len:\n",
    "            pad_left = (seq_len - tempo_len) // 2\n",
    "            pad_right = seq_len - tempo_len - pad_left\n",
    "            random_mel = np.pad(random_mel, ((0, 0), (pad_left, pad_right)), mode=\"reflect\")\n",
    "        elif tempo_len > seq_len:\n",
    "            max_seq_start = tempo_len - seq_len\n",
    "            seq_start = np.random.randint(0, max_seq_start)\n",
    "            seq_end = seq_start + seq_len\n",
    "            random_mel = random_mel[:, seq_start:seq_end]\n",
    "        return random_mel\n",
    "    \n",
    "    def magtitude(self):\n",
    "        return self.processor.audio_to_magnitude_db(self.raw())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Processor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Audio Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioConfig:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class AudioPreprocessor:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "\n",
    "    def normalize(self, spectrogram_in_db):\n",
    "        '''Normalize spectrogram in decibel values between 0 and 1.'''\n",
    "        normalized_spectrogram_in_db = (\n",
    "            spectrogram_in_db - self.config.REF_LEVEL_DB - self.config.MIN_LEVEL_DB\n",
    "        ) / -self.config.MIN_LEVEL_DB\n",
    "\n",
    "        return np.clip(normalized_spectrogram_in_db, self.config.ZERO_THRESHOLD, 1)\n",
    "\n",
    "    def magnitude_to_mel(self, magnitude):\n",
    "        '''Convert a magnitude spectrogram to a mel spectrogram.'''\n",
    "        return librosa.feature.melspectrogram(\n",
    "            S=magnitude,\n",
    "            sr=self.config.SAMPLE_RATE,\n",
    "            n_fft=self.config.N_FFT,\n",
    "            n_mels=self.config.N_MELS,\n",
    "            fmin=self.config.FMIN,\n",
    "            fmax=self.config.FMAX,\n",
    "        )\n",
    "\n",
    "    def amp_to_db(self, mel_spectrogram):\n",
    "        '''Convert amplitude spectrogram to decibel scale.'''\n",
    "        return 20.0 * np.log10(\n",
    "            np.maximum(self.config.ZERO_THRESHOLD, mel_spectrogram)\n",
    "        )\n",
    "\n",
    "    def audio_to_stft(self, audio):\n",
    "        '''Generate Short-Time Fourier Transform (STFT) from the audio time series.'''\n",
    "        return librosa.stft(\n",
    "            y=audio,\n",
    "            n_fft=self.config.N_FFT,\n",
    "            hop_length=self.config.HOP_LENGTH,\n",
    "            win_length=self.config.WIN_LENGTH,\n",
    "        )\n",
    "\n",
    "    def apply_pre_emphasis(self, y):\n",
    "        '''Apply a pre-emphasis filter to the audio signal.'''\n",
    "        return np.append(y[0], y[1:] - self.config.PRE_EMPHASIS * y[:-1])\n",
    "\n",
    "    def stft_to_magnitude(self, linear):\n",
    "        '''Compute the magnitude spectrogram from STFT.'''\n",
    "        return np.abs(linear)\n",
    "\n",
    "    def audio_to_mel_db(self, audio):\n",
    "        '''Convert a given linear spectrogram to a log mel spectrogram (mel spectrogram in db) and return it.'''\n",
    "        stft = self.audio_to_stft(audio)\n",
    "        magnitude = self.stft_to_magnitude(stft)\n",
    "        mel = self.magnitude_to_mel(magnitude)\n",
    "        mel = self.amp_to_db(mel)\n",
    "        return self.normalize(mel)\n",
    "    \n",
    "    def audio_to_magnitude_db(self, audio):\n",
    "        '''Convert a given linear spectrogram to a magnitude spectrogram.'''\n",
    "        stft = self.audio_to_stft(audio)\n",
    "        magnitude_in_amp =  self.stft_to_magnitude(stft)\n",
    "        magnitude_in_db = self.amp_to_db(magnitude_in_amp)\n",
    "        return self.normalize(magnitude_in_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Text Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_by_key(dictionary: dict):\n",
    "    return dict(sorted(dictionary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Acronym Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "ACRONYMS_FILEPATH = \"./acronyms.json\"\n",
    "\n",
    "with open(ACRONYMS_FILEPATH, \"r\", encoding=\"utf-8\") as file:\n",
    "    ACRONYMS = sort_by_key(json.load(file).items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class AcronymNormalizer(object):\n",
    "\n",
    "    pattern = re.compile(r\"\\b(\" + \"|\".join(map(re.escape, ACRONYMS)) + r\")\\b\")\n",
    "\n",
    "    @classmethod\n",
    "    def normalize(cls, text: str):\n",
    "        def replace_unit(match):\n",
    "            return ACRONYMS[match.group(0)]\n",
    "\n",
    "        return cls.pattern.sub(replace_unit, text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Breaker Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "BREAKS = {\n",
    "    \".\": \" chấm \",\n",
    "    \",\": \" phẩy \",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class BreakNormalizer(object):\n",
    "\n",
    "    BREAKS = BREAKS\n",
    "\n",
    "    duplicate_dot_comma_pattern = re.compile(r\"([,.]){2,}\")\n",
    "    adjacent_symbols_pattern = re.compile(r\"(\\S)([,.])(\\S)\")\n",
    "    left_symbol_pattern = re.compile(r\"(\\S)([,.])\")\n",
    "    right_symbol_pattern = re.compile(r\"([,.])(\\S)\")\n",
    "\n",
    "    @classmethod\n",
    "    def normalize(cls, text):\n",
    "        text = cls.duplicate_dot_comma_pattern.sub(lambda m: m.group(1), text)\n",
    "\n",
    "        def replace_dot_and_comma(match):\n",
    "            return match.group(1) + cls.BREAKS[match.group(2)] + match.group(3)\n",
    "\n",
    "        text = cls.adjacent_symbols_pattern.sub(replace_dot_and_comma, text)\n",
    "        text = cls.left_symbol_pattern.sub(r\"\\1 \\2\", text)\n",
    "        text = cls.right_symbol_pattern.sub(r\"\\1 \\2\", text)\n",
    "\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3 Character Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "class CharacterNormalizer(object):\n",
    "\n",
    "    pattern = re.compile(r\"[^a-zA-Z0-9\\sđâăêôơư.,]\")\n",
    "\n",
    "    @classmethod\n",
    "    def normalize(cls, text: str):\n",
    "\n",
    "        return cls.pattern.sub(\"\", text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.4 Date Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "DATE_PREFIXS_FILEPATH = \"./date_prefixs.json\"\n",
    "\n",
    "with open(DATE_PREFIXS_FILEPATH, \"r\", encoding=\"utf-8\") as file:\n",
    "    DATE_PREFIXS = sorted(json.load(file), key=len, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "class DateNormalizer(object):\n",
    "\n",
    "    DATE_PREFIXS = DATE_PREFIXS\n",
    "\n",
    "    date_pattern1 = re.compile(r\"(\\b\\w{0,4}\\b)\\s*([12][0-9]|3[01]|0?[1-9])\\/(1[0-2]|0?[1-9])\\/(\\d{1,4})\")\n",
    "    date_pattern2 = re.compile(r\"(\\b\\w{0,4}\\b)\\s*([12][0-9]|3[01]|0?[1-9])\\-(1[0-2]|0?[1-9])\\-(\\d{1,4})\")\n",
    "    date_pattern3 = re.compile(r\"(\\b\\w{0,5}\\b)\\s*(0?[1-9]|1[0,1,2])[\\/|\\-](\\d{4})\")\n",
    "    prefixs = \"|\".join(DATE_PREFIXS)\n",
    "    date_pattern4 = re.compile(r\"(\" + prefixs + r\")\\s([12][0-9]|3[01]|0?[1-9])[\\-|\\/](1[0-2]|0?[1-9])\")\n",
    "\n",
    "    @classmethod\n",
    "    def normalize_date_pattern1(cls, text: str):\n",
    "        # Date pattern 1\n",
    "        # Example: 11/12/2002\n",
    "\n",
    "        def replace(match):\n",
    "            prefix = match.group(1).strip()\n",
    "            day = match.group(2)\n",
    "            month = match.group(3)\n",
    "            year = match.group(4)\n",
    "\n",
    "            if prefix == \"ngày\":\n",
    "                return f\"{prefix} {day} tháng {month} năm {year}\"\n",
    "            else:\n",
    "                return f'{prefix + \" \" if prefix != \"\" else \"\"}ngày {day} tháng {month} năm {year}'\n",
    "\n",
    "        return cls.date_pattern1.sub(replace, text)\n",
    "\n",
    "    @classmethod\n",
    "    def normalize_date_pattern2(cls, text: str):\n",
    "        # Date pattern 1\n",
    "        # Example: 11-12-2002\n",
    "\n",
    "        def replace(match):\n",
    "            prefix = match.group(1).strip()\n",
    "            day = match.group(2)\n",
    "            month = match.group(3)\n",
    "            year = match.group(4)\n",
    "\n",
    "            if prefix == \"ngày\":\n",
    "                return f\"{prefix} {day} tháng {month} năm {year}\"\n",
    "            else:\n",
    "                return f'{prefix + \" \" if prefix != \"\" else \"\"}ngày {day} tháng {month} năm {year}'\n",
    "\n",
    "        return cls.date_pattern2.sub(replace, text)\n",
    "\n",
    "    @classmethod\n",
    "    def normalize_date_pattern3(cls, text: str):\n",
    "        # Date pattern 3\n",
    "        # Example: 12/2022 -> tháng 12 năm 2002\n",
    "\n",
    "        def replace(match):\n",
    "            prefix = match.group(1)\n",
    "            month = match.group(2)\n",
    "            year = match.group(3)\n",
    "\n",
    "            if prefix == \"tháng\":\n",
    "                return f\"tháng {month} năm {year}\"\n",
    "            else:\n",
    "                return f'{prefix + \" \" if prefix != \"\" else \"\"}tháng {month} năm {year}'\n",
    "\n",
    "        return cls.date_pattern3.sub(replace, text)\n",
    "\n",
    "    @classmethod\n",
    "    def normalize_date_pattern4(cls, text: str):\n",
    "        # Date pattern 4\n",
    "        # Example: ngày 11/12\n",
    "\n",
    "        def replace(match):\n",
    "            prefix = match.group(1)\n",
    "            day = match.group(2)\n",
    "            month = match.group(3)\n",
    "\n",
    "            if prefix == \"ngày\":\n",
    "                return f\"ngày {day} tháng {month}\"\n",
    "            else:\n",
    "                return f'{prefix + \" \" if prefix != \"\" else \"\"}ngày {day} tháng {month}'\n",
    "\n",
    "        return cls.date_pattern4.sub(replace, text)\n",
    "\n",
    "    @classmethod\n",
    "    def normalize(cls, text: str):\n",
    "        text = cls.normalize_date_pattern4(text)\n",
    "        text = cls.normalize_date_pattern1(text)\n",
    "        text = cls.normalize_date_pattern2(text)\n",
    "        text = cls.normalize_date_pattern3(text)\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.5 Letter Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "LETTERS_FILEPATH = \"./letters.json\"\n",
    "\n",
    "with open(LETTERS_FILEPATH, \"r\", encoding=\"utf-8\") as file:\n",
    "    LETTERS = sort_by_key(json.load(file).items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class LetterNormalizer(object):\n",
    "\n",
    "    pattern = re.compile(r\"\\b(\" + \"|\".join(map(re.escape, LETTERS)) + r\")\\b\")\n",
    "\n",
    "    @classmethod\n",
    "    def normalize(cls, text: str):\n",
    "\n",
    "        def replace_unit(match):\n",
    "            return LETTERS[match.group(0)]\n",
    "\n",
    "        return cls.pattern.sub(replace_unit, text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.6 Number Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "BASE_NUMBERS_FILEPATH = \"./base_numbers.json\"\n",
    "\n",
    "with open(BASE_NUMBERS_FILEPATH, \"r\") as file:\n",
    "    BASE_NUMBERS = {int(key): value for key, value in json.load(file).items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "NUMBER_LEVEL_FILEPATH = \"./number_levels.json\"\n",
    "\n",
    "with open(NUMBER_LEVEL_FILEPATH, \"r\") as file:\n",
    "    NUMBER_LEVELS = {int(key): value for key, value in json.load(file).items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class NumberNomalizer(object):\n",
    "\n",
    "    pattern = re.compile(r\"\\d+\")\n",
    "\n",
    "    @classmethod\n",
    "    def _convert_number_2_digits(cls, number: int):\n",
    "        if number in BASE_NUMBERS:\n",
    "            return BASE_NUMBERS[number]\n",
    "\n",
    "        tens = number // 10\n",
    "        base = number % 10\n",
    "        if base > 0:\n",
    "            return f\"{BASE_NUMBERS[tens]} mươi {BASE_NUMBERS[base]}\"\n",
    "\n",
    "        return f\"{BASE_NUMBERS[tens]} mươi\"\n",
    "\n",
    "    @classmethod\n",
    "    def _convert_number_3_digits(cls, number: int):\n",
    "        if number == 0:\n",
    "            return \"\"\n",
    "\n",
    "        remainder = number % 100\n",
    "        hundred = number // 100\n",
    "        if remainder == 0:\n",
    "            return f\"{BASE_NUMBERS[hundred]} trăm\"\n",
    "\n",
    "        if remainder < 10:\n",
    "            return f\"{BASE_NUMBERS[number // 100]} trăm linh {BASE_NUMBERS[remainder]}\"\n",
    "\n",
    "        return f\"{BASE_NUMBERS[hundred]} trăm {cls._convert_number_2_digits(remainder)}\"\n",
    "\n",
    "    @classmethod\n",
    "    def number_to_vietnamese(cls, number: int):\n",
    "        if number == 0:\n",
    "            return \"không\"\n",
    "\n",
    "        if number in BASE_NUMBERS:\n",
    "            return BASE_NUMBERS[number]\n",
    "\n",
    "        if number < 100:\n",
    "            return cls._convert_number_2_digits(number)\n",
    "\n",
    "        result = cls._convert_number_3_digits(number % 1000)\n",
    "        current_level = None\n",
    "\n",
    "        for current_level in NUMBER_LEVELS:\n",
    "            next_level = current_level * 1000\n",
    "            if number // (next_level) == 0:\n",
    "                break\n",
    "            level_base = number % (next_level) // current_level\n",
    "            result = f\"{cls._convert_number_3_digits(level_base)} {NUMBER_LEVELS[current_level]} {result}\"\n",
    "\n",
    "        level_base = number // current_level\n",
    "\n",
    "        if level_base == 0:\n",
    "            return result\n",
    "\n",
    "        if level_base in BASE_NUMBERS:\n",
    "            return f\"{BASE_NUMBERS[level_base]} {NUMBER_LEVELS[current_level]} {result}\"\n",
    "\n",
    "        if level_base > 99:\n",
    "            return f\"{cls._convert_number_3_digits(level_base)} {NUMBER_LEVELS[current_level]} {result}\"\n",
    "\n",
    "        if level_base > 11:\n",
    "            return f\"{cls._convert_number_2_digits(level_base)} {NUMBER_LEVELS[current_level]} {result}\"\n",
    "\n",
    "    @classmethod\n",
    "    def normalize(cls, text: str) -> str:\n",
    "\n",
    "        replaced_text = cls.pattern.sub(lambda x: cls.number_to_vietnamese(int(x.group())), text)\n",
    "\n",
    "        return replaced_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.7 Phoneme Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "SAME_PHONEMES_FILEPATH = \"./same_phonemes.json\"\n",
    "\n",
    "with open(SAME_PHONEMES_FILEPATH, \"r\", encoding=\"utf-8\") as file:\n",
    "    SAME_PHONEMES = sort_by_key(json.load(file).items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class PhonemeNormalizer(object):\n",
    "\n",
    "    pattern = re.compile(r\"(\" + \"|\".join(map(re.escape, SAME_PHONEMES)) + r\")\")\n",
    "\n",
    "    @classmethod\n",
    "    def normalize(cls, text: str):\n",
    "\n",
    "        def replace_symbol(match):\n",
    "            return SAME_PHONEMES[match.group(0)]\n",
    "\n",
    "        return cls.pattern.sub(replace_symbol, text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.8 Symbol Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "SYMBOLS_FILEPATH = \"./symbols.json\"\n",
    "\n",
    "with open(SYMBOLS_FILEPATH, \"r\", encoding=\"utf-8\") as file:\n",
    "    SYMBOLS = sort_by_key(json.load(file).items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class SymbolNormalizer(object):\n",
    "\n",
    "    pattern = re.compile(r\"([\\s\\S])(\" + \"|\".join(map(re.escape, SYMBOLS)) + r\")([\\s\\S])\")\n",
    "\n",
    "    @classmethod\n",
    "    def normalize(cls, text: str):\n",
    "\n",
    "        def replace_symbol(match):\n",
    "            return (\n",
    "                (match.group(1) if match.group(1) == \" \" else match.group(1) + \" \")\n",
    "                + SYMBOLS[match.group(2)]\n",
    "                + (match.group(3) if match.group(3) == \" \" else match.group(3) + \" \")\n",
    "            )\n",
    "\n",
    "        return cls.pattern.sub(replace_symbol, text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.9 Tone Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "TONES_FILEPATH = \"./tones.json\"\n",
    "\n",
    "with open(TONES_FILEPATH, \"r\") as file:\n",
    "    TONES = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class ToneNormalizer(object):\n",
    "    pattern = re.compile(r\"(\\w*)([áàảãạấầẩẫậắằẳẵặéèẻẽẹếềểễệíìỉĩịóòỏõọốồổỗộớờởỡợúùủũụứừửữựýỳỷỹỵ])(\\w*)\")\n",
    "\n",
    "    @classmethod\n",
    "    def normalize(cls, text):\n",
    "\n",
    "        def replace(match):\n",
    "            accented = match.group(2)\n",
    "            base, tone = TONES[accented]\n",
    "            return f\"{match.group(1)}{base}{match.group(3)}{tone}\"\n",
    "\n",
    "        text = cls.pattern.sub(replace, text)\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.10 Unit Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "UNITS_FILEPATH = \"./units.json\"\n",
    "\n",
    "with open(UNITS_FILEPATH, \"r\", encoding=\"utf-8\") as file:\n",
    "    UNITS = sort_by_key(json.load(file).items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class UnitNormalizer(object):\n",
    "\n",
    "    pattern = re.compile(r\"\\b(\" + \"|\".join(map(re.escape, UNITS)) + r\")\\b\")\n",
    "\n",
    "    @classmethod\n",
    "    def normalize(cls, text):\n",
    "\n",
    "        def replace_unit(match):\n",
    "            return UNITS[match.group(0)]\n",
    "\n",
    "        return cls.pattern.sub(replace_unit, text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.11 Text Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_PIPELINE = [\n",
    "    DateNormalizer,\n",
    "    NumberNomalizer,\n",
    "    LetterNormalizer,\n",
    "    AcronymNormalizer,\n",
    "    SymbolNormalizer,\n",
    "    UnitNormalizer,\n",
    "    PhonemeNormalizer,\n",
    "    ToneNormalizer,\n",
    "    CharacterNormalizer,\n",
    "    BreakNormalizer,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextNormalizer(object):\n",
    "\n",
    "    def __init__(self, pipeline=DEFAULT_PIPELINE, lower=True):\n",
    "        self.pipeline = pipeline\n",
    "        self.lower = lower\n",
    "\n",
    "    def normalize(self, text):\n",
    "        if self.lower:\n",
    "            text = text.lower()\n",
    "\n",
    "        for processor in self.pipeline:\n",
    "            text = processor.normalize(text)\n",
    "\n",
    "        return text\n",
    "\n",
    "    def __call__(self, text):\n",
    "        return self.normalize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.12 Text To Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACCENTS = ['1', '2', '3', '4', '5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOWELS_FILEPATH = \"./vowels.json\"\n",
    "\n",
    "with open(VOWELS_FILEPATH, \"r\", encoding=\"utf-8\") as file:\n",
    "    VOWELS = sorted(json.load(file), key=len, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEAD_CONSONANTS_FILEPATH = \"./head_consonants.json\"\n",
    "\n",
    "with open(HEAD_CONSONANTS_FILEPATH, \"r\", encoding=\"utf-8\") as file:\n",
    "    HEAD_CONSONANTS = sorted(json.load(file), key=len, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "FINAL_CONSONANTS_FILEPATH = \"./final_consonants.json\"\n",
    "    \n",
    "with open(FINAL_CONSONANTS_FILEPATH, \"r\", encoding=\"utf-8\") as file:\n",
    "    FINAL_CONSONANTS = sorted(json.load(file), key=len, reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "PHONEMES = sorted(VOWELS + HEAD_CONSONANTS + FINAL_CONSONANTS + ACCENTS + list(BREAKS.keys()), key=len, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class WordByPhonemesEmbedding(object):\n",
    "\n",
    "    def __init__(self, phonemes=PHONEMES, normalize=TextNormalizer(), spliter=\" \"):\n",
    "        self.phonemes = phonemes\n",
    "        self.normalize = normalize\n",
    "        self.spliter = spliter\n",
    "\n",
    "    def _parse_head_constants(self, word):\n",
    "        pattern = r'^(' + '|'.join(HEAD_CONSONANTS) + ')'\n",
    "        match = re.match(pattern, word)\n",
    "        head_consonant = None\n",
    "        if match:\n",
    "            head_consonant = r'\\b' + match.group(1)\n",
    "        return re.sub(pattern, '', word), head_consonant\n",
    "    \n",
    "    def _parse_vowels(self, word):\n",
    "        pattern = r'^(' + '|'.join(VOWELS) + ')'\n",
    "        match = re.match(pattern, word)\n",
    "        vowel = None\n",
    "        if match:\n",
    "            vowel =  match.group(1)\n",
    "        return re.sub(pattern, '', word), vowel\n",
    "\n",
    "    def _parse_final_constants(self, word):\n",
    "        pattern = r'^(' + '|'.join(FINAL_CONSONANTS) + ')'\n",
    "        match = re.match(pattern, word)\n",
    "        final_consonant = None\n",
    "        if match:\n",
    "            final_consonant =  match.group(1)\n",
    "        return re.sub(pattern, '', word), final_consonant\n",
    "\n",
    "    def word2vec(self, word:str):\n",
    "        embedding_vector = [0] * len(PHONEMES)\n",
    "        \n",
    "        word, head_consonant = self._parse_head_constants(word)\n",
    "        word, vowel = self._parse_vowels(word)\n",
    "        word, final_consonant = self._parse_final_constants(word)\n",
    "        \n",
    "        if len(word) > 0 and word[-1] in PHONEMES:\n",
    "            accent_or_break = word[-1]\n",
    "            embedding_vector[PHONEMES.index(accent_or_break)] = 1\n",
    "            \n",
    "        if head_consonant is not None:\n",
    "            embedding_vector[PHONEMES.index(head_consonant)] = 1\n",
    "            \n",
    "        if vowel is not None:\n",
    "            embedding_vector[PHONEMES.index(vowel)] = 1\n",
    "            \n",
    "        if final_consonant is not None:\n",
    "            embedding_vector[PHONEMES.index(final_consonant)] = 1\n",
    "            \n",
    "        return {\n",
    "            \"head_consonant\": head_consonant,\n",
    "            \"final_consonant\": final_consonant,\n",
    "            \"vowel\": vowel,\n",
    "            \"emmbedding_vector\": embedding_vector\n",
    "        }\n",
    "\n",
    "    def embedding(self, text):\n",
    "        text = self.normalize.normalize(text)\n",
    "        words = text.split(self.spliter)\n",
    "\n",
    "        return [self.word2vec(word)[\"emmbedding_vector\"] for word in words]\n",
    "\n",
    "    def __call__(self, text):\n",
    "        return self.embedding(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Speech Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Speech Encoder Audio Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeakerEncoderAudioConfig(AudioConfig):\n",
    "    N_MELS = 80\n",
    "    SAMPLE_RATE = 16000\n",
    "    FRAME_SHIFT = 0.01\n",
    "    FRAME_LENGTH = 0.025\n",
    "    HOP_LENGTH = int(SAMPLE_RATE * FRAME_SHIFT)\n",
    "    WIN_LENGTH = int(SAMPLE_RATE * FRAME_LENGTH)\n",
    "    N_FFT = 1024\n",
    "    FMIN = 90\n",
    "    FMAX = 7600\n",
    "    ZERO_THRESHOLD = 1e-5\n",
    "    MIN_AMPLITUDE = 0.3\n",
    "    MAX_AMPLITUDE = 1.0\n",
    "    MIN_LEVEL_DB = -100\n",
    "    REF_LEVEL_DB = 0\n",
    "    NUM_FRAMES = 160 * 30\n",
    "    SCALING_FACTOR = 0.95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Speech Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from scipy.interpolate import interp1d\n",
    "from sklearn.metrics import roc_curve\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from scipy.optimize import brentq\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "class SpeechTransformerEncoder(nn.Module):\n",
    "    def __init__(self, input_size=80, hidden_size=786, num_layers=12, num_heads=8, device='cpu', loss_device='cpu'):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.loss_device = loss_device\n",
    "        \n",
    "        # Transformer encoder layers\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=input_size,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=hidden_size,\n",
    "            dropout=0.05,\n",
    "            activation='relu',\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers).to(device)\n",
    "\n",
    "        self.linear = nn.Linear(in_features=input_size, out_features=256).to(device)\n",
    "        self.relu = nn.ReLU().to(device)\n",
    "\n",
    "        # Cosine similarity scaling (with fixed initial parameter values)\n",
    "        self.similarity_weight = nn.Parameter(torch.tensor([10.])).to(loss_device)\n",
    "        self.similarity_bias = nn.Parameter(torch.tensor([-5.])).to(loss_device)\n",
    "\n",
    "        # Loss\n",
    "        self.loss_fn = nn.CrossEntropyLoss().to(loss_device)\n",
    "\n",
    "    def do_gradient_ops(self):\n",
    "        # Gradient scale\n",
    "        self.similarity_weight.grad *= 0.01\n",
    "        self.similarity_bias.grad *= 0.01\n",
    "\n",
    "        # Gradient clipping\n",
    "        clip_grad_norm_(self.parameters(), 3, norm_type=2)\n",
    "\n",
    "    def forward(self, utterances, hidden_init=None):\n",
    "        \"\"\"\n",
    "        Computes the embeddings of a batch of utterance spectrograms.\n",
    "\n",
    "        :param utterances: batch of mel-scale filterbanks of same duration as a tensor of shape\n",
    "        (batch_size, n_frames, n_channels)\n",
    "        :param hidden_init: not used in the Transformer version\n",
    "        :return: the embeddings as a tensor of shape (batch_size, embedding_size)\n",
    "        \"\"\"\n",
    "        utterances = utterances.to(self.device)\n",
    "        # Pass the input through the Transformer Encoder\n",
    "        out = self.transformer_encoder(utterances)\n",
    "\n",
    "        # We take the mean of all time steps (similar to a global pooling)\n",
    "        embeds_raw = self.relu(self.linear(out.mean(dim=1)))\n",
    "\n",
    "        # L2-normalize it\n",
    "        embeds = embeds_raw / torch.norm(embeds_raw, dim=1, keepdim=True)\n",
    "\n",
    "        return embeds\n",
    "\n",
    "    def similarity_matrix(self, embeds):\n",
    "        \"\"\"\n",
    "        Computes the similarity matrix according of GE2E.\n",
    "\n",
    "        :param embeds: the embeddings as a tensor of shape (speakers_per_batch,\n",
    "        utterances_per_speaker, embedding_size)\n",
    "        :return: the similarity matrix as a tensor of shape (speakers_per_batch,\n",
    "        utterances_per_speaker, speakers_per_batch)\n",
    "        \"\"\"\n",
    "        speakers_per_batch, utterances_per_speaker = embeds.shape[:2]\n",
    "\n",
    "        # Inclusive centroids (1 per speaker). Cloning is needed for reverse differentiation\n",
    "        centroids_incl = torch.mean(embeds, dim=1, keepdim=True)\n",
    "        centroids_incl = centroids_incl.clone() / torch.norm(centroids_incl, dim=2, keepdim=True)\n",
    "\n",
    "        # Exclusive centroids (1 per utterance)\n",
    "        centroids_excl = (torch.sum(embeds, dim=1, keepdim=True) - embeds)\n",
    "        centroids_excl /= (utterances_per_speaker - 1)\n",
    "        centroids_excl = centroids_excl.clone() / torch.norm(centroids_excl, dim=2, keepdim=True)\n",
    "\n",
    "        # Similarity matrix computation\n",
    "        sim_matrix = torch.zeros(speakers_per_batch, utterances_per_speaker,\n",
    "                                 speakers_per_batch).to(self.loss_device)\n",
    "        mask_matrix = 1 - np.eye(speakers_per_batch, dtype=np.int64)\n",
    "        for j in range(speakers_per_batch):\n",
    "            mask = np.where(mask_matrix[j])[0]\n",
    "            sim_matrix[mask, :, j] = (embeds[mask] * centroids_incl[j]).sum(dim=2)\n",
    "            sim_matrix[j, :, j] = (embeds[j] * centroids_excl[j]).sum(dim=1)\n",
    "\n",
    "        sim_matrix = sim_matrix * self.similarity_weight + self.similarity_bias\n",
    "        return sim_matrix\n",
    "\n",
    "    def loss(self, embeds):\n",
    "        \"\"\"\n",
    "        Computes the softmax loss according the section 2.1 of GE2E.\n",
    "\n",
    "        :param embeds: the embeddings as a tensor of shape (speakers_per_batch,\n",
    "        utterances_per_speaker, embedding_size)\n",
    "        :return: the loss and the EER for this batch of embeddings.\n",
    "        \"\"\"\n",
    "        speakers_per_batch, utterances_per_speaker = embeds.shape[:2]\n",
    "\n",
    "        # Loss\n",
    "        sim_matrix = self.similarity_matrix(embeds)\n",
    "        sim_matrix = sim_matrix.reshape((speakers_per_batch * utterances_per_speaker,\n",
    "                                         speakers_per_batch))\n",
    "        ground_truth = np.repeat(np.arange(speakers_per_batch), utterances_per_speaker)\n",
    "        target = torch.from_numpy(ground_truth).long().to(self.loss_device)\n",
    "        loss = self.loss_fn(sim_matrix, target)\n",
    "\n",
    "        # EER (not backpropagated)\n",
    "        with torch.no_grad():\n",
    "            inv_argmax = lambda i: np.eye(1, speakers_per_batch, i, dtype=np.int64)[0]\n",
    "            labels = np.array([inv_argmax(i) for i in ground_truth])\n",
    "            preds = sim_matrix.detach().cpu().numpy()\n",
    "\n",
    "            # Snippet from https://yangcha.github.io/EER-ROC/\n",
    "            fpr, tpr, thresholds = roc_curve(labels.flatten(), preds.flatten())\n",
    "            eer = brentq(lambda x: 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n",
    "\n",
    "        return loss, eer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Speech Encoder Model Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeechrTransformerEncoderModelConfigs:\n",
    "    DEVICE = \"cuda:0\"\n",
    "    LOSS_DEVICE = \"cpu\"\n",
    "    MODEL_PATH = \"./000000036000.pt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 Load Speech Encoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def load_speaker_transformer_encoder(model_settings):\n",
    "    model = SpeechTransformerEncoder(\n",
    "        device=model_settings.DEVICE, loss_device=model_settings.LOSS_DEVICE\n",
    "    )\n",
    "    ckpt = torch.load(model_settings.MODEL_PATH, weights_only=False,\n",
    "                      map_location=torch.device('cuda:0'))\n",
    "\n",
    "    if ckpt:\n",
    "        model.load_state_dict(ckpt[\"model_state_dict\"], strict=False)\n",
    "\n",
    "    model.eval()\n",
    "    model.to(model_settings.DEVICE)\n",
    "    return nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "PytorchStreamReader failed reading zip archive: failed finding central directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m SPEECH_TRANSFORMER_ENCODER \u001b[38;5;241m=\u001b[39m \u001b[43mload_speaker_transformer_encoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mSpeechrTransformerEncoderModelConfigs\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 8\u001b[0m, in \u001b[0;36mload_speaker_transformer_encoder\u001b[0;34m(model_settings)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_speaker_transformer_encoder\u001b[39m(model_settings):\n\u001b[1;32m      5\u001b[0m     model \u001b[38;5;241m=\u001b[39m SpeechTransformerEncoder(\n\u001b[1;32m      6\u001b[0m         device\u001b[38;5;241m=\u001b[39mmodel_settings\u001b[38;5;241m.\u001b[39mDEVICE, loss_device\u001b[38;5;241m=\u001b[39mmodel_settings\u001b[38;5;241m.\u001b[39mLOSS_DEVICE\n\u001b[1;32m      7\u001b[0m     )\n\u001b[0;32m----> 8\u001b[0m     ckpt \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_settings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMODEL_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda:0\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ckpt:\n\u001b[1;32m     12\u001b[0m         model\u001b[38;5;241m.\u001b[39mload_state_dict(ckpt[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_state_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m], strict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/serialization.py:1326\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1324\u001b[0m orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n\u001b[1;32m   1325\u001b[0m overall_storage \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1326\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_zipfile_reader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_file\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m   1327\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_torchscript_zip(opened_zipfile):\n\u001b[1;32m   1328\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1329\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorch.load\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m received a zip file that looks like a TorchScript archive\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1330\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m dispatching to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorch.jit.load\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m (call \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorch.jit.load\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m directly to\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1331\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m silence this warning)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1332\u001b[0m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[1;32m   1333\u001b[0m         )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/serialization.py:671\u001b[0m, in \u001b[0;36m_open_zipfile_reader.__init__\u001b[0;34m(self, name_or_buffer)\u001b[0m\n\u001b[1;32m    670\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name_or_buffer) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 671\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPyTorchFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: PytorchStreamReader failed reading zip archive: failed finding central directory"
     ]
    }
   ],
   "source": [
    "# SPEECH_TRANSFORMER_ENCODER = load_speaker_transformer_encoder(\n",
    "#     SpeechrTransformerEncoderModelConfigs\n",
    "# )\n",
    "\n",
    "SPEECH_TRANSFORMER_ENCODER = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Text to Speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Text to Speech Audio Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Text2SpeechAudioConfig(AudioConfig):\n",
    "    N_MELS = 80\n",
    "    SAMPLE_RATE = 16000\n",
    "    N_FFT = 2048\n",
    "    FRAME_SHIFT = 0.0125\n",
    "    FRAME_LENGTH = 0.05\n",
    "    REF_LEVEL_DB = 20\n",
    "    HOP_LENGTH = int(SAMPLE_RATE * FRAME_SHIFT)\n",
    "    WIN_LENGTH = int(SAMPLE_RATE * FRAME_LENGTH)\n",
    "    PRE_EMPHASIS = 0.97\n",
    "    POWER = 1.2\n",
    "    FMIN = 90\n",
    "    FMAX = 7600\n",
    "    ZERO_THRESHOLD = 1e-5\n",
    "    MIN_LEVEL_DB = -100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Text to Speech Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def get_sinusoid_encoding_table(n_position, num_hidden, padding_idx=None):\n",
    "    ''' Sinusoid position encoding table '''\n",
    "\n",
    "    def cal_angle(position, hid_idx):\n",
    "        return position / np.power(10000, 2 * (hid_idx // 2) / num_hidden)\n",
    "\n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, hid_j) for hid_j in range(num_hidden)]\n",
    "\n",
    "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(n_position)])\n",
    "\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # dim 2i\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # dim 2i+1\n",
    "\n",
    "    if padding_idx is not None:\n",
    "        # zero vector for padding dimension\n",
    "        sinusoid_table[padding_idx] = 0.\n",
    "\n",
    "    return torch.FloatTensor(sinusoid_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "def get_sinusoid_encoding_table(n_position, num_hidden, padding_idx=None):\n",
    "    ''' Sinusoid position encoding table '''\n",
    "\n",
    "    def cal_angle(position, hid_idx):\n",
    "        return position / np.power(10000, 2 * (hid_idx // 2) / num_hidden)\n",
    "\n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, hid_j) for hid_j in range(num_hidden)]\n",
    "\n",
    "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(n_position)])\n",
    "\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # dim 2i\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # dim 2i+1\n",
    "\n",
    "    if padding_idx is not None:\n",
    "        # zero vector for padding dimension\n",
    "        sinusoid_table[padding_idx] = 0.\n",
    "\n",
    "    return torch.FloatTensor(sinusoid_table)\n",
    "\n",
    "\n",
    "class ScaledPositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, n_position=2048, dropout=0.1, padding_idx=None):\n",
    "        super(ScaledPositionalEncoding, self).__init__()\n",
    "        \n",
    "        self.alpha = nn.Parameter(torch.ones(1))  # Scaling factor\n",
    "        \n",
    "        # Get sinusoid encoding table\n",
    "        self.positional_embedding = nn.Embedding.from_pretrained(\n",
    "            get_sinusoid_encoding_table(n_position, d_model, padding_idx=padding_idx),\n",
    "            freeze=True\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x, pos):\n",
    "        pos = self.positional_embedding(pos)\n",
    "        x = pos * self.alpha + x\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class EncoderPrenet(nn.Module):\n",
    "    def __init__(self, input_dim, conv_channels, kernel_size, num_layers=3):\n",
    "        super(EncoderPrenet, self).__init__()\n",
    "        layers = []\n",
    "        for _ in range(num_layers):\n",
    "            conv_layer = nn.Conv1d(input_dim, conv_channels, kernel_size, padding=kernel_size//2)\n",
    "            nn.init.xavier_uniform_(conv_layer.weight)  # Xavier initialization\n",
    "            layers.append(conv_layer)\n",
    "            layers.append(nn.ReLU())\n",
    "            input_dim = conv_channels  # Update input dimension for next layer\n",
    "        self.prenet = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.prenet(x)\n",
    "        return x.transpose(1, 2)\n",
    "\n",
    "\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, num_layers, embedding_size=91,max_position_encoding=4096*2, dropout=0.1):\n",
    "        super(TextEncoder, self).__init__()\n",
    "        # Encoder Prenet\n",
    "        self.encoder_prenet = EncoderPrenet(embedding_size, d_model, kernel_size=5)\n",
    "        \n",
    "        \n",
    "        # Scaled Positional Encoding\n",
    "        self.positional_encoding = ScaledPositionalEncoding(d_model, n_position=max_position_encoding, dropout=dropout, padding_idx=0)\n",
    "        \n",
    "        # Transformer Encoder Layers\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=num_heads, dropout=dropout, dim_feedforward=d_model, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        \n",
    "    def forward(self, text, pos_text, src_mask=None, src_key_padding_mask=None):\n",
    "        text = self.encoder_prenet(text)\n",
    "        \n",
    "        # Positional encoding\n",
    "        text = self.positional_encoding(text, pos_text)\n",
    "        \n",
    "        # Pass through transformer encoder\n",
    "        encoded_text = self.transformer_encoder(text, src_key_padding_mask=src_key_padding_mask)\n",
    "        # encoded_text = self.transformer_encoder(text)\n",
    "        \n",
    "        \n",
    "        return encoded_text\n",
    "\n",
    "\n",
    "class Projection(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super(Projection, self).__init__()\n",
    "        self.fc = nn.Linear(d_model * 2, d_model)  # Concatenates Text + Audio encodings\n",
    "\n",
    "    def forward(self, encoded_speech, memory):\n",
    "        # Ensure both tensors are on the same device\n",
    "#         encoded_speech = encoded_speech.to(\"cuda:0\")\n",
    "        duplicated_encoded_speech = encoded_speech.repeat(1, memory.shape[1], 1)\n",
    "#         memory = memory.to(\"cuda:0\")\n",
    "\n",
    "        # Concatenate along the feature dimension\n",
    "        concat_enc = torch.cat((duplicated_encoded_speech, memory), dim=-1)\n",
    "\n",
    "        # Pass through fully connected layer\n",
    "        return self.fc(concat_enc)\n",
    "\n",
    "\n",
    "class DecoderPrenet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers=3):\n",
    "        super(DecoderPrenet, self).__init__()\n",
    "        layers = []\n",
    "        for _ in range(num_layers):\n",
    "            linear_layer = nn.Linear(input_dim, hidden_dim)\n",
    "            nn.init.xavier_uniform_(linear_layer.weight)  # Xavier initialization\n",
    "            layers.append(linear_layer)\n",
    "            layers.append(nn.ReLU())\n",
    "            input_dim = hidden_dim  # Update input dimension for next layer\n",
    "        self.prenet = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.prenet(x)\n",
    "\n",
    "\n",
    "\n",
    "class PostConvNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Post Convolutional Network (mel --> mel)\n",
    "    \"\"\"\n",
    "    def __init__(self, num_hidden, num_mels=80, outputs_per_step=1, dropout_prob=0.1):\n",
    "        \"\"\"\n",
    "        :param num_hidden: Dimension of hidden layers.\n",
    "        :param num_mels: Number of mel bands.\n",
    "        :param outputs_per_step: Number of outputs per step.\n",
    "        :param dropout_prob: Probability for dropout layers.\n",
    "        \"\"\"\n",
    "        super(PostConvNet, self).__init__()\n",
    "        \n",
    "        # Define input and output channel dimensions\n",
    "        self.num_mels = num_mels\n",
    "        self.outputs_per_step = outputs_per_step\n",
    "        self.num_hidden = num_hidden\n",
    "        \n",
    "        # First convolutional layer\n",
    "        self.conv1 = nn.Conv1d(\n",
    "            in_channels=num_mels * outputs_per_step,\n",
    "            out_channels=num_hidden,\n",
    "            kernel_size=5,\n",
    "            padding=4\n",
    "        )\n",
    "        \n",
    "        # Three repeated convolutional layers with batch normalization\n",
    "        self.conv_list = nn.ModuleList([\n",
    "            nn.Conv1d(\n",
    "                in_channels=num_hidden,\n",
    "                out_channels=num_hidden,\n",
    "                kernel_size=5,\n",
    "                padding=4\n",
    "            )\n",
    "            for _ in range(3)\n",
    "        ])\n",
    "        self.batch_norm_list = nn.ModuleList([nn.BatchNorm1d(num_hidden) for _ in range(3)])\n",
    "        \n",
    "        # Final convolutional layer\n",
    "        self.conv2 = nn.Conv1d(\n",
    "            in_channels=num_hidden,\n",
    "            out_channels=num_mels * outputs_per_step,\n",
    "            kernel_size=5,\n",
    "            padding=4\n",
    "        )\n",
    "        \n",
    "        # Batch normalization for the first convolution\n",
    "        self.pre_batchnorm = nn.BatchNorm1d(num_hidden)\n",
    "        \n",
    "        # Dropout layers\n",
    "        self.dropout1 = nn.Dropout(p=dropout_prob)\n",
    "        self.dropout_list = nn.ModuleList([nn.Dropout(p=dropout_prob) for _ in range(3)])\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass of the PostConvNet.\n",
    "        \n",
    "        :param x: Input tensor (batch_size, num_mels * outputs_per_step, seq_len).\n",
    "        :param mask: Mask (optional, not used in this implementation).\n",
    "        :return: Output tensor (batch_size, num_mels * outputs_per_step, seq_len).\n",
    "        \"\"\"\n",
    "        # Apply the first convolution, batch normalization, activation, and dropout\n",
    "        x = self.conv1(x)\n",
    "        x = self.pre_batchnorm(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.dropout1(x[:, :, :-4])  # Slice to simulate causal behavior\n",
    "        \n",
    "        # Apply repeated convolutions with batch norm, activation, and dropout\n",
    "        for conv, batch_norm, dropout in zip(self.conv_list, self.batch_norm_list, self.dropout_list):\n",
    "            x = conv(x)\n",
    "            x = batch_norm(x)\n",
    "            x = torch.tanh(x)\n",
    "            x = dropout(x[:, :, :-4])  # Slice to simulate causal behavior\n",
    "        \n",
    "        # Apply the final convolution and slicing\n",
    "        x = self.conv2(x)[:, :, :-4]\n",
    "        return x\n",
    "\n",
    "\n",
    "class MelDecoder(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, num_layers, mel_dim, max_position_encoding=8192, dropout=0.1, padding_idx=0):\n",
    "        super(MelDecoder, self).__init__()\n",
    "        \n",
    "        # Decoder prenet: 3 linear layers with xavier uniform initialization\n",
    "        self.prenet = DecoderPrenet(input_dim=mel_dim, hidden_dim=d_model, num_layers=3)\n",
    "        \n",
    "        # Scaled Positional Encoding\n",
    "        self.positional_encoding = ScaledPositionalEncoding(d_model, n_position=max_position_encoding, dropout=dropout)\n",
    "        \n",
    "        # Transformer Decoder Layers\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=d_model, nhead=num_heads, dropout=dropout, dim_feedforward=d_model)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # Output layer for mel spectrogram prediction\n",
    "        self.mel_linear = nn.Linear(d_model, mel_dim)\n",
    "        \n",
    "        self.postconvnet = PostConvNet(d_model)\n",
    "\n",
    "\n",
    "    def forward(self, memory, input_mel, pos_mel, memory_mask=None, memory_key_padding_mask=None, tgt_mask=None, tgt_key_padding_mask=None):\n",
    "        \n",
    "        # Prenet processing on mel spectrogram input\n",
    "        mel_embeds = self.prenet(input_mel)\n",
    "        \n",
    "        # Positional encoding for mel spectrogram\n",
    "        mel_embeds = self.positional_encoding(mel_embeds, pos_mel)\n",
    "        \n",
    "        # Reshape for TransformerDecoder (sequence length first)\n",
    "        mel_embeds_reshaped = mel_embeds.permute(1, 0, 2)  # [seq_len, batch_size, d_model]\n",
    "        memory_reshaped = memory.permute(1, 0, 2)          # [seq_len, batch_size, d_model]\n",
    "        \n",
    "\n",
    "        # Transformer decoder: Multi-head attention and feed-forward layers\n",
    "        decoded_output = self.transformer_decoder(\n",
    "            tgt=mel_embeds_reshaped, \n",
    "            memory=memory_reshaped,\n",
    "            memory_mask=memory_mask,\n",
    "            tgt_mask=tgt_mask,                      # Attention mask for target sequence\n",
    "            memory_key_padding_mask=memory_key_padding_mask,  # Key padding mask for memory (encoder outputs)\n",
    "            tgt_key_padding_mask=tgt_key_padding_mask,        # Key padding mask for target (mel spectrograms)\n",
    "#             tgt_is_causal=True,\n",
    "#             memory_is_causal=True\n",
    "        )\n",
    "        \n",
    "        decoded_output = decoded_output.permute(1, 0, 2)  # Reshape back to [batch_size, seq_len, d_model]\n",
    "        \n",
    "        # Linear layer to predict mel spectrogram\n",
    "        mel_output = self.mel_linear(decoded_output)\n",
    "        postnet_ouput = self.postconvnet(mel_output.permute(0, 2, 1))\n",
    "        postnet_ouput = postnet_ouput.permute(0, 2, 1)\n",
    "        return mel_output, postnet_ouput\n",
    "    \n",
    "class TNTModel(nn.Module):\n",
    "    def __init__(self, d_model=256, num_heads=8):\n",
    "        super(TNTModel, self).__init__()\n",
    "        self.encoder = TextEncoder(d_model=d_model, num_heads=num_heads, num_layers=3)\n",
    "        self.projection = Projection(d_model)\n",
    "        self.decoder = MelDecoder(d_model=d_model, num_heads=num_heads, num_layers=3, mel_dim=80)\n",
    "        \n",
    "    def forward(self, text, pos_text, input_mel, pos_mel, encoded_speech):\n",
    "        batch_size = text.size(0)\n",
    "        decoder_len = input_mel.size(1)\n",
    "        input_len = text.size(1)\n",
    "        \n",
    "        if self.training:\n",
    "            # Create a triangular mask for the target sequence\n",
    "#             tgt_mask = torch.triu(\n",
    "#                 torch.full((decoder_len, decoder_len), float(\"-inf\"), dtype=torch.float32, device=input_mel.device), diagonal=1\n",
    "#             )\n",
    "            tgt_mask = nn.Transformer.generate_square_subsequent_mask(decoder_len, device=input_mel.device)\n",
    "            # memory_mask = self._generate_subsequent_mask(decoder_len, input_len).to(input_mel.device)\n",
    "            memory_mask = None\n",
    "            tgt_key_padding_mask = pos_mel.eq(0)\n",
    "            src_key_padding_mask = memory_key_padding_mask = pos_text.eq(0)\n",
    "            src_mask = None\n",
    "        else:\n",
    "            memory_mask = None\n",
    "            # memory_mask = self._generate_subsequent_mask(decoder_len, input_len).to(input_mel.device)\n",
    "            tgt_mask = torch.triu(\n",
    "                torch.full((decoder_len, decoder_len), float(\"-inf\"), dtype=torch.float32, device=input_mel.device), diagonal=1\n",
    "            )\n",
    "            src_mask, tgt_key_padding_mask, src_key_padding_mask, memory_key_padding_mask = None, None, None, None\n",
    "        \n",
    "        memory = self.encoder(text, pos_text, src_mask=src_mask, src_key_padding_mask=src_key_padding_mask)\n",
    "        memory = self.projection(encoded_speech, memory)\n",
    "        mel_output, postnet_ouput = self.decoder(memory, input_mel, pos_mel, memory_mask=memory_mask, memory_key_padding_mask=memory_key_padding_mask, tgt_mask=tgt_mask, tgt_key_padding_mask=tgt_key_padding_mask)\n",
    "        return mel_output, postnet_ouput\n",
    "    \n",
    "    def _generate_subsequent_mask(self, tgt_sz, src_sz):\n",
    "        mask = (torch.triu(torch.ones(src_sz, tgt_sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Dataset Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def load_mel(file_path: Path):\n",
    "    \"\"\"\n",
    "    Load a Mel spectrogram saved as a .npy file.\n",
    "\n",
    "    :param file_path: Path to the .npy file containing the Mel spectrogram\n",
    "    :return: Numpy array of the Mel spectrogram\n",
    "    \"\"\"\n",
    "    if not file_path.exists():\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    try:\n",
    "        mel_spectrogram = np.load(file_path)\n",
    "    except EOFError:\n",
    "        print(f\"File could not be loaded due to EOFError: {random_file}\")\n",
    "        return False\n",
    "    return mel_spectrogram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "\n",
    "class TTSDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, root: Path=None, text_map: dict=None, text_map_file: Path=None, tokenizer=None):\n",
    "        \"\"\"\n",
    "        :param root: Path to the audio files directory\n",
    "        :param text_map: A dictionary mapping audio file names to their corresponding texts\n",
    "        :param text_map_file: Path to the JSON file containing the text map\n",
    "        :param tokenizer: A custom tokenizer for text processing\n",
    "        \"\"\"\n",
    "        self.root = root\n",
    "        self.text_map = text_map if text_map is not None else self.load_text_map(text_map_file)\n",
    "        self.audios = os.listdir(\"./encoded_speechs\")\n",
    "        self.texts = self.text_map\n",
    "        self.tokenizer = tokenizer if tokenizer is not None else self.default_tokenizer\n",
    "        self.encoded_speech_dir = Path(\"./encoded_speechs\")\n",
    "        self.mel_dir = Path(\"./mels\")\n",
    "\n",
    "        \n",
    "    def load_text_map(self, text_map_file: Path):\n",
    "        with text_map_file.open('r') as file:\n",
    "            text_map = json.load(file)\n",
    "        return text_map\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.audios)\n",
    "\n",
    "    def _get_audio_input(self, speech_output):\n",
    "        \"\"\"\n",
    "        Method to get input audio for the decoder, used for inference, like teacher-forcing mode\n",
    "        \"\"\"\n",
    "        speech_input = np.concatenate([np.zeros([1, Text2SpeechAudioConfig.N_MELS], np.float32), \n",
    "                                       speech_output[:-1, :]], axis=0)\n",
    "        return speech_input\n",
    "\n",
    "    def _get_audio_output(self, idx):\n",
    "        \"\"\"\n",
    "        Get ground truth audio (target output), loading from .npy file if it exists.\n",
    "        \"\"\"\n",
    "        mel_file = self.mel_dir / f\"{self.audios[idx]}.npy\"\n",
    "        \n",
    "        if mel_file.exists():\n",
    "            return load_mel(mel_file).T\n",
    "        else:\n",
    "            print(f\"not found: {mel_file}\")\n",
    "            utterance = Utterance(\n",
    "                raw_file=self.root / f\"{self.audios[idx]}.npy\",\n",
    "                processor=AudioPreprocessor(Text2SpeechAudioConfig)\n",
    "            )\n",
    "            return utterance.mel_in_db().T\n",
    "    \n",
    "    def default_tokenizer(self, text):\n",
    "        \"\"\"\n",
    "        Default tokenizer method if no custom tokenizer is provided.\n",
    "        Tokenizes text to a sequence of integers or phonemes.\n",
    "        \"\"\"\n",
    "        # Example: basic character tokenizer, can be replaced with more complex tokenizers\n",
    "        return [ord(char) for char in text]\n",
    "    \n",
    "    def _get_encoded_speech(self, idx):\n",
    "        \"\"\"\n",
    "        Get encoded speech randomly from a folder.\n",
    "        \"\"\"\n",
    "        audio_name = self.audios[idx]  # Extract the audio name without extension\n",
    "        encoded_speech_subfolder = self.encoded_speech_dir / audio_name  # Path to subfolder\n",
    "        encoded_speech_files = list(encoded_speech_subfolder.glob(\"*.npy\"))  # List of all `.pt` files\n",
    "        \n",
    "        if not encoded_speech_files:\n",
    "            utterance = Utterance(raw_file=self.root / self.audios[idx], \n",
    "                              processor=AudioPreprocessor(SpeakerEncoderAudioConfig))\n",
    "        \n",
    "            random_mel = torch.tensor(np.array([utterance.random_mel_in_db(4800)])).transpose(1, 2)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                encoded_speech = SPEECH_TRANSFORMER_ENCODER(random_mel)\n",
    "                \n",
    "            return encoded_speech\n",
    "        \n",
    "        random_file = random.choice(encoded_speech_files)  # Select a random `.pt` file\n",
    "        try:\n",
    "            encoded_speech = np.load(random_file)\n",
    "        except EOFError:\n",
    "            utterance = Utterance(raw_file=self.root / self.audios[idx], \n",
    "                              processor=AudioPreprocessor(SpeakerEncoderAudioConfig))\n",
    "        \n",
    "            random_mel = torch.tensor(np.array([utterance.random_mel_in_db(4800)])).transpose(1, 2)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                encoded_speech = SPEECH_TRANSFORMER_ENCODER(random_mel)\n",
    "                \n",
    "            return encoded_speech.cpu().numpy()\n",
    "        \n",
    "        return encoded_speech\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get text sequence, input audio (for teacher-forcing), and output audio (ground truth)\n",
    "        \"\"\"\n",
    "        # Text to sequence using custom tokenizer\n",
    "        text_sequence = self.tokenizer(self.texts[self.audios[idx]])\n",
    "\n",
    "        # Get Encoding Speech\n",
    "        encoded_speech = self._get_encoded_speech(idx)\n",
    "\n",
    "        # Get the audio output (target output)\n",
    "        output_audio = self._get_audio_output(idx)\n",
    "        \n",
    "        # Get the audio input (decoder input)\n",
    "        input_audio = self._get_audio_input(output_audio)\n",
    "        \n",
    "        # Get positional text\n",
    "        text_length = len(text_sequence)\n",
    "        pos_text = np.arange(1, text_length + 1)\n",
    "        \n",
    "        # Get positional mel\n",
    "        pos_audio = np.arange(1, input_audio.shape[0] + 1)\n",
    "        \n",
    "        return {\n",
    "            \"text\": np.array(text_sequence),\n",
    "            \"text_length\": len(text_sequence),\n",
    "            \"input_mel\": input_audio,\n",
    "            \"encoded_speech\": encoded_speech,\n",
    "            \"mel\": output_audio,\n",
    "            \"pos_text\": pos_text,\n",
    "            \"pos_mel\": pos_audio\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "DATA_PATH = Path(r\"./mags\")\n",
    "TEXT_MAP_PATH = Path(r\"./transcripts.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TTSDataset(root=DATA_PATH, text_map_file=TEXT_MAP_PATH, tokenizer=WordByPhonemesEmbedding())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size], generator=torch.Generator().manual_seed(42))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Collate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def _pad_data(x, length):\n",
    "    _pad = 0\n",
    "    return np.pad(x, (0, length - x.shape[0]), mode='constant', constant_values=_pad)\n",
    "\n",
    "\n",
    "\n",
    "def _prepare_data(inputs):\n",
    "    max_len = max((len(x) for x in inputs))\n",
    "    return np.stack([_pad_data(x, max_len) for x in inputs])\n",
    "\n",
    "\n",
    "def _pad_2d(inputs):\n",
    "    _pad = 0\n",
    "    def _pad_one(x, max_len):\n",
    "        mel_len = x.shape[0]\n",
    "        return np.pad(x, [[0,max_len - mel_len],[0,0]], mode='constant', constant_values=_pad)\n",
    "    max_len = max((x.shape[0] for x in inputs))\n",
    "    return np.stack([_pad_one(x, max_len) for x in inputs])\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    batch = sorted(batch, key=lambda x: x['text_length'], reverse=True)\n",
    "    \n",
    "    text = [d['text'] for d in batch]\n",
    "    mel = [d['mel'] for d in batch]\n",
    "    mel_input = [d['input_mel'] for d in batch]\n",
    "    pos_text = [d['pos_text'] for d in batch]\n",
    "    pos_mel = [d['pos_mel'] for d in batch]\n",
    "    encoded_speech= [d['encoded_speech'] for d in batch]\n",
    "\n",
    "    text = torch.FloatTensor(_pad_2d(text))\n",
    "    mel = torch.FloatTensor(_pad_2d(mel))\n",
    "    mel_input = torch.FloatTensor(_pad_2d(mel_input))\n",
    "    pos_text = torch.LongTensor(_prepare_data(pos_text))\n",
    "    pos_mel = torch.LongTensor(_prepare_data(pos_mel))\n",
    "    encoded_speech = torch.FloatTensor(np.array(encoded_speech))\n",
    "\n",
    "\n",
    "    return text, mel, mel_input, encoded_speech, pos_text, pos_mel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create the DataLoader with the custom collate function\n",
    "eval_dataloader = DataLoader(test_dataset, batch_size=16, collate_fn=collate_fn, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Trainning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10000\n",
    "LEARNING_RATE = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(\n",
    "    model, optimizer, epoch, global_step, train_losses, eval_losses, save_path, current_subset=0\n",
    "):\n",
    "    checkpoint = {\n",
    "        \"epoch\": epoch,\n",
    "        \"global_step\": global_step,\n",
    "        \"current_subset\": current_subset,\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"train_losses\": train_losses,\n",
    "        \"eval_losses\": eval_losses,\n",
    "    }\n",
    "    model_path = f\"{save_path}/model_epoch_{epoch}_subset_{current_subset}.pt\"\n",
    "    torch.save(checkpoint, f\"{model_path}\")\n",
    "    print(f\"Checkpoint saved at {model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def load_checkpoint(save_path, model, optimizer, device):\n",
    "    if os.path.exists(save_path):\n",
    "        checkpoint = torch.load(save_path, map_location=device)\n",
    "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "        epoch = checkpoint[\"epoch\"]\n",
    "        global_step = checkpoint[\"global_step\"]\n",
    "        current_subset = checkpoint[\"current_subset\"]\n",
    "        train_losses = checkpoint[\"train_losses\"]\n",
    "        eval_losses = checkpoint[\"eval_losses\"]\n",
    "        print(f\"Checkpoint loaded from {save_path}\")\n",
    "        return model, optimizer, epoch, global_step, current_subset, train_losses, eval_losses\n",
    "    else:\n",
    "        print(f\"No checkpoint found at {save_path}, starting fresh.\")\n",
    "        return model, optimizer, 0, 0, 0, [], []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_train_dataset(dataset, num_splits=4):\n",
    "    \"\"\"Splits the dataset into `num_splits` equal parts.\"\"\"\n",
    "    indices = list(range(len(dataset)))\n",
    "    split_size = len(dataset) // num_splits\n",
    "    subsets = []\n",
    "    \n",
    "    for i in range(num_splits):\n",
    "        start_idx = i * split_size\n",
    "        end_idx = len(dataset) if i == num_splits - 1 else (i + 1) * split_size\n",
    "        subsets.append(Subset(dataset, indices[start_idx:end_idx]))\n",
    "    return subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    eval_start_time = time.time()\n",
    "    \n",
    "    # Wrap dataloader with tqdm for progress bar\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:  # Wrap with tqdm\n",
    "            text, mel, mel_input, encoded_speech, pos_text, pos_mel = batch\n",
    "            text = text.to(device)\n",
    "            pos_text = pos_text.to(device)\n",
    "            mel_input = mel_input.to(device)\n",
    "            pos_mel = pos_mel.to(device)\n",
    "            encoded_speech = encoded_speech.to(device)\n",
    "            mel = mel.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            mel_pred, postnet_pred = model(text, pos_text, mel_input, pos_mel, encoded_speech)\n",
    "\n",
    "            # Calculate loss\n",
    "            mel_loss = criterion(mel_pred, mel)\n",
    "            postnet_loss = criterion(postnet_pred, mel)\n",
    "            total_loss += (mel_loss.item() + postnet_loss.item())\n",
    "\n",
    "    average_loss = total_loss / len(dataloader)\n",
    "    print(f'Evaluation Loss: {average_loss:.4f}, Eval Time: {time.time() - eval_start_time}')\n",
    "    return average_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "def train_and_evaluate_splits(\n",
    "    model, optimizer, criterion, train_splits, eval_dataloader, \n",
    "    device, num_epochs, start_epoch, save_path, global_step=0, \n",
    "    current_subset=0, train_losses=None, eval_losses=None\n",
    "):\n",
    "    if train_losses is None:\n",
    "        train_losses = []\n",
    "    if eval_losses is None:\n",
    "        eval_losses = []\n",
    "    \n",
    "    num_splits = len(train_splits)\n",
    "\n",
    "    scaler = GradScaler()\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        print(f\"Starting Epoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "        epoch_loss = 0.0\n",
    "        for subset_idx in range(current_subset, num_splits):\n",
    "            print(f\"Training on subset {subset_idx + 1}/{num_splits}\")\n",
    "\n",
    "            # Create DataLoader for the current subset\n",
    "            train_dataloader = DataLoader(train_splits[subset_idx], batch_size=16, shuffle=True, collate_fn=collate_fn, num_workers=4)\n",
    "            \n",
    "\n",
    "            # Wrap train_dataloader with tqdm for progress bar\n",
    "            subset_loss = 0.0\n",
    "            for i, batch in enumerate(tqdm(train_dataloader, desc=f\"Subset {subset_idx + 1}/{num_splits}\", leave=True)):\n",
    "            # for i, batch in enumerate(train_dataloader):\n",
    "                global_step += 1\n",
    "\n",
    "                if global_step < 400000:\n",
    "                    adjust_learning_rate(optimizer, global_step, warmup_step=4000)\n",
    "\n",
    "                # Unpack batch data\n",
    "                text, mel, mel_input, encoded_speech, pos_text, pos_mel = batch\n",
    "                text = text.to(device)\n",
    "                pos_text = pos_text.to(device)\n",
    "                mel_input = mel_input.to(device)\n",
    "                pos_mel = pos_mel.to(device)\n",
    "                encoded_speech = encoded_speech.to(device)\n",
    "                mel = mel.to(device)\n",
    "            \n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "                with autocast():  # Mixed precision\n",
    "                    mel_pred, postnet_pred = model(text, pos_text, mel_input, pos_mel, encoded_speech)\n",
    "                    mel_loss = criterion(mel_pred, mel)\n",
    "                    postnet_loss = criterion(mel_pred, postnet_pred)\n",
    "                    loss = mel_loss + postnet_loss\n",
    "            \n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "\n",
    "                subset_loss += (mel_loss.item() + postnet_loss.item())\n",
    "\n",
    "            # Average loss for the subset\n",
    "            subset_avg_loss = subset_loss / len(train_dataloader)\n",
    "            tqdm.write(f\"Subset {subset_idx + 1}/{num_splits} Loss: {subset_avg_loss:.4f}\")\n",
    "\n",
    "            epoch_loss += subset_avg_loss\n",
    "\n",
    "            # Save progress after each subset\n",
    "            save_checkpoint(\n",
    "                model, optimizer, epoch, global_step, train_losses, eval_losses, save_path, subset_idx\n",
    "            )\n",
    "            \n",
    "        current_subset = 0\n",
    "        # Average loss for the epoch\n",
    "        epoch_avg_loss = epoch_loss / num_splits\n",
    "        tqdm.write(f\"Epoch {epoch + 1} Average Training Loss: {epoch_avg_loss:.4f}\")\n",
    "        train_losses.append(epoch_avg_loss)\n",
    "\n",
    "        # # Evaluate the model after finishing all subsets for the epoch\n",
    "        # eval_loss = evaluate(model, eval_dataloader, criterion, device)\n",
    "        # eval_losses.append(eval_loss)\n",
    "\n",
    "        # Save checkpoint after evaluation\n",
    "        save_checkpoint(\n",
    "            model, optimizer, epoch, global_step, train_losses, eval_losses, save_path, current_subset=0\n",
    "        )\n",
    "\n",
    "        # tqdm.write(f\"Epoch {epoch + 1} complete. Training Loss: {epoch_avg_loss:.4f}, Eval Loss: {eval_loss:.4f}\")\n",
    "        tqdm.write(f\"Epoch {epoch + 1} complete. Training Loss: {epoch_avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Adjust learning rate function\n",
    "def adjust_learning_rate(optimizer, step_num, warmup_step=4000):\n",
    "    lr = LEARNING_RATE * warmup_step**0.5 * min(step_num * warmup_step**-1.5, step_num**-0.5)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 3090\n",
      "NVIDIA GeForce RTX 3090\n",
      "NVIDIA GeForce RTX 3090\n",
      "NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "for i in range(torch.cuda.device_count()):\n",
    "   print(torch.cuda.get_device_properties(i).name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_train_dataset(dataset, num_splits=4):\n",
    "    \"\"\"Splits the dataset into `num_splits` equal parts.\"\"\"\n",
    "    indices = list(range(len(dataset)))\n",
    "    split_size = len(dataset) // num_splits\n",
    "    subsets = []\n",
    "    \n",
    "    for i in range(num_splits):\n",
    "        start_idx = i * split_size\n",
    "        end_idx = len(dataset) if i == num_splits - 1 else (i + 1) * split_size\n",
    "        subsets.append(Subset(dataset, indices[start_idx:end_idx]))\n",
    "    return subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_splits = split_train_dataset(train_dataset, num_splits=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_save_path ='./saved_models/model_epoch_59_subset_0.pt'\n",
    "# model, optimizer, start_epoch, global_step, current_subset, train_losses, eval_losses = load_checkpoint(\n",
    "#     save_path=model_save_path, model=model, optimizer=optimizer, device=device\n",
    "# )\n",
    "# current_subset = 0\n",
    "# start_epoch += 1\n",
    "# if current_subset == 3:\n",
    "#     current_subset = 0\n",
    "#     start_epoch += 1\n",
    "# else:\n",
    "#     current_subset += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/opt/conda/lib/python3.11/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/multiprocessing/spawn.py\", line 132, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'main' on <module '__main__' (built-in)>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/opt/conda/lib/python3.11/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/multiprocessing/spawn.py\", line 132, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'main' on <module '__main__' (built-in)>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/opt/conda/lib/python3.11/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/multiprocessing/spawn.py\", line 132, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'main' on <module '__main__' (built-in)>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/opt/conda/lib/python3.11/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/multiprocessing/spawn.py\", line 132, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'main' on <module '__main__' (built-in)>\n",
      "W1214 16:54:25.000000 58455 site-packages/torch/multiprocessing/spawn.py:160] Terminating process 161397 via signal SIGTERM\n",
      "W1214 16:54:25.002000 58455 site-packages/torch/multiprocessing/spawn.py:160] Terminating process 161398 via signal SIGTERM\n",
      "W1214 16:54:25.004000 58455 site-packages/torch/multiprocessing/spawn.py:160] Terminating process 161399 via signal SIGTERM\n"
     ]
    },
    {
     "ename": "ProcessExitedException",
     "evalue": "process 3 terminated with exit code 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mProcessExitedException\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m world_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspawn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnprocs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworld_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjoin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/multiprocessing/spawn.py:328\u001b[0m, in \u001b[0;36mspawn\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    322\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    323\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis method only supports start_method=spawn (got: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart_method\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    324\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo use a different start_method use:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    325\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m torch.multiprocessing.start_processes(...)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    326\u001b[0m     )\n\u001b[1;32m    327\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(msg, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 328\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mstart_processes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnprocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdaemon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspawn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/multiprocessing/spawn.py:284\u001b[0m, in \u001b[0;36mstart_processes\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m context\n\u001b[1;32m    283\u001b[0m \u001b[38;5;66;03m# Loop on join until it returns True or raises an exception.\u001b[39;00m\n\u001b[0;32m--> 284\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/multiprocessing/spawn.py:192\u001b[0m, in \u001b[0;36mProcessContext.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ProcessExitedException(\n\u001b[1;32m    185\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocess \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m terminated with signal \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (error_index, name),\n\u001b[1;32m    186\u001b[0m             error_index\u001b[38;5;241m=\u001b[39merror_index,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    189\u001b[0m             signal_name\u001b[38;5;241m=\u001b[39mname,\n\u001b[1;32m    190\u001b[0m         )\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 192\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ProcessExitedException(\n\u001b[1;32m    193\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocess \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m terminated with exit code \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (error_index, exitcode),\n\u001b[1;32m    194\u001b[0m             error_index\u001b[38;5;241m=\u001b[39merror_index,\n\u001b[1;32m    195\u001b[0m             error_pid\u001b[38;5;241m=\u001b[39mfailed_process\u001b[38;5;241m.\u001b[39mpid,\n\u001b[1;32m    196\u001b[0m             exit_code\u001b[38;5;241m=\u001b[39mexitcode,\n\u001b[1;32m    197\u001b[0m         )\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_files[error_index], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fh:\n\u001b[1;32m    200\u001b[0m     original_trace \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(fh)\n",
      "\u001b[0;31mProcessExitedException\u001b[0m: process 3 terminated with exit code 1"
     ]
    }
   ],
   "source": [
    "# Split dataset into subsets\n",
    "train_splits = split_train_dataset(train_dataset, num_splits=1)\n",
    "EPOCHS = 10000\n",
    "LEARNING_RATE = 1e-4\n",
    "# Default Model, Optimizer and params\n",
    "device = \"cuda:0\"\n",
    "model = nn.DataParallel(TNTModel().to(device))\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "global_step = 0\n",
    "current_subset = 0\n",
    "start_epoch = 0\n",
    "train_losses = []\n",
    "eval_losses = []\n",
    "save_path = \"./saved_models2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_evaluate_splits(\n",
    "    model, optimizer, criterion, train_splits, eval_dataloader,\n",
    "    device, num_epochs=EPOCHS, start_epoch=start_epoch, save_path=save_path,\n",
    "    global_step=global_step, current_subset=current_subset,\n",
    "    train_losses=train_losses, eval_losses=eval_losses\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5786986,
     "sourceId": 9507723,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5843294,
     "sourceId": 9582705,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 161435,
     "modelInstanceId": 138814,
     "sourceId": 163221,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
